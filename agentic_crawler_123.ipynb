{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bee8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 1\n",
    "\n",
    "import os, re, json, time, asyncio, random, hashlib\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "\n",
    "import httpx\n",
    "import pandas as pd\n",
    "import aiosqlite\n",
    "from dateutil import parser as dateparser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# =========================\n",
    "# Config / Tunables (minimal)\n",
    "# =========================\n",
    "INPUT_PATH = os.environ.get(\"SHOP_CRAWLER_INPUT\", \"D:/museai/data/Indian d2c brands.xlsx\")\n",
    "OUT_DIR    = os.environ.get(\"SHOP_CRAWLER_OUT\", \"./out\")\n",
    "DB_PATH    = os.environ.get(\"SHOP_CRAWLER_DB\", \"D:/museai/data/db/crawler_meta.db\")\n",
    "\n",
    "# LLM — compulsory\n",
    "LLM_MODEL       = os.environ.get(\"LLM_MODEL\", \"gpt-5-nano\")\n",
    "OPENAI_API_KEY  = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Politeness / concurrency\n",
    "GLOBAL_CONCURRENCY   = int(os.environ.get(\"GLOBAL_CONCURRENCY\", 20))\n",
    "PER_HOST_CONCURRENCY = int(os.environ.get(\"PER_HOST_CONCURRENCY\", 2))\n",
    "HOST_GAP_SECONDS     = float(os.environ.get(\"HOST_GAP_SECONDS\", 1.0))\n",
    "REQUEST_TIMEOUT      = float(os.environ.get(\"REQUEST_TIMEOUT\", 18.0))\n",
    "\n",
    "# Agent3 specifics (safe defaults)\n",
    "AGENT3_CONCURRENCY       = int(os.environ.get(\"AGENT3_CONCURRENCY\", \"6\"))\n",
    "SHOPIFY_DELAY_SECONDS    = float(os.environ.get(\"SHOPIFY_DELAY_SECONDS\", \"3.0\"))\n",
    "WORDPRESS_DELAY_SECONDS  = float(os.environ.get(\"WORDPRESS_DELAY_SECONDS\", \"1.2\"))\n",
    "FRESH_START_AGENT3       = int(os.environ.get(\"FRESH_START_AGENT3\", \"0\"))  # 0=merge, 1=truncate latest tables first\n",
    "A2_ONLY_NEW = int(os.environ.get(\"A2_ONLY_NEW\", \"1\"))\n",
    "# Debug / logging\n",
    "VERBOSE = True\n",
    "PRINT_EVERY = 1                    # log every site\n",
    "DEBUG_MAX_BRANDS = 10              # limit to first 10 brands for debug\n",
    "\n",
    "def log(*a):\n",
    "    if VERBOSE:\n",
    "        print(*a, flush=True)\n",
    "\n",
    "UA_POOL = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36\",\n",
    "]\n",
    "BASE_HEADERS = {\n",
    "    \"Accept\": \"application/xml, text/xml;q=0.9, text/plain;q=0.7, */*;q=0.6\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "}\n",
    "\n",
    "ROBOTS_PATH = \"/robots.txt\"\n",
    "\n",
    "# --- ensure folders exist (Windows-safe when DB_PATH has a directory) ---\n",
    "_db_dir = os.path.dirname(DB_PATH)\n",
    "if _db_dir:\n",
    "    os.makedirs(_db_dir, exist_ok=True)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bba93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 — Utils + DB (with safe migrations for legacy NOT NULL columns) + PoliteFetcher\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "\n",
    "def now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def _looks_like_domain_or_url(val: str) -> bool:\n",
    "    if not isinstance(val, str): return False\n",
    "    s = val.strip()\n",
    "    if not s or \" \" in s: return False\n",
    "    return bool(re.search(r\"([a-z0-9-]+\\.)+[a-z]{2,}\", s, re.I))\n",
    "\n",
    "def _norm_root(u: str) -> Optional[str]:\n",
    "    if not isinstance(u, str): return None\n",
    "    s = u.strip()\n",
    "    if not s: return None\n",
    "    if not re.match(r\"^https?://\", s, re.I):\n",
    "        s = \"https://\" + s\n",
    "    parts = urlsplit(s)\n",
    "    if not parts.netloc: return None\n",
    "    return urlunsplit((parts.scheme, parts.netloc, \"\", \"\", \"\"))\n",
    "\n",
    "def _with_www(base: str) -> Optional[str]:\n",
    "    if not base: return None\n",
    "    parts = urlsplit(base)\n",
    "    host = parts.netloc\n",
    "    if host.startswith(\"www.\"): return base\n",
    "    return urlunsplit((parts.scheme, \"www.\"+host, \"\", \"\", \"\"))\n",
    "\n",
    "def _without_www(base: str) -> Optional[str]:\n",
    "    if not base: return None\n",
    "    parts = urlsplit(base)\n",
    "    host = parts.netloc\n",
    "    if host.startswith(\"www.\"):\n",
    "        return urlunsplit((parts.scheme, host[4:], \"\", \"\", \"\"))\n",
    "    return base\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Minimal DB (+ product_sitemaps migrations)\n",
    "# =========================\n",
    "SCHEMA_SQL = r\"\"\"\n",
    "PRAGMA journal_mode=WAL;\n",
    "PRAGMA synchronous=NORMAL;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS sites (\n",
    "  canonical_base TEXT PRIMARY KEY,\n",
    "  brand          TEXT,\n",
    "  first_seen_at  TEXT,\n",
    "  last_seen_at   TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS discoveries (\n",
    "  id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  base_url        TEXT NOT NULL,\n",
    "  robots_url      TEXT,\n",
    "  robots_status   INTEGER,\n",
    "  llm_model       TEXT,\n",
    "  primary_sitemap TEXT,\n",
    "  sitemaps_json   TEXT,\n",
    "  platform        TEXT,\n",
    "  success         INTEGER,\n",
    "  reason          TEXT,\n",
    "  extracted_at    TEXT\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_disc_base ON discoveries(base_url);\n",
    "\n",
    "/* One row per site for product sitemap expansion (new shape) */\n",
    "CREATE TABLE IF NOT EXISTS product_sitemaps (\n",
    "  site                   TEXT,\n",
    "  platform               TEXT,\n",
    "  primary_sitemap        TEXT,\n",
    "  sitemap_kind           TEXT,\n",
    "  product_sitemaps_json  TEXT,   -- JSON array of product-sitemap URLs (deduped)\n",
    "  urls_count             INTEGER,\n",
    "  fetched_at             TEXT,\n",
    "  status                 INTEGER, -- 1=ok, 0=fail, 2=skipped\n",
    "  reason                 TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS product_urls (\n",
    "  site          TEXT,\n",
    "  platform      TEXT,\n",
    "  sitemap_url   TEXT,\n",
    "  url           TEXT,\n",
    "  lastmod       TEXT,\n",
    "  changefreq    TEXT,\n",
    "  images_json   TEXT,\n",
    "  fetched_at    TEXT,\n",
    "  PRIMARY KEY(site, url)        -- de-dupe across retries; URL belongs to a site\n",
    ");\n",
    "CREATE INDEX IF NOT EXISTS idx_product_urls_url ON product_urls(url);\n",
    "CREATE INDEX IF NOT EXISTS idx_product_urls_site_time ON product_urls(site, fetched_at);\n",
    "CREATE INDEX IF NOT EXISTS idx_product_urls_site_sitemap ON product_urls(site, sitemap_url);\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS product_urls_sitemaps (\n",
    "  sitemap_url    TEXT PRIMARY KEY,\n",
    "  site           TEXT,\n",
    "  platform       TEXT,\n",
    "  fetched_at     TEXT,\n",
    "  http_status    INTEGER,\n",
    "  content_type   TEXT,\n",
    "  content_length INTEGER,\n",
    "  etag           TEXT,\n",
    "  last_modified  TEXT,\n",
    "  urls_found     INTEGER,\n",
    "  status         INTEGER,   -- 1=ok, 0=fail\n",
    "  reason         TEXT\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class DB:\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "        self._db: Optional[aiosqlite.Connection] = None\n",
    "        self._ps_cols: Dict[str, Dict[str, Any]] = {}  # column_name -> {notnull:int, dflt:str|None}\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self._db = await aiosqlite.connect(self.path)\n",
    "        await self._db.executescript(SCHEMA_SQL)\n",
    "        await self._db.commit()\n",
    "        await self._migrate_product_sitemaps()\n",
    "        await self._migrate_agent4_tables()   # <-- add this\n",
    "        return self\n",
    "\n",
    "    async def _migrate_agent4_tables(self):\n",
    "        await self._db.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS product_urls (\n",
    "          site TEXT, platform TEXT, sitemap_url TEXT, url TEXT,\n",
    "          lastmod TEXT, changefreq TEXT, images_json TEXT, fetched_at TEXT,\n",
    "          PRIMARY KEY(site, url)\n",
    "        )\"\"\")\n",
    "        await self._db.execute(\"CREATE INDEX IF NOT EXISTS idx_product_urls_url ON product_urls(url)\")\n",
    "        await self._db.execute(\"CREATE INDEX IF NOT EXISTS idx_product_urls_site_time ON product_urls(site, fetched_at)\")\n",
    "        await self._db.execute(\"CREATE INDEX IF NOT EXISTS idx_product_urls_site_sitemap ON product_urls(site, sitemap_url)\")\n",
    "\n",
    "        await self._db.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS product_urls_sitemaps (\n",
    "          sitemap_url TEXT PRIMARY KEY,\n",
    "          site TEXT, platform TEXT, fetched_at TEXT,\n",
    "          http_status INTEGER, content_type TEXT, content_length INTEGER,\n",
    "          etag TEXT, last_modified TEXT, urls_found INTEGER,\n",
    "          status INTEGER, reason TEXT\n",
    "        )\"\"\")\n",
    "        await self._db.commit()\n",
    "\n",
    "    async def upsert_product_urls_bulk(self, site: str, platform: str, sitemap_url: str, rows: List[Dict[str, Any]]):\n",
    "        \"\"\"rows: [{url,lastmod,changefreq,images_json}]\"\"\"\n",
    "        if not rows:\n",
    "            return 0\n",
    "        now = now_iso()\n",
    "        await self._db.execute(\"BEGIN\")\n",
    "        await self._db.executemany(\n",
    "            \"\"\"\n",
    "            INSERT INTO product_urls(site, platform, sitemap_url, url, lastmod, changefreq, images_json, fetched_at)\n",
    "            VALUES(?,?,?,?,?,?,?,?)\n",
    "            ON CONFLICT(site, url) DO UPDATE SET\n",
    "              platform=excluded.platform,\n",
    "              sitemap_url=excluded.sitemap_url,\n",
    "              lastmod=COALESCE(excluded.lastmod, product_urls.lastmod),\n",
    "              changefreq=COALESCE(excluded.changefreq, product_urls.changefreq),\n",
    "              images_json=COALESCE(NULLIF(excluded.images_json,''), product_urls.images_json),\n",
    "              fetched_at=excluded.fetched_at\n",
    "            \"\"\",\n",
    "            [\n",
    "                (site, platform, sitemap_url, r.get(\"url\"), r.get(\"lastmod\"),\n",
    "                 r.get(\"changefreq\"), r.get(\"images_json\") or \"\", now)\n",
    "                for r in rows if r.get(\"url\")\n",
    "            ]\n",
    "        )\n",
    "        await self._db.commit()\n",
    "        return len(rows)\n",
    "\n",
    "    async def mark_product_sitemap(self, sitemap_url: str, site: str, platform: str,\n",
    "                                   http_status: int, content_type: str, content_length: int,\n",
    "                                   etag: str, last_modified: str,\n",
    "                                   urls_found: int, status: int, reason: str):\n",
    "        await self._db.execute(\"\"\"\n",
    "            INSERT INTO product_urls_sitemaps(sitemap_url, site, platform, fetched_at,\n",
    "              http_status, content_type, content_length, etag, last_modified,\n",
    "              urls_found, status, reason)\n",
    "            VALUES(?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "            ON CONFLICT(sitemap_url) DO UPDATE SET\n",
    "              site=excluded.site,\n",
    "              platform=excluded.platform,\n",
    "              fetched_at=excluded.fetched_at,\n",
    "              http_status=excluded.http_status,\n",
    "              content_type=excluded.content_type,\n",
    "              content_length=excluded.content_length,\n",
    "              etag=excluded.etag,\n",
    "              last_modified=excluded.last_modified,\n",
    "              urls_found=excluded.urls_found,\n",
    "              status=excluded.status,\n",
    "              reason=excluded.reason\n",
    "        \"\"\", (\n",
    "            sitemap_url, site, platform, now_iso(),\n",
    "            http_status, content_type or \"\", int(content_length or 0),\n",
    "            etag or \"\", last_modified or \"\",\n",
    "            int(urls_found or 0), int(status or 0), (reason or \"\")[:240]\n",
    "        ))\n",
    "        await self._db.commit()\n",
    "\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        if self._db:\n",
    "            await self._db.commit()\n",
    "            await self._db.close()\n",
    "\n",
    "    async def _migrate_product_sitemaps(self):\n",
    "        \"\"\"Ensure product_sitemaps has required columns, a UNIQUE(site), and capture legacy cols.\"\"\"\n",
    "        # Ensure table exists\n",
    "        await self._db.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS product_sitemaps (\n",
    "          site TEXT, platform TEXT, primary_sitemap TEXT, sitemap_kind TEXT,\n",
    "          product_sitemaps_json TEXT, urls_count INTEGER, fetched_at TEXT,\n",
    "          status INTEGER, reason TEXT\n",
    "        )\"\"\")\n",
    "        await self._db.commit()\n",
    "\n",
    "        # Add missing modern columns\n",
    "        required = {\n",
    "            \"site\":\"TEXT\",\"platform\":\"TEXT\",\"primary_sitemap\":\"TEXT\",\"sitemap_kind\":\"TEXT\",\n",
    "            \"product_sitemaps_json\":\"TEXT\",\"urls_count\":\"INTEGER\",\"fetched_at\":\"TEXT\",\n",
    "            \"status\":\"INTEGER\",\"reason\":\"TEXT\"\n",
    "        }\n",
    "        cur = await self._db.execute(\"PRAGMA table_info(product_sitemaps)\")\n",
    "        info = await cur.fetchall()  # cid, name, type, notnull, dflt_value, pk\n",
    "        have = {r[1] for r in info}\n",
    "        for col, coltype in required.items():\n",
    "            if col not in have:\n",
    "                await self._db.execute(f\"ALTER TABLE product_sitemaps ADD COLUMN {col} {coltype}\")\n",
    "        await self._db.commit()\n",
    "\n",
    "        # Refresh info and cache notnull/defaults for dynamic UPSERTs\n",
    "        cur = await self._db.execute(\"PRAGMA table_info(product_sitemaps)\")\n",
    "        info = await cur.fetchall()\n",
    "        self._ps_cols = {r[1]: {\"notnull\": int(r[3] or 0), \"dflt\": r[4]} for r in info}\n",
    "\n",
    "        # Ensure UNIQUE index on site for ON CONFLICT(site)\n",
    "        # Deduplicate existing rows that would violate uniqueness\n",
    "        cur = await self._db.execute(\"\"\"\n",
    "            SELECT site, MIN(rowid) AS keep_id, COUNT(*) AS c\n",
    "            FROM product_sitemaps\n",
    "            WHERE site IS NOT NULL AND TRIM(site) <> ''\n",
    "            GROUP BY site\n",
    "            HAVING c > 1\n",
    "        \"\"\")\n",
    "        dups = await cur.fetchall()\n",
    "        for site, keep_id, _ in dups:\n",
    "            await self._db.execute(\"DELETE FROM product_sitemaps WHERE site=? AND rowid<>?\", (site, keep_id))\n",
    "        await self._db.commit()\n",
    "\n",
    "        # Create UNIQUE index if missing\n",
    "        cur = await self._db.execute(\"PRAGMA index_list(product_sitemaps)\")\n",
    "        idx_rows = await cur.fetchall()  # seq, name, unique, origin, partial\n",
    "        has_unique_on_site = False\n",
    "        for _, idx_name, is_unique, *_ in idx_rows:\n",
    "            if is_unique:\n",
    "                cur2 = await self._db.execute(f\"PRAGMA index_info({idx_name})\")\n",
    "                cols = [r[2] for r in await cur2.fetchall()]\n",
    "                if cols == [\"site\"]:\n",
    "                    has_unique_on_site = True\n",
    "                    break\n",
    "        if not has_unique_on_site:\n",
    "            await self._db.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS ux_product_sitemaps_site ON product_sitemaps(site)\")\n",
    "            await self._db.commit()\n",
    "\n",
    "        # Friendly secondary index\n",
    "        await self._db.execute(\"CREATE INDEX IF NOT EXISTS idx_ps_site ON product_sitemaps(site)\")\n",
    "        await self._db.commit()\n",
    "\n",
    "    async def upsert_site(self, canonical_base: str, brand: str):\n",
    "        now = now_iso()\n",
    "        await self._db.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO sites(canonical_base, brand, first_seen_at, last_seen_at)\n",
    "            VALUES(?,?,?,?)\n",
    "            ON CONFLICT(canonical_base) DO UPDATE SET\n",
    "              brand=excluded.brand,\n",
    "              last_seen_at=excluded.last_seen_at\n",
    "            \"\"\",\n",
    "            (canonical_base, brand, now, now),\n",
    "        )\n",
    "        await self._db.commit()\n",
    "\n",
    "    async def insert_discovery(self, rec: Dict[str, Any]):\n",
    "        cols = [\n",
    "            \"base_url\",\"robots_url\",\"robots_status\",\"llm_model\",\"primary_sitemap\",\n",
    "            \"sitemaps_json\",\"platform\",\"success\",\"reason\",\"extracted_at\"\n",
    "        ]\n",
    "        vals = [rec.get(c) for c in cols]\n",
    "        placeholders = \",\".join([\"?\"]*len(cols))\n",
    "        await self._db.execute(\n",
    "            f\"INSERT INTO discoveries({','.join(cols)}) VALUES({placeholders})\",\n",
    "            tuple(vals)\n",
    "        )\n",
    "        await self._db.commit()\n",
    "\n",
    "    async def merge_product_sitemaps(\n",
    "        self,\n",
    "        site: str,\n",
    "        platform: Optional[str],\n",
    "        primary_sitemap: Optional[str],\n",
    "        sitemap_kind: Optional[str],\n",
    "        new_urls: List[str],\n",
    "        status: int,\n",
    "        reason: str\n",
    "    ):\n",
    "        \"\"\"Upsert one row per site; merge/dedupe arrays. Compatible with legacy NOT NULL columns.\"\"\"\n",
    "        site = (site or \"\").rstrip(\"/\")\n",
    "\n",
    "        # Load existing URLs\n",
    "        cur = await self._db.execute(\"SELECT product_sitemaps_json FROM product_sitemaps WHERE site=?\", (site,))\n",
    "        row = await cur.fetchone()\n",
    "        old_urls: List[str] = []\n",
    "        if row and row[0]:\n",
    "            try:\n",
    "                old_urls = json.loads(row[0])\n",
    "            except Exception:\n",
    "                old_urls = []\n",
    "\n",
    "        merged, seen = [], set()\n",
    "        for u in (old_urls + new_urls):\n",
    "            u = (u or \"\").strip()\n",
    "            if u and u not in seen:\n",
    "                merged.append(u); seen.add(u)\n",
    "\n",
    "        arr_json = json.dumps(merged, ensure_ascii=False)\n",
    "        now = now_iso()\n",
    "\n",
    "        # ---------- Dynamic column handling (legacy-safe) ----------\n",
    "        # Base set we always write\n",
    "        insert_cols = [\n",
    "            \"site\",\"platform\",\"primary_sitemap\",\"sitemap_kind\",\n",
    "            \"product_sitemaps_json\",\"urls_count\",\"fetched_at\",\"status\",\"reason\"\n",
    "        ]\n",
    "        insert_vals = [\n",
    "            site, platform, primary_sitemap, sitemap_kind,\n",
    "            arr_json, len(merged), now, status, reason[:240]\n",
    "        ]\n",
    "\n",
    "        # If legacy NOT NULL columns exist (e.g., 'product_sitemap', 'url'), populate them\n",
    "        def legacy_value(col: str) -> Any:\n",
    "            if col == \"product_sitemap\":\n",
    "                # fill with primary_sitemap (or first product sitemap) to satisfy NOT NULL\n",
    "                return primary_sitemap or (merged[0] if merged else \"\")\n",
    "            if col == \"url\":\n",
    "                # old schemas often had a single 'url' — store primary or first\n",
    "                return primary_sitemap or (merged[0] if merged else \"\")\n",
    "            # sensible defaults for other unknown NOT NULL columns\n",
    "            return \"\"  # empty string keeps NOT NULL happy without meaning\n",
    "\n",
    "        for col, meta in self._ps_cols.items():\n",
    "            if col in insert_cols:\n",
    "                continue\n",
    "            if int(meta.get(\"notnull\", 0)) == 1:\n",
    "                insert_cols.append(col)\n",
    "                insert_vals.append(legacy_value(col))\n",
    "\n",
    "        # Build UPDATE list mirroring what we inserted (excluding PK)\n",
    "        update_assignments = [f\"{c}=excluded.{c}\" for c in insert_cols if c != \"site\"]\n",
    "\n",
    "        sql_upsert = f\"\"\"\n",
    "            INSERT INTO product_sitemaps({', '.join(insert_cols)})\n",
    "            VALUES({', '.join(['?']*len(insert_cols))})\n",
    "            ON CONFLICT(site) DO UPDATE SET\n",
    "              {', '.join(update_assignments)}\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            await self._db.execute(sql_upsert, tuple(insert_vals))\n",
    "        except Exception:\n",
    "            # Fallback for very old DBs with no UNIQUE(site): manual update/insert\n",
    "            set_clause = \", \".join([f\"{c}=?\" for c in insert_cols if c != \"site\"])\n",
    "            params_update = [v for c, v in zip(insert_cols, insert_vals) if c != \"site\"] + [site]\n",
    "            await self._db.execute(f\"UPDATE product_sitemaps SET {set_clause} WHERE site=?\", params_update)\n",
    "            cur2 = await self._db.execute(\"SELECT changes()\")\n",
    "            changed = (await cur2.fetchone() or [0])[0]\n",
    "            if not changed:\n",
    "                await self._db.execute(\n",
    "                    f\"INSERT INTO product_sitemaps({', '.join(insert_cols)}) VALUES({', '.join(['?']*len(insert_cols))})\",\n",
    "                    tuple(insert_vals)\n",
    "                )\n",
    "\n",
    "        await self._db.commit()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HTTP client (polite & 429-safe)\n",
    "# =========================\n",
    "# =========================\n",
    "# HTTP client (polite & 429-safe)\n",
    "# =========================\n",
    "# =========================\n",
    "# HTTP client (polite & 429-safe)\n",
    "# =========================\n",
    "class PoliteFetcher:\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_concurrency: Optional[int]=None,\n",
    "        per_host_concurrency: Optional[int]=None,\n",
    "        force_http2: Optional[bool]=None,\n",
    "        timeout: Optional[float]=None,\n",
    "    ):\n",
    "        self._client: Optional[httpx.AsyncClient] = None\n",
    "        self._gsem = asyncio.Semaphore(global_concurrency or GLOBAL_CONCURRENCY)\n",
    "        self._per_host_conc = per_host_concurrency or PER_HOST_CONCURRENCY\n",
    "        self._host_sem: Dict[str, asyncio.Semaphore] = {}\n",
    "        self._host_next: Dict[str, float] = {}\n",
    "        self._lock = asyncio.Lock()\n",
    "        self._http2 = True if force_http2 is None else bool(force_http2)\n",
    "        self._timeout = timeout or REQUEST_TIMEOUT\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        self._client = httpx.AsyncClient(\n",
    "            headers={**BASE_HEADERS, \"User-Agent\": random.choice(UA_POOL)},\n",
    "            timeout=self._timeout,\n",
    "            follow_redirects=True,\n",
    "            http2=self._http2,\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        if self._client:\n",
    "            await self._client.aclose()\n",
    "\n",
    "    async def _throttle(self, base: str):\n",
    "        async with self._lock:\n",
    "            now = time.monotonic()\n",
    "            next_allowed = self._host_next.get(base)\n",
    "            if next_allowed is not None and now < next_allowed:\n",
    "                await asyncio.sleep(next_allowed - now)\n",
    "            self._host_next[base] = time.monotonic() + HOST_GAP_SECONDS + random.uniform(0, 0.35)\n",
    "\n",
    "    async def get(self, url: str, *, headers: Optional[Dict[str, str]] = None, allow_redirects: bool = True) -> httpx.Response:\n",
    "        parts = urlsplit(url)\n",
    "        base = urlunsplit((parts.scheme, parts.netloc, \"\", \"\", \"\"))\n",
    "        if base not in self._host_sem:\n",
    "            self._host_sem[base] = asyncio.Semaphore(self._per_host_conc)\n",
    "        async with self._gsem, self._host_sem[base]:\n",
    "            await self._throttle(base)\n",
    "            try:\n",
    "                # start from client defaults, rotate UA, then let per-call headers override\n",
    "                per_req_headers = dict(self._client.headers)\n",
    "                per_req_headers[\"User-Agent\"] = random.choice(UA_POOL)\n",
    "                if headers:\n",
    "                    per_req_headers.update(headers)\n",
    "                return await self._client.get(\n",
    "                    url,\n",
    "                    headers=per_req_headers,\n",
    "                    follow_redirects=allow_redirects,  # map allow_redirects -> httpx follow_redirects\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # preserve the exception message in the response body for debugging\n",
    "                return httpx.Response(0, request=httpx.Request(\"GET\", url), content=str(e).encode(\"utf-8\"))\n",
    "\n",
    "    async def get_with_backoff(\n",
    "        self,\n",
    "        url: str,\n",
    "        *,\n",
    "        retries: int = 3,\n",
    "        headers: Optional[Dict[str, str]] = None,\n",
    "        allow_redirects: bool = True,\n",
    "    ) -> httpx.Response:\n",
    "        r = await self.get(url, headers=headers, allow_redirects=allow_redirects)\n",
    "        if r.status_code in (429, 403, 503, 520, 522) and retries > 0:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            try:\n",
    "                delay = float(ra) if ra and re.match(r\"^[0-9.]+$\", ra) else random.uniform(3.5, 8.0)\n",
    "            except Exception:\n",
    "                delay = random.uniform(3.5, 8.0)\n",
    "            await asyncio.sleep(delay)\n",
    "            try:\n",
    "                if self._client:\n",
    "                    self._client.headers[\"User-Agent\"] = random.choice(UA_POOL)\n",
    "            except Exception:\n",
    "                pass\n",
    "            return await self.get_with_backoff(\n",
    "                url,\n",
    "                retries=retries - 1,\n",
    "                headers=headers,\n",
    "                allow_redirects=allow_redirects,\n",
    "            )\n",
    "        return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de08f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 3\n",
    "\n",
    "# =========================\n",
    "# LLM wrapper (compulsory) — Chat Completions only, with origin + hints\n",
    "# =========================\n",
    "class LLM:\n",
    "    def __init__(self, model: str, api_key: Optional[str]):\n",
    "        if not api_key:\n",
    "            raise RuntimeError(\"OPENAI_API_KEY required: LLM is compulsory for extraction\")\n",
    "        self.model = model\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def _make_hints(self, robots_text: str) -> List[str]:\n",
    "        urls = re.findall(r\"https?://[^\\s#]+\", robots_text, flags=re.I)\n",
    "        seen = set(); out = []\n",
    "        for u in urls:\n",
    "            u = u.strip().rstrip(\").,;\")\n",
    "            if u not in seen:\n",
    "                out.append(u); seen.add(u)\n",
    "        return out[:25]\n",
    "\n",
    "    async def extract(self, robots_text: str, origin_base: str) -> Dict[str, Any]:\n",
    "        hints = self._make_hints(robots_text)\n",
    "        sys_prompt = (\n",
    "            \"You are a precise extractor for robots.txt files.\\n\"\n",
    "            \"Output ONLY JSON with keys: primary_sitemap (string|null), sitemaps (array), \"\n",
    "            \"platform ('shopify'|'wordpress'|'other'), success (bool), reason (string).\\n\"\n",
    "            \"Prefer https://<host>/sitemap.xml if present; for WordPress prefer product sitemaps when available.\\n\"\n",
    "            \"Do not invent URLs; resolve relative paths using the provided ORIGIN.\"\n",
    "        )\n",
    "        user_block = f\"ORIGIN: {origin_base}\\nROBOTS.TXT:\\n{robots_text[:48000]}\\n\"\n",
    "        if hints:\n",
    "            user_block += \"HINTS:\\n\" + \"\\n\".join(hints)\n",
    "\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"system\", \"content\": sys_prompt},\n",
    "                      {\"role\": \"user\", \"content\": user_block}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        out_text = resp.choices[0].message.content or \"{}\"\n",
    "        m = re.search(r\"\\{.*\\}\", out_text, re.S)\n",
    "        data = json.loads(m.group(0)) if m else json.loads(out_text)\n",
    "\n",
    "        def _abs(u: str) -> bool:\n",
    "            return bool(re.match(r\"^https?://\", u, re.I))\n",
    "\n",
    "        sitemaps = [u.strip() for u in (data.get(\"sitemaps\") or []) if isinstance(u, str) and _abs(u.strip())]\n",
    "        primary = data.get(\"primary_sitemap\")\n",
    "        if not (isinstance(primary, str) and _abs(primary.strip())):\n",
    "            primary = None\n",
    "        else:\n",
    "            primary = primary.strip()\n",
    "\n",
    "        platform = str(data.get(\"platform\") or \"other\").lower()\n",
    "        if platform not in {\"shopify\", \"wordpress\", \"other\"}:\n",
    "            platform = \"other\"\n",
    "\n",
    "        success = bool(data.get(\"success\")) and (primary is not None or len(sitemaps) > 0)\n",
    "        reason = str(data.get(\"reason\") or (\"ok\" if success else \"no sitemap lines\"))[:240]\n",
    "\n",
    "        return {\n",
    "            \"primary_sitemap\": primary,\n",
    "            \"sitemaps\": sitemaps,\n",
    "            \"platform\": platform,\n",
    "            \"success\": success,\n",
    "            \"reason\": reason,\n",
    "        }\n",
    "\n",
    "    async def extract_product_sitemaps(self, sitemap_text: str, origin_base: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return an array of absolute URLs that are PRODUCT sitemap files only.\n",
    "        Shopify: sitemap_products_*.xml\n",
    "        WordPress: product-sitemap.xml, product-*.xml (exclude product_tag, product_cat, product_variation, attributes)\n",
    "        For mixed or index sitemaps, return only the product sitemap files.\n",
    "        \"\"\"\n",
    "        sys_prompt = (\n",
    "            \"You are a precise extractor for XML/HTML sitemaps. \"\n",
    "            \"Output ONLY JSON with keys: products (array of absolute URLs), success (bool), reason (string).\\n\"\n",
    "            \"Include ONLY product sitemap files:\\n\"\n",
    "            \"- Shopify: URLs like 'sitemap_products_*.xml'.\\n\"\n",
    "            \"- WordPress: URLs like 'product-sitemap.xml' or 'product-*.xml'.\\n\"\n",
    "            \"EXCLUDE category/tag/attribute/variation sitemaps (e.g., product_cat, product-tag, product_tag, \"\n",
    "            \"product-attribute, product_variation). Do NOT invent URLs. Resolve relative URLs against ORIGIN.\"\n",
    "        )\n",
    "        user = f\"ORIGIN: {origin_base}\\nSITEMAP CONTENT (raw):\\n{sitemap_text[:48000]}\\n\"\n",
    "\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"system\", \"content\": sys_prompt},\n",
    "                      {\"role\": \"user\", \"content\": user}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        out_text = resp.choices[0].message.content or \"{}\"\n",
    "        try:\n",
    "            data = json.loads(out_text)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\{.*\\}\", out_text, re.S)\n",
    "            data = json.loads(m.group(0)) if m else {\"products\":[]}\n",
    "\n",
    "        products = []\n",
    "        for u in (data.get(\"products\") or []):\n",
    "            if isinstance(u, str) and re.match(r\"^https?://\", u.strip(), re.I):\n",
    "                products.append(u.strip())\n",
    "        # de-dupe\n",
    "        seen = set(); out = []\n",
    "        for u in products:\n",
    "            if u not in seen:\n",
    "                out.append(u); seen.add(u)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6440492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 4\n",
    "\n",
    "# =========================\n",
    "# Agent 1: load brands\n",
    "# =========================\n",
    "async def agent1_load_brands(db: DB, path: str) -> List[Dict[str,str]]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"INPUT_PATH not found: {path}\")\n",
    "    if path.lower().endswith('.csv'):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "    found: List[Dict[str,str]] = []\n",
    "    for _, row in df.iterrows():\n",
    "        cells = [str(v) for v in row.to_dict().values() if pd.notna(v)]\n",
    "        url_val = None\n",
    "        for c in cells:\n",
    "            if _looks_like_domain_or_url(c):\n",
    "                url_val = c.strip(); break\n",
    "        base = _norm_root(url_val) if url_val else None\n",
    "        if not base: continue\n",
    "        brand = None\n",
    "        for cname in df.columns:\n",
    "            if 'brand' in str(cname).lower():\n",
    "                v = row[cname]\n",
    "                brand = str(v).strip() if pd.notna(v) else None\n",
    "                break\n",
    "        brand = brand or base\n",
    "        found.append({\"brand\": brand, \"base\": base})\n",
    "    # dedupe by base\n",
    "    uniq, seen = [], set()\n",
    "    for r in found:\n",
    "        if r[\"base\"] not in seen:\n",
    "            uniq.append(r); seen.add(r[\"base\"])\n",
    "    for r in uniq:\n",
    "        await db.upsert_site(r[\"base\"], r[\"brand\"])\n",
    "    return uniq\n",
    "\n",
    "# =========================\n",
    "# Helper: resolve canonical base (follows redirects)\n",
    "# =========================\n",
    "async def resolve_canonical_base(fetch: PoliteFetcher, base: str) -> str:\n",
    "    try:\n",
    "        r = await fetch.get_with_backoff(base.rstrip('/'), retries=2)\n",
    "        if r.status_code and str(r.url):\n",
    "            p = urlsplit(str(r.url))\n",
    "            if p.scheme and p.netloc:\n",
    "                canon = urlunsplit((p.scheme, p.netloc, \"\", \"\", \"\"))\n",
    "                log(f\"[A2] canonical: {base} → {canon}\")\n",
    "                return canon\n",
    "    except Exception:\n",
    "        pass\n",
    "    return base\n",
    "\n",
    "def make_variants(canon: str) -> List[str]:\n",
    "    p = urlsplit(canon)\n",
    "    https_host = p.netloc\n",
    "    https_base = urlunsplit((\"https\", https_host, \"\", \"\", \"\"))\n",
    "\n",
    "    with_www = https_base if https_host.startswith(\"www.\") else urlunsplit((\"https\", \"www.\"+https_host, \"\", \"\", \"\"))\n",
    "    without_www = https_base if not https_host.startswith(\"www.\") else urlunsplit((\"https\", https_host[4:], \"\", \"\", \"\"))\n",
    "\n",
    "    https_trials = [\n",
    "        https_base.rstrip('/') + ROBOTS_PATH,\n",
    "        with_www.rstrip('/') + ROBOTS_PATH,\n",
    "        without_www.rstrip('/') + ROBOTS_PATH,\n",
    "    ]\n",
    "    http_trials = [u.replace(\"https://\", \"http://\", 1) for u in https_trials]\n",
    "\n",
    "    seen = set(); all_u = []\n",
    "    for u in https_trials + http_trials:\n",
    "        if u not in seen:\n",
    "            all_u.append(u); seen.add(u)\n",
    "    return all_u\n",
    "\n",
    "# =========================\n",
    "# Agent 2: robots → LLM extract (ONLY)\n",
    "# =========================\n",
    "async def fetch_robots(fetch: PoliteFetcher, base: str) -> Tuple[str, int, Optional[str], Dict[str,str]]:\n",
    "    canon = await resolve_canonical_base(fetch, base)\n",
    "    trials = make_variants(canon)\n",
    "\n",
    "    last_status, last_url = 0, trials[0]\n",
    "    for u in trials:\n",
    "        log(f\"[A2] robots try: {u}\")\n",
    "        r = await fetch.get_with_backoff(u, retries=2)\n",
    "        ct = r.headers.get(\"content-type\", \"\")\n",
    "        clen = r.headers.get(\"content-length\", \"\")\n",
    "        log(f\"[A2] robots status: {u} → {r.status_code} (ct={ct}, len={clen})\")\n",
    "        if r.status_code == 200:\n",
    "            preview = (r.text or \"\")[:240].replace(\"\\r\", \"\\\\r\").replace(\"\\n\", \"\\\\n\")\n",
    "            log(f\"[A2] robots head: {preview}\")\n",
    "            return u, r.status_code, (r.text or \"\"), {\"content-type\": ct}\n",
    "        last_status, last_url = r.status_code, u\n",
    "\n",
    "    return last_url, last_status, None, {\"content-type\": \"\"}\n",
    "\n",
    "async def agent2_probe(db: DB, rows: List[Dict[str,str]]):\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY is required — LLM is compulsory for extraction\")\n",
    "    llm = LLM(LLM_MODEL, OPENAI_API_KEY)\n",
    "\n",
    "    if DEBUG_MAX_BRANDS > 0:\n",
    "        rows = rows[:DEBUG_MAX_BRANDS]\n",
    "    total = len(rows)\n",
    "    log(f\"[A2] Starting robots+LLM on {total} site(s)...\")\n",
    "\n",
    "    async with PoliteFetcher() as fetch:\n",
    "        sem = asyncio.Semaphore(GLOBAL_CONCURRENCY)\n",
    "\n",
    "        async def one(i: int, r: Dict[str,str]):\n",
    "            base = r[\"base\"]\n",
    "            try:\n",
    "                async with sem:\n",
    "                    if (i % PRINT_EVERY) == 0:\n",
    "                        log(f\"[A2] [{i}/{total}] start: {base}\")\n",
    "                    robots_url, status, text, meta = await fetch_robots(fetch, base)\n",
    "                    if status == 200 and text:\n",
    "                        p = urlsplit(robots_url) if robots_url else urlsplit(base)\n",
    "                        origin = f\"{p.scheme}://{p.netloc}\"\n",
    "                        data = await llm.extract(text, origin_base=origin)\n",
    "                        rec = {\n",
    "                            \"base_url\": base,\n",
    "                            \"robots_url\": robots_url,\n",
    "                            \"robots_status\": status,\n",
    "                            \"llm_model\": LLM_MODEL,\n",
    "                            \"primary_sitemap\": data.get(\"primary_sitemap\"),\n",
    "                            \"sitemaps_json\": json.dumps(data.get(\"sitemaps\") or [], ensure_ascii=False),\n",
    "                            \"platform\": data.get(\"platform\"),\n",
    "                            \"success\": 1 if data.get(\"success\") else 0,\n",
    "                            \"reason\": data.get(\"reason\"),\n",
    "                            \"extracted_at\": now_iso(),\n",
    "                        }\n",
    "                        await db.insert_discovery(rec)\n",
    "                        ok = \"OK\" if rec[\"success\"] else \"NO-SM\"\n",
    "                        log(f\"[A2] [{i}/{total}] {ok}: {base} → platform={rec['platform']} \"\n",
    "                            f\"primary={rec['primary_sitemap'] or '-'}\")\n",
    "                    else:\n",
    "                        rec = {\n",
    "                            \"base_url\": base,\n",
    "                            \"robots_url\": robots_url,\n",
    "                            \"robots_status\": status,\n",
    "                            \"llm_model\": LLM_MODEL,\n",
    "                            \"primary_sitemap\": None,\n",
    "                            \"sitemaps_json\": json.dumps([], ensure_ascii=False),\n",
    "                            \"platform\": None,\n",
    "                            \"success\": 0,\n",
    "                            \"reason\": f\"robots_fetch_failed:{status}\",\n",
    "                            \"extracted_at\": now_iso(),\n",
    "                        }\n",
    "                        await db.insert_discovery(rec)\n",
    "                        log(f\"[A2] [{i}/{total}] FAIL: {base} → robots {status}\")\n",
    "            except Exception as e:\n",
    "                rec = {\n",
    "                    \"base_url\": base,\n",
    "                    \"robots_url\": None,\n",
    "                    \"robots_status\": None,\n",
    "                    \"llm_model\": LLM_MODEL,\n",
    "                    \"primary_sitemap\": None,\n",
    "                    \"sitemaps_json\": json.dumps([], ensure_ascii=False),\n",
    "                    \"platform\": None,\n",
    "                    \"success\": 0,\n",
    "                    \"reason\": f\"exception:{type(e).__name__}:{str(e)[:180]}\",\n",
    "                    \"extracted_at\": now_iso(),\n",
    "                }\n",
    "                await db.insert_discovery(rec)\n",
    "                log(f\"[A2] [{i}/{total}] EXC: {base} → {type(e).__name__}: {e}\")\n",
    "\n",
    "        tasks = [one(i, r) for i, r in enumerate(rows, start=1)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "# LangGraph nodes\n",
    "#CELL 4\n",
    "\n",
    "from typing import TypedDict\n",
    "import sqlite3\n",
    "\n",
    "class AState(TypedDict, total=False):\n",
    "    brands: List[Dict[str, str]]\n",
    "\n",
    "# --- nodes ---\n",
    "async def node_load(state: AState) -> AState:\n",
    "    async with DB(DB_PATH) as db:\n",
    "        rows = await agent1_load_brands(db, INPUT_PATH)\n",
    "    print(f\"[Agent1] Loaded brands: {len(rows)}\", flush=True)\n",
    "    for r in rows[:min(5, len(rows))]:\n",
    "        print(\"   ↳\", r[\"brand\"], \"→\", r[\"base\"], flush=True)\n",
    "    return {\"brands\": rows}\n",
    "\n",
    "async def node_probe(state: AState) -> AState:\n",
    "    # Honor SKIP_A2\n",
    "    if os.environ.get(\"SKIP_A2\", \"0\") == \"1\":\n",
    "        print(\"[A2] SKIP_A2=1 → skipping Agent2 (robots+LLM).\", flush=True)\n",
    "        return state\n",
    "\n",
    "    brands = state.get(\"brands\", []) or []\n",
    "    total_all = len(brands)\n",
    "\n",
    "    # Only-new mode (skip bases already in sites_latest)\n",
    "    only_new = os.environ.get(\"A2_ONLY_NEW\", \"1\") == \"1\"\n",
    "    bases_in_latest = set()\n",
    "    if only_new:\n",
    "        try:\n",
    "            with sqlite3.connect(DB_PATH) as conn:\n",
    "                try:\n",
    "                    cur = conn.execute(\"SELECT site FROM sites_latest\")\n",
    "                    bases_in_latest = { (row[0] or \"\").rstrip(\"/\") for row in cur.fetchall() }\n",
    "                except sqlite3.OperationalError:\n",
    "                    bases_in_latest = set()\n",
    "        except Exception:\n",
    "            bases_in_latest = set()\n",
    "\n",
    "    rows = []\n",
    "    if only_new and bases_in_latest:\n",
    "        for r in brands:\n",
    "            b = (r[\"base\"] or \"\").rstrip(\"/\")\n",
    "            if b not in bases_in_latest:\n",
    "                rows.append(r)\n",
    "        print(f\"[A2] Only-new mode on → {len(rows)} site(s) to process (filtered from {total_all}).\", flush=True)\n",
    "        if not rows:\n",
    "            print(\"[A2] Nothing to do (all sites already present in sites_latest).\", flush=True)\n",
    "            return state\n",
    "    else:\n",
    "        rows = brands\n",
    "        print(f\"[A2] Preparing to process {min(len(rows), DEBUG_MAX_BRANDS) if DEBUG_MAX_BRANDS>0 else len(rows)} site(s) (of {len(rows)})...\", flush=True)\n",
    "\n",
    "    async with DB(DB_PATH) as db:\n",
    "        await agent2_probe(db, rows)\n",
    "\n",
    "    print(\"[Agent2] Probing complete\", flush=True)\n",
    "    return state\n",
    "\n",
    "# --- build & compile a fresh graph every time this cell runs ---\n",
    "def build_graph():\n",
    "    g = StateGraph(AState)\n",
    "    g.add_node(\"load_brands\", node_load)\n",
    "    g.add_node(\"probe\", node_probe)\n",
    "    g.add_edge(START, \"load_brands\")\n",
    "    g.add_edge(\"load_brands\", \"probe\")\n",
    "    g.add_edge(\"probe\", END)\n",
    "    return g.compile()\n",
    "\n",
    "app = build_graph()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b63cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 — Agent 3: LLM-only expansion (skip product primaries)\n",
    "\n",
    "import sqlite3, re, random, asyncio\n",
    "from urllib.parse import urlsplit, urlunsplit\n",
    "\n",
    "# Concurrency knobs\n",
    "A3_CONCURRENCY = int(os.environ.get(\"AGENT3_CONCURRENCY\", os.environ.get(\"A3_CONCURRENCY\", \"6\")))\n",
    "A3_PER_HOST    = int(os.environ.get(\"A3_PER_HOST\", \"1\"))\n",
    "A3_MAX_SITES   = int(os.environ.get(\"A3_MAX_SITES\", \"0\"))   # 0 = all\n",
    "\n",
    "# Fresh start switch (truncate product_sitemaps before writing)\n",
    "# Fresh start switch (truncate product_sitemaps before writing)\n",
    "# => FRESH_START_AGENT3=\"1\" means DO fresh; \"0\" means incremental.\n",
    "FRESH_START_AGENT3 = os.environ.get(\"FRESH_START_AGENT3\", \"0\") == \"1\"\n",
    "\n",
    "# Skip rows already OK (only relevant when NOT fresh)\n",
    "A3_SKIP_IF_ALREADY_OK = (os.environ.get(\"A3_SKIP_IF_ALREADY_OK\", \"1\") == \"1\") and (not FRESH_START_AGENT3)\n",
    "\n",
    "# NEW: exclude product primaries entirely (default ON)\n",
    "A3_EXCLUDE_PRODUCT_PRIMARIES = os.environ.get(\"A3_EXCLUDE_PRODUCT_PRIMARIES\", \"1\") == \"1\"\n",
    "\n",
    "def _origin_from_url(u: str) -> str:\n",
    "    p = urlsplit(u)\n",
    "    return urlunsplit((p.scheme, p.netloc, \"\", \"\", \"\"))\n",
    "\n",
    "async def agent3_expand_product_sitemaps():\n",
    "    # Ensure schema exists and optionally fresh-start\n",
    "    async with DB(DB_PATH) as _db:\n",
    "        if FRESH_START_AGENT3:\n",
    "            await _db._db.execute(\"DELETE FROM product_sitemaps\")\n",
    "            await _db._db.commit()\n",
    "            print(f\"[A3] FRESH_START_AGENT3={int(FRESH_START_AGENT3)} → cleared product_sitemaps.\", flush=True)\n",
    "        else:\n",
    "            print(f\"[A3] FRESH_START_AGENT3={int(FRESH_START_AGENT3)} → incremental; \"\n",
    "                f\"A3_SKIP_IF_ALREADY_OK={int(A3_SKIP_IF_ALREADY_OK)}.\", flush=True)\n",
    "\n",
    "\n",
    "    # Load exactly what's in sites_latest\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH) as conn:\n",
    "            cur = conn.execute(\"\"\"\n",
    "                SELECT site,\n",
    "                       LOWER(COALESCE(platform,'other')) AS platform,\n",
    "                       COALESCE(sitemap,'') AS sitemap,\n",
    "                       LOWER(COALESCE(sitemap_kind,'')) AS kind\n",
    "                FROM sites_latest\n",
    "                WHERE sitemap IS NOT NULL\n",
    "            \"\"\")\n",
    "            rows = [(r[0] or \"\", r[1] or \"other\", r[2] or \"\", r[3] or \"\") for r in cur.fetchall()]\n",
    "    except Exception as e:\n",
    "        print(f\"[A3] ERROR reading sites_latest: {e}\", flush=True)\n",
    "        return\n",
    "\n",
    "    # Optional cap\n",
    "    if A3_MAX_SITES > 0:\n",
    "        rows = rows[:A3_MAX_SITES]\n",
    "\n",
    "    # Optionally skip already-OK sites unless fresh\n",
    "    if A3_SKIP_IF_ALREADY_OK:\n",
    "        try:\n",
    "            with sqlite3.connect(DB_PATH) as conn:\n",
    "                done = { (s or \"\").rstrip(\"/\") for (s,) in conn.execute(\n",
    "                    \"SELECT site FROM product_sitemaps WHERE status=1 AND urls_count>0\"\n",
    "                ).fetchall() }\n",
    "            rows = [r for r in rows if (r[0] or \"\").rstrip(\"/\") not in done]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Split into: to-skip (already product primaries) vs to-expand\n",
    "    if A3_EXCLUDE_PRODUCT_PRIMARIES:\n",
    "        skip = [(s,p,sm,k) for (s,p,sm,k) in rows if k == \"product\"]\n",
    "        todo = [(s,p,sm,k) for (s,p,sm,k) in rows if k != \"product\"]\n",
    "    else:\n",
    "        skip, todo = [], rows\n",
    "\n",
    "    print(f\"[A3] Expanding product sitemaps for {len(todo)} site(s) …\", flush=True)\n",
    "    if skip:\n",
    "        # Record product primaries as-is (no fetch/LLM)\n",
    "        async with DB(DB_PATH) as db:\n",
    "            for s,p,sm,k in skip:\n",
    "                await db.merge_product_sitemaps(\n",
    "                    site=s, platform=p, primary_sitemap=sm, sitemap_kind=k,\n",
    "                    new_urls=[sm], status=1, reason=\"primary already product sitemap; not expanded\"\n",
    "                )\n",
    "\n",
    "    if not todo:\n",
    "        print(\"[Agent3] Product sitemap expansion complete.\", flush=True)\n",
    "        return\n",
    "\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY is required for LLM extraction in A3\")\n",
    "    llm = LLM(LLM_MODEL, OPENAI_API_KEY)\n",
    "\n",
    "    # Use HTTP/1.1 to avoid some TLS/0 issues seen with certain CDNs\n",
    "    async with PoliteFetcher(\n",
    "        global_concurrency=A3_CONCURRENCY,\n",
    "        per_host_concurrency=A3_PER_HOST,\n",
    "        force_http2=False\n",
    "    ) as fetch, DB(DB_PATH) as db:\n",
    "\n",
    "        async def handle_one(idx: int, site: str, platform: str, primary: str, kind: str):\n",
    "            label = f\"[A3] [{idx}] {site}\"\n",
    "            primary = (primary or \"\").strip()\n",
    "\n",
    "            # Validate primary; no guessing/alternates\n",
    "            if not re.match(r\"^https?://\", primary, re.I):\n",
    "                await db.merge_product_sitemaps(site, platform, primary, kind, [], 0, \"invalid primary sitemap URL\")\n",
    "                print(f\"{label} primary invalid: {primary!r}\", flush=True)\n",
    "                return\n",
    "\n",
    "            # Gentle pacing by platform\n",
    "            if platform == \"shopify\":\n",
    "                await asyncio.sleep(SHOPIFY_DELAY_SECONDS + random.uniform(0.0, 0.6))\n",
    "            elif platform == \"wordpress\":\n",
    "                await asyncio.sleep(WORDPRESS_DELAY_SECONDS + random.uniform(0.0, 0.4))\n",
    "            else:\n",
    "                await asyncio.sleep(0.3 + random.uniform(0.0, 0.3))\n",
    "\n",
    "            # Fetch exactly the stored primary\n",
    "            r = await fetch.get_with_backoff(primary, retries=3)\n",
    "            ct = r.headers.get(\"content-type\", \"\")\n",
    "            clen = r.headers.get(\"content-length\", \"\")\n",
    "            head = (r.text or \"\")[:200].replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            print(f\"{label} fetch: {primary} → {r.status_code} (ct={ct}, len={clen}) head='{head}'\", flush=True)\n",
    "\n",
    "            if r.status_code != 200 or not (r.text or \"\").strip():\n",
    "                await db.merge_product_sitemaps(site, platform, primary, kind, [], 0, f\"primary fetch failed: {r.status_code}\")\n",
    "                print(f\"{label} primary fetch failed: {r.status_code}\", flush=True)\n",
    "                return\n",
    "\n",
    "            # LLM-only extraction from this primary text\n",
    "            origin = _origin_from_url(primary)\n",
    "            products = await llm.extract_product_sitemaps(r.text, origin_base=origin)\n",
    "\n",
    "            # De-dupe; no heuristics\n",
    "            out, seen = [], set()\n",
    "            for u in (products or []):\n",
    "                u = (u or \"\").strip()\n",
    "                if u and u not in seen:\n",
    "                    out.append(u); seen.add(u)\n",
    "\n",
    "            if out:\n",
    "                await db.merge_product_sitemaps(site, platform, primary, kind, out, 1, \"ok\")\n",
    "                print(f\"{label} → product_sitemaps: {len(out)}\", flush=True)\n",
    "            else:\n",
    "                await db.merge_product_sitemaps(site, platform, primary, kind, [], 0, \"llm-empty\")\n",
    "                print(f\"{label} → product_sitemaps: 0 (llm-empty)\", flush=True)\n",
    "\n",
    "        tasks = [handle_one(i, s, p, sm, sk) for i, (s,p,sm,sk) in enumerate(todo, start=1)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"[Agent3] Product sitemap expansion complete.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a26bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 — Agent 4: collect product URLs from product_sitemaps (robust, keep-all fallback)\n",
    "\n",
    "import asyncio, json, os, re, time, textwrap, sqlite3, random\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from urllib.parse import urlsplit\n",
    "from lxml import etree\n",
    "\n",
    "# Tuning\n",
    "A4_CONCURRENCY    = int(os.environ.get(\"A4_CONCURRENCY\", \"6\"))\n",
    "A4_PER_HOST       = int(os.environ.get(\"A4_PER_HOST\", \"1\"))\n",
    "A4_SKIP_IF_DONE   = os.environ.get(\"A4_SKIP_IF_DONE\", \"1\") == \"1\"\n",
    "A4_WRITE_NDJSON   = os.environ.get(\"A4_WRITE_NDJSON\", \"0\") == \"1\"\n",
    "# Empty default = keep ALL URLs (no products-only filter)\n",
    "A4_URL_INCLUDE_RE = os.environ.get(\"A4_URL_INCLUDE_REGEX\", \"\").strip()\n",
    "A4_FOLLOW_CHILD   = os.environ.get(\"A4_FOLLOW_CHILDREN\", \"0\") == \"1\"\n",
    "# If a fetched \"sitemap\" is actually HTML (not XML), store the URL itself as a single product URL\n",
    "A4_TREAT_NONXML_AS_SINGLE = os.environ.get(\"A4_TREAT_NONXML_AS_SINGLE\", \"1\") == \"1\"\n",
    "\n",
    "def _regex_keep() -> Optional[re.Pattern]:\n",
    "    \"\"\"Return a compiled regex if provided, else None (no filtering).\"\"\"\n",
    "    return re.compile(A4_URL_INCLUDE_RE, re.I) if A4_URL_INCLUDE_RE else None\n",
    "\n",
    "def _gzip_unwrap(b: bytes) -> bytes:\n",
    "    if len(b) >= 2 and b[:2] == b\"\\x1f\\x8b\":\n",
    "        import gzip\n",
    "        try:\n",
    "            return gzip.decompress(b)\n",
    "        except Exception:\n",
    "            return b\n",
    "    return b\n",
    "\n",
    "def _safe_xml_root(xml_text: str) -> Optional[etree._Element]:\n",
    "    \"\"\"Try to parse XML and return the root, or None on failure.\"\"\"\n",
    "    try:\n",
    "        parser = etree.XMLParser(recover=True, huge_tree=True)\n",
    "        return etree.fromstring(xml_text.encode(\"utf-8\", errors=\"ignore\"), parser=parser)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _looks_like_xml_sitemap(text: str, content_type: str) -> bool:\n",
    "    \"\"\"Heuristic: is this response likely an XML sitemap (urlset or sitemapindex)?\"\"\"\n",
    "    ct = (content_type or \"\").lower()\n",
    "    if \"xml\" in ct:\n",
    "        return True\n",
    "    # quick sniff in the first ~2KB\n",
    "    head = text[:2048].lower()\n",
    "    return (\"<urlset\" in head) or (\"<sitemapindex\" in head)\n",
    "\n",
    "def _parse_urlset(xml_text: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Return list of {url,lastmod,changefreq,images_json} from a standard <urlset>.\n",
    "    If it's not a <urlset>, returns [].\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    root = _safe_xml_root(xml_text)\n",
    "    if root is None:\n",
    "        return out\n",
    "\n",
    "    # ensure we're looking at a urlset-like doc\n",
    "    ln = etree.QName(root.tag).localname.lower() if hasattr(root, \"tag\") else \"\"\n",
    "    if ln != \"urlset\":\n",
    "        return out\n",
    "\n",
    "    urls = root.xpath(\"//*[local-name()='url']\")\n",
    "    for node in urls:\n",
    "        locs = node.xpath(\"./*[local-name()='loc']/text()\")\n",
    "        if not locs:\n",
    "            continue\n",
    "        loc = (locs[0] or \"\").strip()\n",
    "        lastmod  = (node.xpath(\"./*[local-name()='lastmod']/text()\") or [None])[0]\n",
    "        changefq = (node.xpath(\"./*[local-name()='changefreq']/text()\") or [None])[0]\n",
    "        imgs: List[Dict[str, Any]] = []\n",
    "        for img in node.xpath(\".//*[local-name()='image'] | .//*[local-name()='image:image']\"):\n",
    "            iloc   = (img.xpath(\"./*[local-name()='loc']/text()\") or [None])[0]\n",
    "            ititle = (img.xpath(\"./*[local-name()='title']/text()\") or [None])[0]\n",
    "            icap   = (img.xpath(\"./*[local-name()='caption']/text()\") or [None])[0]\n",
    "            if iloc:\n",
    "                imgs.append({\"loc\": iloc, \"title\": ititle, \"caption\": icap})\n",
    "        out.append({\n",
    "            \"url\": loc,\n",
    "            \"lastmod\": (lastmod or \"\").strip() or None,\n",
    "            \"changefreq\": (changefq or \"\").strip() or None,\n",
    "            \"images_json\": json.dumps(imgs, ensure_ascii=False) if imgs else \"\",\n",
    "        })\n",
    "    return out\n",
    "\n",
    "def _parse_child_sitemaps(xml_text: str) -> List[str]:\n",
    "    \"\"\"When a product sitemap is itself an index; disabled by default unless A4_FOLLOW_CHILD=1.\"\"\"\n",
    "    kids: List[str] = []\n",
    "    root = _safe_xml_root(xml_text)\n",
    "    if root is None:\n",
    "        return kids\n",
    "    ln = etree.QName(root.tag).localname.lower()\n",
    "    if ln != \"sitemapindex\":\n",
    "        return kids\n",
    "    for loc in root.xpath(\"//*[local-name()='sitemap']/*[local-name()='loc']/text()\"):\n",
    "        u = (loc or \"\").strip()\n",
    "        if u:\n",
    "            kids.append(u)\n",
    "    return kids\n",
    "\n",
    "def _host(u: str) -> str:\n",
    "    try:\n",
    "        return urlsplit(u).netloc\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "async def _a4_load_tasks() -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Return list of (site, platform, sitemap_url) to process.\n",
    "    Only from product_sitemaps where status=1, urls_count>0.\n",
    "    If A4_SKIP_IF_DONE=1, skip sitemaps already processed successfully.\n",
    "    \"\"\"\n",
    "    tasks: List[Tuple[str, str, str]] = []\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        cur = conn.execute(\"\"\"\n",
    "          SELECT site, COALESCE(platform,'other'), product_sitemaps_json\n",
    "          FROM product_sitemaps\n",
    "          WHERE status=1 AND urls_count>0 AND product_sitemaps_json IS NOT NULL\n",
    "        \"\"\")\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        done = set()\n",
    "        if A4_SKIP_IF_DONE:\n",
    "            done = {\n",
    "                r[0] for r in conn.execute(\n",
    "                    \"SELECT sitemap_url FROM product_urls_sitemaps WHERE status=1 AND urls_found>0\"\n",
    "                ).fetchall()\n",
    "            }\n",
    "\n",
    "        for site, platform, arr_json in rows:\n",
    "            try:\n",
    "                arr = json.loads(arr_json) if arr_json else []\n",
    "            except Exception:\n",
    "                arr = []\n",
    "            for sm in arr:\n",
    "                su = (sm or \"\").strip()\n",
    "                if not su:\n",
    "                    continue\n",
    "                if A4_SKIP_IF_DONE and su in done:\n",
    "                    continue\n",
    "                tasks.append(((site or \"\").rstrip('/'), (platform or \"other\").lower(), su))\n",
    "    return tasks\n",
    "\n",
    "async def agent4_collect_product_urls():\n",
    "    keep_re = _regex_keep()\n",
    "    tasks = await _a4_load_tasks()\n",
    "    if not tasks:\n",
    "        print(\"[A4] Nothing to do (no new product sitemaps).\", flush=True)\n",
    "        return\n",
    "\n",
    "    async with PoliteFetcher(\n",
    "        global_concurrency=A4_CONCURRENCY,\n",
    "        per_host_concurrency=A4_PER_HOST,\n",
    "        force_http2=False\n",
    "    ) as fetch, DB(DB_PATH) as db:\n",
    "\n",
    "        async def handle_one(idx: int, site: str, platform: str, sm_url: str):\n",
    "            label = f\"[A4] [{idx}] {site} :: {sm_url}\"\n",
    "            try:\n",
    "                # platform pacing\n",
    "                if platform == \"shopify\":\n",
    "                    await asyncio.sleep(SHOPIFY_DELAY_SECONDS + random.uniform(0.0, 0.6))\n",
    "                elif platform == \"wordpress\":\n",
    "                    await asyncio.sleep(WORDPRESS_DELAY_SECONDS + random.uniform(0.0, 0.4))\n",
    "                else:\n",
    "                    await asyncio.sleep(0.35 + random.uniform(0.0, 0.3))\n",
    "\n",
    "                t0 = time.perf_counter()\n",
    "                r = await fetch.get_with_backoff(sm_url, retries=3)\n",
    "                body = _gzip_unwrap(r.content or b\"\")\n",
    "                try:\n",
    "                    text = body.decode(\"utf-8\")\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        text = body.decode(\"latin-1\")\n",
    "                    except Exception:\n",
    "                        text = body.decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "                head = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "                print(f\"{label} → {r.status_code} ct={r.headers.get('content-type','')} len={len(body)} head='{textwrap.shorten(head, 240)}'\", flush=True)\n",
    "\n",
    "                if r.status_code != 200 or not text.strip():\n",
    "                    await db.mark_product_sitemap(\n",
    "                        sitemap_url=sm_url, site=site, platform=platform,\n",
    "                        http_status=r.status_code, content_type=r.headers.get(\"content-type\", \"\"),\n",
    "                        content_length=len(body), etag=r.headers.get(\"etag\", \"\"),\n",
    "                        last_modified=r.headers.get(\"last-modified\", \"\"),\n",
    "                        urls_found=0, status=0, reason=f\"fetch_failed:{r.status_code}\"\n",
    "                    )\n",
    "                    return\n",
    "\n",
    "                # Decide how to handle the payload\n",
    "                is_xml_sitemap = _looks_like_xml_sitemap(text, r.headers.get(\"content-type\", \"\"))\n",
    "\n",
    "                # Optionally follow child sitemaps if this is a <sitemapindex>\n",
    "                if is_xml_sitemap and A4_FOLLOW_CHILD:\n",
    "                    for child in _parse_child_sitemaps(text):\n",
    "                        tasks.append((site, platform, child))\n",
    "\n",
    "                entries: List[Dict[str, Any]] = []\n",
    "                if is_xml_sitemap:\n",
    "                    # Standard sitemap <urlset>\n",
    "                    entries = _parse_urlset(text)\n",
    "                else:\n",
    "                    # Not an XML sitemap (likely a product page) → store the URL itself\n",
    "                    if A4_TREAT_NONXML_AS_SINGLE:\n",
    "                        if (not keep_re) or (keep_re and keep_re.search(sm_url)):\n",
    "                            entries = [{\n",
    "                                \"url\": sm_url,\n",
    "                                \"lastmod\": None,\n",
    "                                \"changefreq\": None,\n",
    "                                \"images_json\": \"\",\n",
    "                            }]\n",
    "\n",
    "                # Keep ALL URLs by default; ensure 'url' exists\n",
    "                entries = [e for e in entries if e.get(\"url\")]\n",
    "\n",
    "                # Upsert URLs\n",
    "                n = await db.upsert_product_urls_bulk(\n",
    "                    site=site, platform=platform, sitemap_url=sm_url, rows=entries\n",
    "                )\n",
    "\n",
    "                # Optional sidecar NDJSON\n",
    "                if A4_WRITE_NDJSON and n > 0:\n",
    "                    nd_dir = os.path.join(OUT_DIR, \"products\")\n",
    "                    os.makedirs(nd_dir, exist_ok=True)\n",
    "                    nd_path = os.path.join(nd_dir, f\"{_host(sm_url)}.ndjson\")\n",
    "                    with open(nd_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                        for e in entries:\n",
    "                            f.write(json.dumps({\n",
    "                                \"site\": site, \"platform\": platform,\n",
    "                                \"sitemap\": sm_url, \"url\": e[\"url\"],\n",
    "                                \"lastmod\": e.get(\"lastmod\"), \"changefreq\": e.get(\"changefreq\")\n",
    "                            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                dt_ms = int((time.perf_counter() - t0) * 1000)\n",
    "                await db.mark_product_sitemap(\n",
    "                    sitemap_url=sm_url, site=site, platform=platform,\n",
    "                    http_status=r.status_code, content_type=r.headers.get(\"content-type\", \"\"),\n",
    "                    content_length=len(body), etag=r.headers.get(\"etag\", \"\"),\n",
    "                    last_modified=r.headers.get(\"last-modified\", \"\"),\n",
    "                    urls_found=len(entries), status=1, reason=f\"ok,{dt_ms}ms\"\n",
    "                )\n",
    "                print(f\"{label} → URLs stored: {len(entries)}\", flush=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                # Never let one failure kill the whole batch\n",
    "                try:\n",
    "                    await db.mark_product_sitemap(\n",
    "                        sitemap_url=sm_url, site=site, platform=platform,\n",
    "                        http_status=0, content_type=\"\", content_length=0,\n",
    "                        etag=\"\", last_modified=\"\", urls_found=0, status=0,\n",
    "                        reason=f\"exception:{type(e).__name__}\"\n",
    "                    )\n",
    "                finally:\n",
    "                    print(f\"{label} → ERROR: {type(e).__name__}: {e}\", flush=True)\n",
    "\n",
    "        jobs = [handle_one(i, s, p, sm) for i, (s, p, sm) in enumerate(tasks, start=1)]\n",
    "        # Exceptions are handled inside handle_one; gather won't cancel siblings\n",
    "        await asyncio.gather(*jobs)\n",
    "\n",
    "    print(\"[Agent4] Product URL collection complete.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202a22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import sqlite3\n",
    "import aiohttp\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- Config ---\n",
    "DB_PATH = os.environ.get(\"DB_PATH\", r\"D:\\museai\\data\\db\\crawler_meta.db\")\n",
    "A4_SRC_TABLE = \"product_sitemaps\"\n",
    "A4_POP_TABLE = \"populated_product_sitemaps\"\n",
    "A4_LLM_MODEL = os.environ.get(\"A4_LLM_MODEL\", \"gpt-5-nano\")\n",
    "A4_CONCURRENCY = int(os.environ.get(\"A4_CONCURRENCY\", \"6\"))\n",
    "A4_LLM_CONCURRENCY = int(os.environ.get(\"A4_LLM_CONCURRENCY\", \"2\"))\n",
    "A4_LLM_MIN_DELAY_MS = int(os.environ.get(\"A4_LLM_MIN_DELAY_MS\", \"120\"))\n",
    "A4_LLM_MAX_RETRIES = int(os.environ.get(\"A4_LLM_MAX_RETRIES\", \"5\"))\n",
    "A4_MIN_CONFIDENCE = float(os.environ.get(\"A4_MIN_CONFIDENCE\", \"0.65\"))\n",
    "A4_OVERWRITE = os.environ.get(\"A4_OVERWRITE\", \"0\") == \"1\"\n",
    "HTTP_TIMEOUT = int(os.environ.get(\"A4_HTTP_TIMEOUT\", \"12\"))\n",
    "\n",
    "ALLOWED_CATS = [\n",
    "    \"Men\", \"Women\", \"Unisex\", \"Kidswear\",\n",
    "    \"Accessories\", \"Jewellery\", \"Footwear\", \"Lingerie\",\n",
    "    \"Beauty\", \"Home & Living\", \"Sportswear\", \"Electronics\", \"Other\"\n",
    "]\n",
    "PRODUCT_CATS = {\"Accessories\",\"Jewellery\",\"Footwear\",\"Lingerie\",\"Beauty\",\"Home & Living\",\"Sportswear\",\"Electronics\"}\n",
    "\n",
    "# --- DB helpers ---\n",
    "def _db(conn, sql, args=()):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, args)\n",
    "    conn.commit()\n",
    "    return cur\n",
    "\n",
    "def _get_columns(conn, table):\n",
    "    rows = conn.execute(f\"PRAGMA table_info({table})\").fetchall()\n",
    "    return [r[1] for r in rows]\n",
    "\n",
    "def _clone_populated_table_schema(conn):\n",
    "    _db(conn, f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {A4_POP_TABLE} AS\n",
    "        SELECT *,\n",
    "               CAST(NULL AS TEXT) AS website_for,\n",
    "               CAST(NULL AS TEXT) AS tags_json,\n",
    "               CAST(0 AS INTEGER) AS a4_status,\n",
    "               CAST(NULL AS REAL) AS a4_confidence,\n",
    "               CAST('' AS TEXT) AS a4_model,\n",
    "               datetime('now') AS updated_at\n",
    "        FROM {A4_SRC_TABLE} WHERE 0;\n",
    "    \"\"\")\n",
    "    for col, typ in [\n",
    "        (\"website_for\", \"TEXT\"),\n",
    "        (\"tags_json\", \"TEXT\"),\n",
    "        (\"a4_status\", \"INTEGER\"),\n",
    "        (\"a4_confidence\", \"REAL\"),\n",
    "        (\"a4_model\", \"TEXT\"),\n",
    "        (\"updated_at\", \"TEXT\"),\n",
    "    ]:\n",
    "        try:\n",
    "            _db(conn, f\"ALTER TABLE {A4_POP_TABLE} ADD COLUMN {col} {typ}\")\n",
    "        except sqlite3.OperationalError:\n",
    "            pass\n",
    "    _db(conn, f\"\"\"\n",
    "        CREATE UNIQUE INDEX IF NOT EXISTS idx_{A4_POP_TABLE}_site_psmap\n",
    "        ON {A4_POP_TABLE}(site, product_sitemap);\n",
    "    \"\"\")\n",
    "\n",
    "def _upsert_missing_rows(conn):\n",
    "    source_cols = _get_columns(conn, A4_SRC_TABLE)\n",
    "    target_cols = _get_columns(conn, A4_POP_TABLE)\n",
    "    shared_cols = [c for c in source_cols if c in target_cols]\n",
    "    cols_csv = \", \".join(shared_cols)\n",
    "    _db(conn, f\"\"\"\n",
    "        INSERT INTO {A4_POP_TABLE} ({cols_csv})\n",
    "        SELECT {cols_csv}\n",
    "        FROM {A4_SRC_TABLE} s\n",
    "        WHERE s.status=1\n",
    "          AND NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM {A4_POP_TABLE} p\n",
    "                WHERE p.site=s.site AND p.product_sitemap=s.product_sitemap\n",
    "          );\n",
    "    \"\"\")\n",
    "\n",
    "def _backfill_new_rows_from_existing(conn):\n",
    "    _db(conn, f\"\"\"\n",
    "        UPDATE {A4_POP_TABLE} AS p\n",
    "           SET website_for = (\n",
    "                   SELECT website_for FROM {A4_POP_TABLE}\n",
    "                    WHERE site=p.site AND a4_status=1 AND website_for IS NOT NULL\n",
    "                    ORDER BY updated_at DESC LIMIT 1\n",
    "               ),\n",
    "               tags_json = (\n",
    "                   SELECT tags_json FROM {A4_POP_TABLE}\n",
    "                    WHERE site=p.site AND a4_status=1 AND tags_json IS NOT NULL\n",
    "                    ORDER BY updated_at DESC LIMIT 1\n",
    "               ),\n",
    "               a4_status = 1,\n",
    "               a4_confidence = (\n",
    "                   SELECT a4_confidence FROM {A4_POP_TABLE}\n",
    "                    WHERE site=p.site AND a4_status=1 AND a4_confidence IS NOT NULL\n",
    "                    ORDER BY updated_at DESC LIMIT 1\n",
    "               ),\n",
    "               a4_model = (\n",
    "                   SELECT a4_model FROM {A4_POP_TABLE}\n",
    "                    WHERE site=p.site AND a4_status=1 AND a4_model IS NOT NULL\n",
    "                    ORDER BY updated_at DESC LIMIT 1\n",
    "               ),\n",
    "               updated_at = datetime('now')\n",
    "         WHERE (p.a4_status IS NULL OR p.a4_status=0)\n",
    "           AND EXISTS (SELECT 1 FROM {A4_POP_TABLE} q WHERE q.site=p.site AND q.a4_status=1);\n",
    "    \"\"\")\n",
    "\n",
    "def _sites_to_process(conn):\n",
    "    if A4_OVERWRITE:\n",
    "        rows = conn.execute(f\"\"\"\n",
    "            SELECT DISTINCT site\n",
    "            FROM {A4_SRC_TABLE}\n",
    "            WHERE status=1\n",
    "            ORDER BY site\n",
    "        \"\"\").fetchall()\n",
    "    else:\n",
    "        rows = conn.execute(f\"\"\"\n",
    "            SELECT DISTINCT s.site\n",
    "            FROM {A4_SRC_TABLE} s\n",
    "            WHERE s.status=1\n",
    "              AND NOT EXISTS (\n",
    "                    SELECT 1 FROM {A4_POP_TABLE} p\n",
    "                    WHERE p.site = s.site AND p.a4_status = 1\n",
    "              )\n",
    "            ORDER BY s.site\n",
    "        \"\"\").fetchall()\n",
    "    return [r[0] for r in rows]\n",
    "\n",
    "def _write_site_result(conn, site, website_for, tags, status, confidence, model_used):\n",
    "    tags_json = json.dumps(tags, ensure_ascii=False) if tags is not None else None\n",
    "    _db(conn, f\"\"\"\n",
    "        UPDATE {A4_POP_TABLE}\n",
    "           SET website_for = ?,\n",
    "               tags_json = ?,\n",
    "               a4_status = ?,\n",
    "               a4_confidence = ?,\n",
    "               a4_model = ?,\n",
    "               updated_at = datetime('now')\n",
    "         WHERE site = ?;\n",
    "    \"\"\", (website_for, tags_json, int(status or 0), confidence, model_used or \"\", site))\n",
    "\n",
    "# --- HTTP helpers ---\n",
    "def _normalize_home(site: str) -> str:\n",
    "    if not site:\n",
    "        return \"\"\n",
    "    s = site.strip()\n",
    "    if not s.startswith(\"http://\") and not s.startswith(\"https://\"):\n",
    "        s = \"https://\" + s\n",
    "    return s\n",
    "\n",
    "def _candidate_paths():\n",
    "    return [\"/collections/all\", \"/shop\", \"/products\"]\n",
    "\n",
    "def _is_html_like(ct: str, body: str) -> bool:\n",
    "    if ct and \"html\" in ct.lower():\n",
    "        return True\n",
    "    return \"<html\" in (body or \"\").lower()\n",
    "\n",
    "async def _one_get(session: aiohttp.ClientSession, url: str):\n",
    "    try:\n",
    "        async with session.get(url, allow_redirects=True) as resp:\n",
    "            ct = resp.headers.get(\"content-type\", \"\")\n",
    "            text = await resp.text(errors=\"ignore\")\n",
    "            return url, resp.status, ct, text, \"\"\n",
    "    except Exception as e:\n",
    "        return url, 0, \"\", \"\", f\"{type(e).__name__}: {e}\"\n",
    "\n",
    "async def _fetch_best_html(session_https: aiohttp.ClientSession,\n",
    "                           session_http: aiohttp.ClientSession,\n",
    "                           site: str):\n",
    "    host = site.replace(\"https://\", \"\").replace(\"http://\", \"\").strip(\"/\")\n",
    "    https_roots = [f\"https://{host}\", f\"https://www.{host}\"]\n",
    "    http_roots  = [f\"http://{host}\",  f\"http://www.{host}\"]\n",
    "\n",
    "    for base in https_roots + http_roots:\n",
    "        url, st, ct, body, err = await _one_get(session_https if base.startswith(\"https\") else session_http, base)\n",
    "        if st == 200 and _is_html_like(ct, body):\n",
    "            return url, st, ct, body, \"\"\n",
    "        if st in (403, 503) and (\"cloudflare\" in body.lower() or \"just a moment\" in body.lower()):\n",
    "            return url, st, ct, body, \"\"\n",
    "\n",
    "    base = https_roots[0]\n",
    "    tasks = [asyncio.create_task(_one_get(session_https, base.rstrip(\"/\") + p)) for p in _candidate_paths()]\n",
    "    done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED, timeout=HTTP_TIMEOUT)\n",
    "    for d in done:\n",
    "        url, st, ct, body, err = d.result()\n",
    "        if (st == 200 and _is_html_like(ct, body)) or (st in (403, 503) and (\"cloudflare\" in body.lower() or \"just a moment\" in body.lower())):\n",
    "            for p in pending: p.cancel()\n",
    "            return url, st, ct, body, \"\"\n",
    "    for p in pending:\n",
    "        try: p.cancel()\n",
    "        except Exception: pass\n",
    "    return \"\", 0, \"\", \"\", \"All attempts failed\"\n",
    "\n",
    "def _squash_html(html: str, max_chars=4000) -> str:\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"(?is)<script.*?>.*?</script>\", \" \", html)\n",
    "    text = re.sub(r\"(?is)<style.*?>.*?</style>\", \" \", text)\n",
    "    text = re.sub(r\"(?is)<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text[:max_chars]\n",
    "\n",
    "# --- Category rules ---\n",
    "def _enforce_single_main_category(raw_value):\n",
    "    if not raw_value:\n",
    "        return None\n",
    "    parts = [p.strip() for p in str(raw_value).split(\",\") if p.strip()]\n",
    "    parts = [p for p in parts if p in ALLOWED_CATS]\n",
    "    if not parts:\n",
    "        return None\n",
    "    for p in parts:\n",
    "        if p in PRODUCT_CATS:\n",
    "            return p\n",
    "    if \"Unisex\" in parts or (\"Men\" in parts and \"Women\" in parts):\n",
    "        return \"Unisex\"\n",
    "    if \"Kidswear\" in parts:\n",
    "        return \"Kidswear\"\n",
    "    if \"Men\" in parts:\n",
    "        return \"Men\"\n",
    "    if \"Women\" in parts:\n",
    "        return \"Women\"\n",
    "    return parts[0]\n",
    "\n",
    "# --- LLM plumbing (rate-limit safe) ---\n",
    "_llm_sem = asyncio.Semaphore(A4_LLM_CONCURRENCY)\n",
    "_llm_lock = asyncio.Lock()\n",
    "_last_llm_ts = 0.0\n",
    "\n",
    "async def _llm_throttle():\n",
    "    global _last_llm_ts\n",
    "    async with _llm_lock:\n",
    "        now = time.monotonic()\n",
    "        wait = max(0.0, (_last_llm_ts + A4_LLM_MIN_DELAY_MS/1000.0) - now)\n",
    "        if wait > 0:\n",
    "            await asyncio.sleep(wait)\n",
    "        _last_llm_ts = time.monotonic()\n",
    "\n",
    "def _extract_json_loose(s: str):\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r'\\{.*\\}', s, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def _llm_complete_json(messages):\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        use_modern = True\n",
    "    except Exception:\n",
    "        use_modern = False\n",
    "\n",
    "    backoff = 0.8\n",
    "    for attempt in range(1, A4_LLM_MAX_RETRIES + 1):\n",
    "        await _llm_sem.acquire()\n",
    "        try:\n",
    "            await _llm_throttle()\n",
    "            if use_modern:\n",
    "                try:\n",
    "                    resp = client.chat.completions.create(\n",
    "                        model=A4_LLM_MODEL,\n",
    "                        response_format={\"type\": \"json_object\"},\n",
    "                        messages=messages,\n",
    "                    )\n",
    "                    content = resp.choices[0].message.content\n",
    "                except Exception:\n",
    "                    resp = client.chat.completions.create(\n",
    "                        model=A4_LLM_MODEL,\n",
    "                        messages=messages,\n",
    "                    )\n",
    "                    content = resp.choices[0].message.content\n",
    "            else:\n",
    "                import openai\n",
    "                openai.api_key = api_key\n",
    "                resp = openai.ChatCompletion.create(model=A4_LLM_MODEL, messages=messages)\n",
    "                content = resp[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            try:\n",
    "                return json.loads(content)\n",
    "            except Exception:\n",
    "                data = _extract_json_loose(content)\n",
    "                if data is not None:\n",
    "                    return data\n",
    "                raise RuntimeError(\"LLM returned non-JSON\")\n",
    "        except Exception:\n",
    "            if attempt >= A4_LLM_MAX_RETRIES:\n",
    "                raise\n",
    "            base = backoff * (2 ** (attempt - 1))\n",
    "            jitter = 0.15 + 0.25 * (attempt % 3)\n",
    "            await asyncio.sleep(base + jitter)\n",
    "        finally:\n",
    "            _llm_sem.release()\n",
    "\n",
    "# --- Prompts & classifiers ---\n",
    "def _system_prompt():\n",
    "    return (\n",
    "        \"You are a precise catalog classifier for Indian fashion/e-commerce brands.\\n\"\n",
    "        \"Decide with NO guesswork. If insufficient evidence, set status=0.\\n\\n\"\n",
    "        \"Return exactly ONE main category for `website_for` using ONLY this set (case-sensitive):\\n\"\n",
    "        f\"{', '.join(ALLOWED_CATS)}\\n\\n\"\n",
    "        \"Selection rules:\\n\"\n",
    "        \"1) If the brand is primarily a product category (Footwear, Accessories, Jewellery, Lingerie, Beauty, Home & Living, Sportswear, Electronics), choose that ONLY.\\n\"\n",
    "        \"2) Otherwise, audience: Men, Women, Unisex, Kidswear.\\n\"\n",
    "        \"3) If you cannot determine confidently, set status=0.\\n\\n\"\n",
    "        \"Also provide 2–6 concise assortment `tags` (array of strings). No duplicates.\"\n",
    "    )\n",
    "\n",
    "def _examples_block():\n",
    "    e1 = \"\"\"{\n",
    "  \"status\": 1,\n",
    "  \"website_for\": \"Footwear\",\n",
    "  \"tags\": [\"Sneakers\", \"Casual Shoes\", \"Slip-ons\"],\n",
    "  \"confidence\": 0.9\n",
    "}\"\"\"\n",
    "    e2 = \"\"\"{\n",
    "  \"status\": 1,\n",
    "  \"website_for\": \"Unisex\",\n",
    "  \"tags\": [\"Streetwear\", \"Oversized T-shirts\", \"Hoodies\"],\n",
    "  \"confidence\": 0.86\n",
    "}\"\"\"\n",
    "    e0 = \"\"\"{\n",
    "  \"status\": 0,\n",
    "  \"website_for\": null,\n",
    "  \"tags\": [],\n",
    "  \"confidence\": 0.0\n",
    "}\"\"\"\n",
    "    return f\"Valid examples:\\n{e1}\\n\\n{e2}\\n\\nIf unsure:\\n{e0}\\n\"\n",
    "\n",
    "async def _classify_brand_only(site: str):\n",
    "    host = urlparse(_normalize_home(site)).netloc or site\n",
    "    user = (\n",
    "        f\"Brand/Homepage Host: {host}\\n\\n\"\n",
    "        \"No page content provided.\\n\\n\"\n",
    "        \"Output JSON ONLY (no code fences, no extra keys) with this schema:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"status\": 0 or 1,\\n'\n",
    "        '  \"website_for\": string from allowed set or null,\\n'\n",
    "        '  \"tags\": array of 2-6 short strings,\\n'\n",
    "        '  \"confidence\": float 0..1\\n'\n",
    "        \"}\\n\\n\"\n",
    "        f\"{_examples_block()}\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- ONE value for website_for only.\\n\"\n",
    "        \"- status=0 if unsure; do not guess.\\n\"\n",
    "        \"- Use your general knowledge of Indian brands if applicable; if still unsure, return status=0.\\n\"\n",
    "    )\n",
    "    msgs = [{\"role\": \"system\", \"content\": _system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": user}]\n",
    "    try:\n",
    "        data = await _llm_complete_json(msgs)\n",
    "    except Exception:\n",
    "        return dict(status=0, website_for=None, tags=[], confidence=None, model=A4_LLM_MODEL)\n",
    "\n",
    "    status = int(data.get(\"status\", 0))\n",
    "    website_for = _enforce_single_main_category(data.get(\"website_for\"))\n",
    "    tags = data.get(\"tags\") or []\n",
    "    if isinstance(tags, str):\n",
    "        try:\n",
    "            maybe = json.loads(tags)\n",
    "            tags = maybe if isinstance(maybe, list) else [t.strip() for t in tags.split(\",\") if t.strip()]\n",
    "        except Exception:\n",
    "            tags = [t.strip() for t in tags.split(\",\") if t.strip()]\n",
    "    if not isinstance(tags, list):\n",
    "        tags = []\n",
    "    tags = [str(t).strip() for t in tags if str(t).strip()]\n",
    "    if len(tags) > 6:\n",
    "        tags = tags[:6]\n",
    "    try:\n",
    "        conf = float(data.get(\"confidence\")) if data.get(\"confidence\") is not None else None\n",
    "    except Exception:\n",
    "        conf = None\n",
    "\n",
    "    if website_for not in ALLOWED_CATS:\n",
    "        status, website_for, tags, conf = 0, None, [], None\n",
    "    if status not in (0, 1):\n",
    "        status, website_for, tags, conf = 0, None, [], None\n",
    "\n",
    "    return dict(status=status, website_for=website_for, tags=tags, confidence=conf, model=A4_LLM_MODEL)\n",
    "\n",
    "async def _classify_with_html(site: str, homepage_html: str):\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        return dict(status=0, website_for=None, tags=[], confidence=None, model=A4_LLM_MODEL)\n",
    "\n",
    "    squashed = _squash_html(homepage_html, 4000)\n",
    "    host = urlparse(_normalize_home(site)).netloc or site\n",
    "    have_html = bool(squashed.strip())\n",
    "    html_block = f\"Homepage text (truncated):\\n{squashed}\\n\\n\" if have_html else \"Homepage text unavailable.\\n\\n\"\n",
    "\n",
    "    user = (\n",
    "        f\"Brand/Homepage Host: {host}\\n\\n\"\n",
    "        f\"{html_block}\"\n",
    "        \"Output JSON ONLY (no code fences, no extra keys) with this schema:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"status\": 0 or 1,\\n'\n",
    "        '  \"website_for\": string from allowed set or null,\\n'\n",
    "        '  \"tags\": array of 2-6 short strings,\\n'\n",
    "        '  \"confidence\": float 0..1\\n'\n",
    "        \"}\\n\\n\"\n",
    "        f\"{_examples_block()}\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- ONE value for website_for only.\\n\"\n",
    "        \"- status=0 if unsure; do not guess.\\n\"\n",
    "        \"- You MAY use both the provided text and your general knowledge; if still unsure, return status=0.\\n\"\n",
    "    )\n",
    "    msgs = [{\"role\": \"system\", \"content\": _system_prompt()},\n",
    "            {\"role\": \"user\", \"content\": user}]\n",
    "    try:\n",
    "        data = await _llm_complete_json(msgs)\n",
    "    except Exception:\n",
    "        return dict(status=0, website_for=None, tags=[], confidence=None, model=A4_LLM_MODEL)\n",
    "\n",
    "    status = int(data.get(\"status\", 0))\n",
    "    website_for = _enforce_single_main_category(data.get(\"website_for\"))\n",
    "    tags = data.get(\"tags\") or []\n",
    "    if isinstance(tags, str):\n",
    "        try:\n",
    "            maybe = json.loads(tags)\n",
    "            tags = maybe if isinstance(maybe, list) else [t.strip() for t in tags.split(\",\") if t.strip()]\n",
    "        except Exception:\n",
    "            tags = [t.strip() for t in tags.split(\",\") if t.strip()]\n",
    "    if not isinstance(tags, list):\n",
    "        tags = []\n",
    "    tags = [str(t).strip() for t in tags if str(t).strip()]\n",
    "    if len(tags) > 6:\n",
    "        tags = tags[:6]\n",
    "    try:\n",
    "        conf = float(data.get(\"confidence\")) if data.get(\"confidence\") is not None else None\n",
    "    except Exception:\n",
    "        conf = None\n",
    "\n",
    "    if website_for not in ALLOWED_CATS:\n",
    "        status, website_for, tags, conf = 0, None, [], None\n",
    "    if status not in (0, 1):\n",
    "        status, website_for, tags, conf = 0, None, [], None\n",
    "\n",
    "    return dict(status=status, website_for=website_for, tags=tags, confidence=conf, model=A4_LLM_MODEL)\n",
    "\n",
    "# --- Orchestrator ---\n",
    "async def agent4_populate_audience_tags():\n",
    "    print(\"[A4] Starting Agent 4 (LLM-first audience & tag classifier)…\", flush=True)\n",
    "\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        _clone_populated_table_schema(conn)\n",
    "        _upsert_missing_rows(conn)\n",
    "        _backfill_new_rows_from_existing(conn)\n",
    "        sites = _sites_to_process(conn)\n",
    "\n",
    "    if not sites:\n",
    "        print(\"[A4] Nothing to classify (either empty or already populated).\", flush=True)\n",
    "        return\n",
    "\n",
    "    headers = {\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.0.0 Safari/537.36\"),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-IN,en;q=0.9\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "    }\n",
    "    timeout = aiohttp.ClientTimeout(sock_connect=6, sock_read=10)\n",
    "    connector_https = aiohttp.TCPConnector(limit_per_host=6, ttl_dns_cache=300)\n",
    "    connector_http  = aiohttp.TCPConnector(ssl=False, limit_per_host=6, ttl_dns_cache=300)\n",
    "\n",
    "    sem_sites = asyncio.Semaphore(A4_CONCURRENCY)\n",
    "\n",
    "    async with aiohttp.ClientSession(headers=headers, timeout=timeout, connector=connector_https) as session_https, \\\n",
    "               aiohttp.ClientSession(headers=headers, timeout=timeout, connector=connector_http) as session_http:\n",
    "\n",
    "        async def handle_site(site):\n",
    "            async with sem_sites:\n",
    "                brand_only = await _classify_brand_only(site)\n",
    "                accepted = (brand_only[\"status\"] == 1 and\n",
    "                            brand_only.get(\"website_for\") in ALLOWED_CATS and\n",
    "                            (brand_only.get(\"confidence\") or 0) >= A4_MIN_CONFIDENCE)\n",
    "                if accepted:\n",
    "                    result = brand_only\n",
    "                    mode = \"brand-only\"\n",
    "                else:\n",
    "                    url, st, ct, body, err = await _fetch_best_html(session_https, session_http, site)\n",
    "                    if st == 0:\n",
    "                        print(f\"[A4] {site} fetch issue: status=0 ct='' err='{err}'\", flush=True)\n",
    "                    elif st != 200 or (ct and \"html\" not in ct.lower()):\n",
    "                        print(f\"[A4] {site} fetch non-HTML/blocked: status={st} ct='{ct}' url={url}\", flush=True)\n",
    "                    result = await _classify_with_html(site, body)\n",
    "                    mode = \"html-fallback\"\n",
    "\n",
    "                with sqlite3.connect(DB_PATH) as conn:\n",
    "                    _write_site_result(\n",
    "                        conn,\n",
    "                        site=site,\n",
    "                        website_for=result.get(\"website_for\"),\n",
    "                        tags=result.get(\"tags\"),\n",
    "                        status=result.get(\"status\", 0),\n",
    "                        confidence=result.get(\"confidence\"),\n",
    "                        model_used=result.get(\"model\"),\n",
    "                    )\n",
    "\n",
    "                label = result.get(\"website_for\") or \"UNKNOWN\"\n",
    "                conf  = result.get(\"confidence\")\n",
    "                print(f\"[A4] {site} [{mode}] → status={result.get('status',0)} website_for={label} conf={conf} tags={result.get('tags')}\", flush=True)\n",
    "\n",
    "        await asyncio.gather(*(handle_site(s) for s in sites))\n",
    "\n",
    "    print(\"[A4] Agent 4 done.\", flush=True)\n",
    "\n",
    "# --- Main (Agent 5 intentionally skipped) ---\n",
    "# def _run_main(coro):\n",
    "#     try:\n",
    "#         loop = asyncio.get_running_loop()\n",
    "#     except RuntimeError:\n",
    "#         loop = None\n",
    "#     if loop and loop.is_running():\n",
    "#         import nest_asyncio; nest_asyncio.apply()\n",
    "#         return asyncio.create_task(coro)\n",
    "#     else:\n",
    "#         asyncio.run(coro)\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     t = _run_main(agent4_populate_audience_tags())\n",
    "#     await t\n",
    "    \n",
    "\n",
    "# ---------------------- Main (Agent 5 temporarily skipped) ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08607751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef56db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A5 15:12:10] === Fresh start ===\n",
      "[A5 15:12:10] DB: D:\\museai\\data\\db\\crawler_meta.db\n",
      "[A5 15:12:10] Sites with direct products (kind='product'): 5 (items: 758)\n",
      "[A5 15:12:10] Sites with sitemaps from JSON:            124 (items: 241)\n",
      "[A5 15:12:10] Phase 1: writing direct products...\n",
      "[A5 15:12:10] [direct] https://www.clovia.com → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://theancestrystore.in → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://peelidori.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:10] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://www.melorra.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:11] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] [direct] https://rapawalk.com → +1\n",
      "[A5 15:12:12] Phase 2: crawling product sitemaps...\n",
      "[A5 15:12:13] [urlset] https://www.hilodesign.co https://www.hilodesign.co/sitemap_products_1.xml?from=4403619889252&to=9222995640563 → +501\n",
      "[A5 15:12:15] [urlset] https://themadrastrunk.com https://themadrastrunk.com/sitemap_products_1.xml?from=7133227384903&to=7278228439111 → +890\n",
      "[A5 15:12:17] [urlset] https://shopmulmul.com https://shopmulmul.com/sitemap_products_1.xml?from=1962782654515&to=7488799473715 → +2501\n",
      "[A5 15:12:20] [urlset] https://www.sassafras.in https://sassafras.in/sitemap_products_1.xml?from=5353296756886&to=9018560643373 → +2501\n",
      "[A5 15:12:22] [urlset] https://www.thebtclub.com https://www.thebtclub.com/sitemap_products_1.xml?from=4793829523540&to=9974666592569 → +1467\n",
      "[A5 15:12:22] [urlset] https://www.exhalelabel.com https://www.exhalelabel.com/sitemap_products_1.xml?from=5569149042845&to=12029917593918 → +88\n",
      "[A5 15:12:23] [urlset] https://shopmulmul.com https://shopmulmul.com/sitemap_products_2.xml?from=7488799506483&to=7669839593523 → +911\n",
      "[A5 15:12:26] [urlset] https://www.sassafras.in https://sassafras.in/sitemap_products_3.xml?from=9902685487405&to=10107104559405 → +2150\n",
      "[A5 15:12:26] [urlset] https://www.hilodesign.co https://www.hilodesign.co/en-au/sitemap_products_1.xml?from=4403619889252&to=9222995640563 → +501\n",
      "[A5 15:12:29] [urlset] https://www.sassafras.in https://sassafras.in/sitemap_products_2.xml?from=9018560708909&to=9902685159725 → +2500\n",
      "[A5 15:12:30] [urlset] https://www.hilodesign.co https://www.hilodesign.co/en-us/sitemap_products_1.xml?from=4403619889252&to=9222995640563 → +501\n",
      "[A5 15:12:38] Summary: https://www.sassafras.in → +7151 products\n",
      "[A5 15:12:38] Summary: https://shopmulmul.com → +3412 products\n",
      "[A5 15:12:38] Summary: https://www.hilodesign.co → +1503 products\n",
      "[A5 15:12:38] Summary: https://www.thebtclub.com → +1467 products\n",
      "[A5 15:12:38] Summary: https://themadrastrunk.com → +890 products\n",
      "[A5 15:12:38] Summary: https://rapawalk.com → +389 products\n",
      "[A5 15:12:38] Summary: https://theancestrystore.in → +187 products\n",
      "[A5 15:12:38] Summary: https://www.melorra.com → +159 products\n",
      "[A5 15:12:38] Summary: https://www.exhalelabel.com → +88 products\n",
      "[A5 15:12:38] Summary: https://peelidori.com → +22 products\n",
      "[A5 15:12:38] Summary: https://www.clovia.com → +1 products\n",
      "[A5 15:12:54] Wrote snapshot → D:\\museai\\data\\exports\\A5_snapshot.json\n"
     ]
    }
   ],
   "source": [
    "# agent5_fresh_start_patched.ipynb cell\n",
    "# ------------------------------------------------------------\n",
    "# Agent 5 — Fresh-start, simple & loud progress. (PATCHED)\n",
    "# Reads:  populated_product_sitemaps (status=1)\n",
    "# Uses:   ONLY product_sitemaps_json\n",
    "# Skips:  website_for LIKE '%Kidswear%'\n",
    "# Writes: product_urls (upsert) + a5_processed_candidates (resume)\n",
    "# Exports: D:\\museai\\data\\exports\\A5_snapshot.json\n",
    "#\n",
    "# Patch highlights:\n",
    "# - Fixes the \"https:// <gap>\" bug via aggressive whitespace stripping.\n",
    "# - Tries www/no-www variants automatically for sitemap fetches.\n",
    "# - Adds WordPress discovery: wp-sitemap.xml, sitemap_index.xml, wp-sitemap-posts-product-*.xml,\n",
    "#   Yoast/RankMath product maps (product-sitemap*.xml).\n",
    "# - Site-level fallback: if a site has < MIN_GOOD (default=5) product URLs in DB after Phase 2,\n",
    "#   we run discovery (robots/common/WP) and ingest from those sitemaps to \"replace\" the weak coverage\n",
    "#   with good product URLs (non-destructive upsert).\n",
    "# - Keeps your concurrency + heartbeat style and your table layout.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os, re, json, time, random, asyncio, sqlite3, aiohttp\n",
    "import nest_asyncio; nest_asyncio.apply()\n",
    "from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode, urljoin\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# ==============================\n",
    "# Config (override via env if needed)\n",
    "# ==============================\n",
    "DB_PATH = os.getenv(\"CRAWLER_DB\", os.getenv(\"DB_PATH\", r\"D:\\museai\\data\\db\\crawler_meta.db\"))\n",
    "\n",
    "POP_TABLE = os.getenv(\"A5_POP_TABLE\", \"populated_product_sitemaps\")\n",
    "PRODUCT_URLS_TABLE = os.getenv(\"A5_PRODUCT_URLS_TABLE\", \"product_urls\")\n",
    "PROCESSED_TABLE = os.getenv(\"A5_PROCESSED_TABLE\", \"a5_processed_candidates\")\n",
    "\n",
    "EXPORT_DIR = os.getenv(\"A5_EXPORT_DIR\", r\"D:\\museai\\data\\exports\")\n",
    "SNAPSHOT_FILE = os.path.join(EXPORT_DIR, \"A5_snapshot.json\")\n",
    "\n",
    "# Pacing (balanced: visible progress, low 429s)\n",
    "A5_CONCURRENCY       = int(os.getenv(\"A5_CONCURRENCY\", \"8\"))\n",
    "A5_PER_HOST_MIN_S    = float(os.getenv(\"A5_PER_HOST_MIN_S\", \"0.9\"))\n",
    "A5_MAX_RETRIES       = int(os.getenv(\"A5_MAX_RETRIES\", \"3\"))\n",
    "A5_CONNECT_TIMEOUT_S = float(os.getenv(\"A5_CONNECT_TIMEOUT_S\", \"12\"))\n",
    "A5_READ_TIMEOUT_S    = float(os.getenv(\"A5_READ_TIMEOUT_S\", \"25\"))\n",
    "# Fallback (for heavy/slow sitemaps only)\n",
    "A5_FALLBACK_CONNECT_S = float(os.getenv(\"A5_FALLBACK_CONNECT_S\", \"20\"))\n",
    "A5_FALLBACK_READ_S    = float(os.getenv(\"A5_FALLBACK_READ_S\", \"90\"))\n",
    "\n",
    "\n",
    "# Heartbeat\n",
    "HEARTBEAT_SEC        = int(os.getenv(\"A5_HEARTBEAT_SEC\", \"20\"))\n",
    "\n",
    "# Filters\n",
    "SKIP_WEBSITE_FOR = os.getenv(\"A5_SKIP_WEBSITE_FOR\", \"Kidswear\").lower().strip()\n",
    "ONLY_SITES = {s.strip().lower() for s in os.getenv(\"A5_ONLY_SITES\", \"\").split(\",\") if s.strip()}\n",
    "SKIP_SITES = {s.strip().lower() for s in os.getenv(\"A5_SKIP_SITES\", \"\").split(\",\") if s.strip()}\n",
    "\n",
    "# Minimum \"good\" URLs per site; if below this we run discovery fallback\n",
    "MIN_GOOD = int(os.getenv(\"A5_MIN_GOOD\", \"5\"))\n",
    "\n",
    "_HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/127.0.0.0 Safari/537.36\"),\n",
    "    \"Accept\": \"application/xml,text/xml,application/xhtml+xml,text/html;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "}\n",
    "\n",
    "TRANSIENT_STATUSES = {429, 503}  # skip now, retry on next run\n",
    "FORBIDDEN_STATUSES = {403}\n",
    "\n",
    "# ==============================\n",
    "# Logging\n",
    "# ==============================\n",
    "def log(msg: str):\n",
    "    print(f\"[A5 {datetime.now().strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "# ==============================\n",
    "# FS & DB\n",
    "# ==============================\n",
    "def ensure_dirs():\n",
    "    os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "def connect_db() -> sqlite3.Connection:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    conn.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    conn.execute(\"PRAGMA foreign_keys=ON;\")\n",
    "    return conn\n",
    "\n",
    "def table_columns(conn: sqlite3.Connection, table: str) -> Dict[str, str]:\n",
    "    cols = {}\n",
    "    for cid, name, ctype, notnull, dflt, pk in conn.execute(f\"PRAGMA table_info({table});\"):\n",
    "        cols[name] = (ctype or \"\").upper()\n",
    "    return cols\n",
    "\n",
    "def migrate_add_missing_columns(conn: sqlite3.Connection, table: str, required: Dict[str, str]):\n",
    "    existing = table_columns(conn, table)\n",
    "    for col, decl in required.items():\n",
    "        if col not in existing:\n",
    "            conn.execute(f\"ALTER TABLE {table} ADD COLUMN {decl};\")\n",
    "    conn.commit()\n",
    "\n",
    "def ensure_product_urls_table(conn: sqlite3.Connection):\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PRODUCT_URLS_TABLE}(\n",
    "            site TEXT NOT NULL,\n",
    "            url TEXT NOT NULL,\n",
    "            source TEXT NOT NULL DEFAULT '',\n",
    "            is_required INTEGER NOT NULL DEFAULT 1,\n",
    "            discovered_at TEXT NOT NULL,\n",
    "            PRIMARY KEY(site, url)\n",
    "        );\n",
    "    \"\"\")\n",
    "    migrate_add_missing_columns(conn, PRODUCT_URLS_TABLE, {\n",
    "        \"source\":       \"source TEXT NOT NULL DEFAULT ''\",\n",
    "        \"is_required\":  \"is_required INTEGER NOT NULL DEFAULT 1\",\n",
    "        \"discovered_at\":\"discovered_at TEXT\"\n",
    "    })\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{PRODUCT_URLS_TABLE}_site ON {PRODUCT_URLS_TABLE}(site);\")\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{PRODUCT_URLS_TABLE}_src ON {PRODUCT_URLS_TABLE}(source);\")\n",
    "    conn.commit()\n",
    "\n",
    "def ensure_processed_table(conn: sqlite3.Connection):\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {PROCESSED_TABLE}(\n",
    "            site TEXT NOT NULL,\n",
    "            url TEXT NOT NULL,\n",
    "            processed_ok INTEGER,\n",
    "            status INTEGER,\n",
    "            note TEXT NOT NULL DEFAULT '',\n",
    "            processed_at TEXT,\n",
    "            PRIMARY KEY(site, url)\n",
    "        );\n",
    "    \"\"\")\n",
    "    migrate_add_missing_columns(conn, PROCESSED_TABLE, {\n",
    "        \"processed_ok\": \"processed_ok INTEGER\",\n",
    "        \"status\":       \"status INTEGER\",\n",
    "        \"note\":         \"note TEXT NOT NULL DEFAULT ''\",\n",
    "        \"processed_at\": \"processed_at TEXT\"\n",
    "    })\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{PROCESSED_TABLE}_site ON {PROCESSED_TABLE}(site);\")\n",
    "    conn.commit()\n",
    "\n",
    "def has_products_for_source(conn: sqlite3.Connection, site: str, source: str) -> bool:\n",
    "    if not source:\n",
    "        return False\n",
    "    cur = conn.execute(f\"SELECT 1 FROM {PRODUCT_URLS_TABLE} WHERE site=? AND source=? LIMIT 1;\", (site, source))\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "def already_processed(conn: sqlite3.Connection, site: str, url: str) -> bool:\n",
    "    if not url:\n",
    "        return False\n",
    "    # normalize url key\n",
    "    url = canonicalize_url(url)\n",
    "    cur = conn.execute(f\"SELECT 1 FROM {PROCESSED_TABLE} WHERE site=? AND url=? LIMIT 1;\", (site, url))\n",
    "    if cur.fetchone() is not None:\n",
    "        return True\n",
    "    if has_products_for_source(conn, site, url):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def mark_processed(conn: sqlite3.Connection, site: str, url: str, ok: int, status: int, note: str):\n",
    "    url = canonicalize_url(url)\n",
    "    conn.execute(f\"\"\"\n",
    "        INSERT INTO {PROCESSED_TABLE}(site, url, processed_ok, status, note, processed_at)\n",
    "        VALUES(?,?,?,?,?,datetime('now'))\n",
    "        ON CONFLICT(site, url) DO UPDATE SET\n",
    "            processed_ok=excluded.processed_ok,\n",
    "            status=excluded.status,\n",
    "            note=excluded.note,\n",
    "            processed_at=excluded.processed_at;\n",
    "    \"\"\", (site, url, ok, status, note))\n",
    "    conn.commit()\n",
    "\n",
    "def upsert_product_url(conn: sqlite3.Connection, site: str, url: str, source: str, is_required: int = 1):\n",
    "    if not url:\n",
    "        return\n",
    "    url = canonicalize_url(url)\n",
    "    source = canonicalize_url(source or site)\n",
    "    conn.execute(f\"\"\"\n",
    "        INSERT INTO {PRODUCT_URLS_TABLE}(site, url, source, is_required, discovered_at)\n",
    "        VALUES(?,?,?,?,datetime('now'))\n",
    "        ON CONFLICT(site, url) DO UPDATE SET\n",
    "            source=excluded.source,\n",
    "            is_required=excluded.is_required,\n",
    "            discovered_at=excluded.discovered_at;\n",
    "    \"\"\", (site.rstrip(\"/\"), url, source or site, int(is_required)))\n",
    "    conn.commit()\n",
    "\n",
    "def count_products_for_site(conn: sqlite3.Connection, site: str) -> int:\n",
    "    cur = conn.execute(f\"SELECT COUNT(1) FROM {PRODUCT_URLS_TABLE} WHERE site=?\", (site,))\n",
    "    row = cur.fetchone()\n",
    "    return int(row[0] or 0)\n",
    "\n",
    "# ==============================\n",
    "# URL utils\n",
    "# ==============================\n",
    "_SPACE_RE = re.compile(r\"\\s+\")\n",
    "_DROP_PARAMS = {\"utm_source\",\"utm_medium\",\"utm_campaign\",\"utm_term\",\"utm_content\",\n",
    "                \"gclid\",\"fbclid\",\"igshid\",\"mc_cid\",\"mc_eid\",\"yclid\",\"pid\",\"cid\",\"affid\",\"ref\"}\n",
    "\n",
    "def strip_spaces(u: Optional[str]) -> str:\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    # aggressive: remove all whitespace anywhere (fixes \"https:// kisah.in\")\n",
    "    return _SPACE_RE.sub(\"\", str(u))\n",
    "\n",
    "def canonicalize_url(url: Optional[str]) -> str:\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    if not isinstance(url, str):\n",
    "        url = str(url)\n",
    "    try:\n",
    "        url = strip_spaces(url).strip()\n",
    "        p = urlsplit(url)\n",
    "        qs = parse_qsl(p.query, keep_blank_values=True)\n",
    "        qs = [(k, v) for (k, v) in qs\n",
    "              if k not in _DROP_PARAMS and not k.lower().startswith(\"utm_\") and k.lower() != \"variant\"]\n",
    "        new_q = urlencode(qs, doseq=True)\n",
    "        path = re.sub(r\"/+\", \"/\", p.path)\n",
    "        # strip trailing slash except root\n",
    "        if path != \"/\" and path.endswith(\"/\"):\n",
    "            path = path[:-1]\n",
    "        return urlunsplit((p.scheme or \"https\", p.netloc.lower(), path, new_q, \"\"))  # strip fragment\n",
    "    except Exception:\n",
    "        return strip_spaces(url).strip()\n",
    "\n",
    "def host_of(u: Optional[str]) -> str:\n",
    "    try:\n",
    "        return urlsplit(strip_spaces(u) or \"\").netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def same_site(url: Optional[str], site: Optional[str]) -> bool:\n",
    "    h1, h2 = host_of(url), host_of(site)\n",
    "    if not h1 or not h2:\n",
    "        return False\n",
    "    return (h1 == h2) or h1.endswith(\".\" + h2) or h2.endswith(\".\" + h1)\n",
    "\n",
    "def toggle_www(u: str) -> str:\n",
    "    u = canonicalize_url(u)\n",
    "    p = urlsplit(u)\n",
    "    host = p.netloc\n",
    "    if host.startswith(\"www.\"):\n",
    "        host = host[4:]\n",
    "    else:\n",
    "        host = \"www.\" + host\n",
    "    return urlunsplit((p.scheme or \"https\", host, p.path or \"/\", p.query, \"\"))\n",
    "\n",
    "_PRODUCT_URL_RE = re.compile(\n",
    "    r\"(/products?/)|\"\n",
    "    r\"(/collections/[^/]+/products/)|\"\n",
    "    r\"(/p/)|(/item/)|(/shop/[^/?#]+$)|\"\n",
    "    r\"(/product-[^/?#]+$)|\"\n",
    "    r\"(\\.html?\\b)\",\n",
    "    re.I\n",
    ")\n",
    "def looks_like_product_page_url(url: str) -> bool:\n",
    "    return bool(url and _PRODUCT_URL_RE.search(url))\n",
    "\n",
    "# ==============================\n",
    "# XML parsing\n",
    "# ==============================\n",
    "def strip_ns(tag: str) -> str:\n",
    "    return tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n",
    "\n",
    "def parse_xml(text: str) -> Tuple[str, List[str]]:\n",
    "    try:\n",
    "        root = ET.fromstring(text)\n",
    "    except Exception:\n",
    "        return \"\", []\n",
    "    tag = strip_ns(root.tag).lower()\n",
    "    locs: List[str] = []\n",
    "    if tag == \"sitemapindex\":\n",
    "        for sm in root.iter():\n",
    "            if strip_ns(sm.tag).lower() == \"loc\" and sm.text:\n",
    "                locs.append(sm.text.strip())\n",
    "        return \"sitemapindex\", locs\n",
    "    elif tag == \"urlset\":\n",
    "        for el in root.iter():\n",
    "            if strip_ns(el.tag).lower() == \"loc\" and el.text:\n",
    "                locs.append(el.text.strip())\n",
    "        return \"urlset\", locs\n",
    "    return \"\", []\n",
    "\n",
    "def xmlish(text: str, ctype: str) -> bool:\n",
    "    if ctype and \"xml\" in ctype.lower():\n",
    "        return True\n",
    "    head = (text or \"\")[:1024].lower()\n",
    "    return (\"<urlset\" in head) or (\"<sitemapindex\" in head) or head.strip().startswith(\"<?xml\")\n",
    "\n",
    "# ==============================\n",
    "# Throttle / HTTP\n",
    "# ==============================\n",
    "class DomainThrottler:\n",
    "    def __init__(self, min_gap_s: float):\n",
    "        self.min_gap_s = max(0.0, float(min_gap_s))\n",
    "        self._locks: Dict[str, asyncio.Lock] = {}\n",
    "        self._last: Dict[str, float] = {}\n",
    "\n",
    "    async def wait(self, host: str):\n",
    "        if host not in self._locks:\n",
    "            self._locks[host] = asyncio.Lock()\n",
    "        async with self._locks[host]:\n",
    "            now = time.monotonic()\n",
    "            last = self._last.get(host, 0.0)\n",
    "            wait_s = self.min_gap_s - (now - last)\n",
    "            if wait_s > 0:\n",
    "                await asyncio.sleep(wait_s + random.uniform(0.05, 0.25))\n",
    "            self._last[host] = time.monotonic()\n",
    "\n",
    "async def fetch_text(session: aiohttp.ClientSession, url: str) -> Tuple[int, str, str]:\n",
    "    \"\"\"\n",
    "    Basic fetch (kept for compatibility). Use get_xmlish() wrapper for sitemap work.\n",
    "    \"\"\"\n",
    "    url = canonicalize_url(url)\n",
    "    backoff = 0.9\n",
    "    status, ctype, text = 0, \"\", \"\"\n",
    "    for attempt in range(1, A5_MAX_RETRIES + 1):\n",
    "        try:\n",
    "            async with session.get(url, headers=_HEADERS, allow_redirects=True) as resp:\n",
    "                status = resp.status\n",
    "                ctype = resp.headers.get(\"content-type\",\"\")\n",
    "                raw = await resp.read()\n",
    "                try:    text = raw.decode(\"utf-8\")\n",
    "                except: text = raw.decode(\"latin-1\", errors=\"ignore\")\n",
    "                if status in TRANSIENT_STATUSES and attempt < A5_MAX_RETRIES:\n",
    "                    await asyncio.sleep(backoff * attempt + random.uniform(0, 0.6))\n",
    "                    continue\n",
    "                return status, ctype, text\n",
    "        except Exception:\n",
    "            if attempt < A5_MAX_RETRIES:\n",
    "                await asyncio.sleep(backoff * attempt + random.uniform(0, 0.6))\n",
    "                continue\n",
    "            return 0, \"\", \"\"\n",
    "    return status, ctype, text\n",
    "\n",
    "async def get_xmlish(session: aiohttp.ClientSession, url: str) -> Tuple[str, int, str, str]:\n",
    "    \"\"\"\n",
    "    Try url, then www/no-www variant. Returns (final_url, status, ctype, text).\n",
    "    \"\"\"\n",
    "    url = canonicalize_url(url)\n",
    "    status, ctype, text = await fetch_text(session, url)\n",
    "    if status == 200 and xmlish(text, ctype):\n",
    "        return url, status, ctype, text\n",
    "    # try toggle variant\n",
    "    alt = toggle_www(url)\n",
    "    if alt != url:\n",
    "        status2, ctype2, text2 = await fetch_text(session, alt)\n",
    "        if status2 == 200 and xmlish(text2, ctype2):\n",
    "            return alt, status2, ctype2, text2\n",
    "        # If original was OK (even if not xml), prefer original outcome\n",
    "        if status == 200:\n",
    "            return url, status, ctype, text\n",
    "        return alt, status2, ctype2, text2\n",
    "    return url, status, ctype, text\n",
    "\n",
    "# ==============================\n",
    "# Discovery helpers (fallback when site < MIN_GOOD)\n",
    "# ==============================\n",
    "async def robots_sitemaps(session: aiohttp.ClientSession, site: str) -> List[str]:\n",
    "    base = canonicalize_url(site.rstrip(\"/\"))\n",
    "    robots = urljoin(base + \"/\", \"robots.txt\")\n",
    "    u, status, ctype, text = await get_xmlish(session, robots)  # may not be XML, but we only want text\n",
    "    out: List[str] = []\n",
    "    if status == 200 and text:\n",
    "        for line in text.splitlines():\n",
    "            if line.lower().startswith(\"sitemap:\"):\n",
    "                raw = line.split(\":\", 1)[1].strip()\n",
    "                if raw:\n",
    "                    out.append(canonicalize_url(raw))\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "async def common_sitemaps(session: aiohttp.ClientSession, site: str) -> List[str]:\n",
    "    base = canonicalize_url(site.rstrip(\"/\"))\n",
    "    candidates = [\n",
    "        \"sitemap.xml\",\n",
    "        \"sitemap_index.xml\",\n",
    "        \"sitemap-products.xml\",\n",
    "        \"sitemap_product.xml\",\n",
    "        \"sitemap_products_1.xml\",\n",
    "        \"product-sitemap.xml\",\n",
    "        \"product-sitemap1.xml\",\n",
    "    ]\n",
    "    out: List[str] = []\n",
    "    for path in candidates:\n",
    "        sm = urljoin(base + \"/\", path)\n",
    "        final, status, ctype, text = await get_xmlish(session, sm)\n",
    "        if status == 200 and xmlish(text, ctype):\n",
    "            out.append(final)\n",
    "    # dedupe\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "async def wordpress_sitemaps(session: aiohttp.ClientSession, site: str) -> List[str]:\n",
    "    base = canonicalize_url(site.rstrip(\"/\"))\n",
    "    seeds = [\n",
    "        urljoin(base + \"/\", \"wp-sitemap.xml\"),\n",
    "        urljoin(base + \"/\", \"sitemap_index.xml\"),\n",
    "        urljoin(base + \"/\", \"product-sitemap.xml\"),\n",
    "        urljoin(base + \"/\", \"product-sitemap1.xml\"),\n",
    "    ]\n",
    "    productish: List[str] = []\n",
    "    seen = set()\n",
    "    for sm in seeds:\n",
    "        final, status, ctype, text = await get_xmlish(session, sm)\n",
    "        if status != 200 or not xmlish(text, ctype):\n",
    "            continue\n",
    "        tag, locs = parse_xml(text)\n",
    "        if tag == \"sitemapindex\":\n",
    "            for loc in locs:\n",
    "                ll = (loc or \"\").lower()\n",
    "                if any(k in ll for k in (\"product\",\"wc-product\",\"tulaproduct\",\"woo\",\"wp-sitemap-posts-product\")):\n",
    "                    loc_c = canonicalize_url(loc)\n",
    "                    if loc_c not in seen:\n",
    "                        seen.add(loc_c); productish.append(loc_c)\n",
    "        elif tag == \"urlset\":\n",
    "            if any(k in final.lower() for k in (\"product\",\"tulaproduct\",\"wp-sitemap-posts-\")):\n",
    "                loc_c = canonicalize_url(final)\n",
    "                if loc_c not in seen:\n",
    "                    seen.add(loc_c); productish.append(loc_c)\n",
    "    # Fallback: collect all wp-sitemap-posts-* urlsets (we'll filter URLs later)\n",
    "    final, status, ctype, text = await get_xmlish(session, urljoin(base + \"/\", \"wp-sitemap.xml\"))\n",
    "    if status == 200 and xmlish(text, ctype):\n",
    "        tag, locs = parse_xml(text)\n",
    "        for loc in locs:\n",
    "            if \"wp-sitemap-posts-\" in (loc or \"\"):\n",
    "                loc_c = canonicalize_url(loc)\n",
    "                if loc_c not in seen:\n",
    "                    seen.add(loc_c); productish.append(loc_c)\n",
    "    return productish\n",
    "\n",
    "# ==============================\n",
    "# Core processing\n",
    "# ==============================\n",
    "async def process_urlset_into_products(conn: sqlite3.Connection, site: str, urlset_url: str,\n",
    "                                       locs: List[str], source_hint: str) -> int:\n",
    "    urls = [canonicalize_url(u) for u in locs if u and same_site(u, site)]\n",
    "    # If the urlset filename doesn't scream \"product\", filter by path\n",
    "    need_filter = not any(k in urlset_url.lower() for k in (\"product\", \"products\", \"wp-sitemap-posts-product\"))\n",
    "    chosen = [u for u in urls if looks_like_product_page_url(u)] if need_filter else urls\n",
    "    added = 0\n",
    "    for pu in chosen:\n",
    "        upsert_product_url(conn, site, pu, source=source_hint, is_required=1)\n",
    "        added += 1\n",
    "    return added\n",
    "\n",
    "async def process_sitemap(conn: sqlite3.Connection, site: str, sitemap_url: str,\n",
    "                          session: aiohttp.ClientSession,\n",
    "                          throttler: DomainThrottler,\n",
    "                          stats: Dict[str,int],\n",
    "                          added_by_site: Dict[str,int]):\n",
    "    if already_processed(conn, site, sitemap_url):\n",
    "        stats[\"skipped\"] += 1\n",
    "        return\n",
    "\n",
    "    host = host_of(sitemap_url)\n",
    "    if host:\n",
    "        await throttler.wait(host)\n",
    "\n",
    "    final_url, status, ctype, text = await get_xmlish(session, sitemap_url)\n",
    "\n",
    "    if status in TRANSIENT_STATUSES or status == 0:\n",
    "        # skip for now; resume next run\n",
    "        log(f\"[skip] {site} {final_url} → transient {status}\")\n",
    "        return\n",
    "    if status in FORBIDDEN_STATUSES or status >= 400:\n",
    "        mark_processed(conn, site, final_url, 0, status, \"unusable\")\n",
    "        stats[\"processed\"] += 1\n",
    "        log(f\"[bad] {site} {final_url} → status={status} unusable\")\n",
    "        return\n",
    "\n",
    "    if xmlish(text, ctype):\n",
    "        tag, locs = parse_xml(text)\n",
    "        locs = [canonicalize_url(u) for u in locs if u]\n",
    "        if tag == \"sitemapindex\":\n",
    "            children = [u for u in locs if same_site(u, site)]\n",
    "            grabbed = 0\n",
    "            for child in children:\n",
    "                if already_processed(conn, site, child):\n",
    "                    continue\n",
    "                ch = host_of(child)\n",
    "                if ch:\n",
    "                    await throttler.wait(ch)\n",
    "                c_final, s2, ct2, t2 = await get_xmlish(session, child)\n",
    "                if s2 in TRANSIENT_STATUSES or s2 == 0:\n",
    "                    log(f\"[skip-child] {site} {c_final} → transient {s2}\")\n",
    "                    continue\n",
    "                if s2 == 200 and xmlish(t2, ct2):\n",
    "                    tag2, urls2 = parse_xml(t2)\n",
    "                    urls2 = [u for u in urls2 if same_site(u, site)]\n",
    "                    if tag2 == \"urlset\":\n",
    "                        got = await process_urlset_into_products(conn, site, c_final, urls2, source_hint=c_final)\n",
    "                        grabbed += got\n",
    "                        mark_processed(conn, site, c_final, 1, 200, f\"urlset:{got}\")\n",
    "                    else:\n",
    "                        mark_processed(conn, site, c_final, 1, 200, f\"{tag2}\")\n",
    "                else:\n",
    "                    if s2 in FORBIDDEN_STATUSES or s2 >= 400:\n",
    "                        mark_processed(conn, site, c_final, 0, s2, \"child-unusable\")\n",
    "            mark_processed(conn, site, final_url, 1, 200, f\"sitemapindex:{len(children)}\")\n",
    "            stats[\"processed\"] += 1\n",
    "            stats[\"added\"] += grabbed\n",
    "            added_by_site[site] += grabbed\n",
    "            log(f\"[index] {site} {final_url} → children={len(children)} products={grabbed}\")\n",
    "            return\n",
    "\n",
    "        elif tag == \"urlset\":\n",
    "            got = await process_urlset_into_products(conn, site, final_url, locs, source_hint=final_url)\n",
    "            mark_processed(conn, site, final_url, 1, 200, f\"urlset:{got}\")\n",
    "            stats[\"processed\"] += 1\n",
    "            stats[\"added\"] += got\n",
    "            added_by_site[site] += got\n",
    "            log(f\"[urlset] {site} {final_url} → +{got}\")\n",
    "            return\n",
    "\n",
    "    # Non-XML\n",
    "    mark_processed(conn, site, final_url, 0, status, \"unusable\")\n",
    "    stats[\"processed\"] += 1\n",
    "    log(f\"[bad] {site} {final_url} → status={status} unusable\")\n",
    "\n",
    "# ==============================\n",
    "# FINAL PROBE (helper)\n",
    "# ==============================\n",
    "async def probe_sitemap(conn, site: str, sm_url: str, session: aiohttp.ClientSession, throttler: DomainThrottler) -> int:\n",
    "    host = host_of(sm_url)\n",
    "    if host:\n",
    "        await throttler.wait(host)\n",
    "    final, status, ctype, text = await get_xmlish(session, sm_url)\n",
    "    if status != 200 or not xmlish(text, ctype):\n",
    "        return 0\n",
    "    tag, locs = parse_xml(text)\n",
    "    if tag != \"urlset\" or not locs:\n",
    "        return 0\n",
    "    chosen = [u for u in locs if looks_like_product_page_url(u) and same_site(u, site)]\n",
    "    added = 0\n",
    "    for u in chosen:\n",
    "        upsert_product_url(conn, site, u, source=sm_url, is_required=1)\n",
    "        added += 1\n",
    "    return added\n",
    "\n",
    "# ==============================\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "# PROBE-style fetchers (httpx → aiohttp → curl) for fallback\n",
    "# ==============================\n",
    "import gzip, subprocess, shutil\n",
    "\n",
    "HEADERS_ROTATION = [\n",
    "    {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/127.0.0.0 Safari/127.0.0.0\"),\n",
    "        \"Accept\": \"application/xml,text/xml,application/xhtml+xml,text/html;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Cache-Control\": \"no-cache\",\n",
    "        \"Pragma\": \"no-cache\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"DNT\": \"1\",\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) \"\n",
    "                       \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15\"),\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"DNT\": \"1\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def site_base(u: str) -> str:\n",
    "    p = urlsplit(u or \"\")\n",
    "    return (p.scheme or \"https\").lower() + \"://\" + (p.netloc or \"\").lower()\n",
    "\n",
    "def canonical_keep_query(u: str) -> str:\n",
    "    if not u: return \"\"\n",
    "    p = urlsplit(strip_spaces(u))\n",
    "    path = re.sub(r\"/+\", \"/\", p.path or \"/\")\n",
    "    return urlunsplit((p.scheme or \"https\", (p.netloc or \"\").lower(), path, p.query, \"\"))\n",
    "\n",
    "async def _fetch_text_httpx(url: str):\n",
    "    try:\n",
    "        import httpx\n",
    "    except Exception as e:\n",
    "        return None, f\"httpx-missing:{type(e).__name__}\", \"\"\n",
    "    timeout = httpx.Timeout(connect=A5_CONNECT_TIMEOUT_S, read=A5_READ_TIMEOUT_S, write=30.0, pool=30.0)\n",
    "    async with httpx.AsyncClient(http2=True, timeout=timeout, verify=True, trust_env=True, follow_redirects=True) as client:\n",
    "        for attempt in range(1, A5_MAX_RETRIES+1):\n",
    "            for headers in HEADERS_ROTATION:\n",
    "                try:\n",
    "                    r = await client.get(url, headers=headers)\n",
    "                    content = r.content\n",
    "                    try:    text = content.decode(\"utf-8\")\n",
    "                    except: text = content.decode(\"latin-1\", errors=\"ignore\")\n",
    "                    if r.status_code in TRANSIENT_STATUSES and attempt < A5_MAX_RETRIES:\n",
    "                        await asyncio.sleep(0.7*attempt + random.uniform(0,0.5))\n",
    "                        continue\n",
    "                    return r.status_code, r.headers.get(\"content-type\",\"\"), text\n",
    "                except Exception as e:\n",
    "                    last = f\"httpx:{type(e).__name__}:{str(e)[:120]}\"\n",
    "                    if attempt < A5_MAX_RETRIES:\n",
    "                        await asyncio.sleep(0.5*attempt + random.uniform(0,0.5))\n",
    "                        continue\n",
    "                    return None, last, \"\"\n",
    "    return None, \"httpx-unknown\", \"\"\n",
    "\n",
    "async def _fetch_text_aiohttp(url: str):\n",
    "    timeout = aiohttp.ClientTimeout(total=None, connect=A5_CONNECT_TIMEOUT_S, sock_read=A5_READ_TIMEOUT_S)\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        for attempt in range(1, A5_MAX_RETRIES+1):\n",
    "            for headers in HEADERS_ROTATION:\n",
    "                try:\n",
    "                    async with session.get(url, headers=headers, allow_redirects=True) as resp:\n",
    "                        raw = await resp.read()\n",
    "                        ctype = resp.headers.get(\"content-type\",\"\")\n",
    "                        if (urlsplit(url).path or \"\").lower().endswith(\".gz\"):\n",
    "                            try:\n",
    "                                raw = gzip.decompress(raw)\n",
    "                                ctype = \"application/xml\"\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                        try:    text = raw.decode(\"utf-8\")\n",
    "                        except: text = raw.decode(\"latin-1\", errors=\"ignore\")\n",
    "                        if resp.status in TRANSIENT_STATUSES and attempt < A5_MAX_RETRIES:\n",
    "                            await asyncio.sleep(0.7*attempt + random.uniform(0,0.5))\n",
    "                            continue\n",
    "                        return resp.status, ctype, text\n",
    "                except Exception as e:\n",
    "                    last = f\"aiohttp:{type(e).__name__}:{str(e)[:120]}\"\n",
    "                    if attempt < A5_MAX_RETRIES:\n",
    "                        await asyncio.sleep(0.5*attempt + random.uniform(0,0.5))\n",
    "                        continue\n",
    "                    return None, last, \"\"\n",
    "    return None, \"aiohttp-unknown\", \"\"\n",
    "\n",
    "def _fetch_text_curl_blocking(url: str):\n",
    "    if not shutil.which(\"curl\"):\n",
    "        return None, \"curl-missing\", \"\"\n",
    "    for headers in HEADERS_ROTATION:\n",
    "        args = [\"curl\",\"-LsS\",\"--compressed\",\"--max-time\",str(int(A5_CONNECT_TIMEOUT_S + A5_READ_TIMEOUT_S))]\n",
    "        for k,v in headers.items():\n",
    "            args += [\"-H\", f\"{k}: {v}\"]\n",
    "        args += [url]\n",
    "        try:\n",
    "            cp = subprocess.run(args, capture_output=True, check=False)\n",
    "            body = cp.stdout or b\"\"\n",
    "            try:    text = body.decode(\"utf-8\")\n",
    "            except: text = body.decode(\"latin-1\", errors=\"ignore\")\n",
    "            ctype = \"application/xml\" if xmlish(text, \"application/xml\") else \"\"\n",
    "            if xmlish(text, ctype):\n",
    "                return 200, ctype, text\n",
    "            return 0, ctype, text\n",
    "        except Exception as e:\n",
    "            return None, f\"curl:{type(e).__name__}:{str(e)[:120]}\", \"\"\n",
    "\n",
    "class _ProbeThrottle:\n",
    "    def __init__(self, gap=A5_PER_HOST_MIN_S):\n",
    "        self.gap = max(0.0, float(gap))\n",
    "        self._locks = {}\n",
    "        self._last  = {}\n",
    "\n",
    "    async def wait(self, url: str):\n",
    "        host = urlsplit(url).netloc.lower()\n",
    "        if host not in self._locks:\n",
    "            self._locks[host] = asyncio.Lock()\n",
    "        async with self._locks[host]:\n",
    "            now = time.monotonic()\n",
    "            last = self._last.get(host, 0.0)\n",
    "            wait = self.gap - (now - last)\n",
    "            if wait > 0:\n",
    "                await asyncio.sleep(wait + random.uniform(0.05, 0.25))\n",
    "            self._last[host] = time.monotonic()\n",
    "\n",
    "async def _fetch_text_probe(url: str, throttler: _ProbeThrottle):\n",
    "    await throttler.wait(url)\n",
    "    s, ctype, text = await _fetch_text_httpx(url)\n",
    "    if s is None or s >= 400 or not xmlish(text, ctype or \"\"):\n",
    "        s2, c2, t2 = await _fetch_text_aiohttp(url)\n",
    "        if s2 and s2 < 400 and xmlish(t2, c2 or \"\"):\n",
    "            return s2, c2, t2\n",
    "        s3, c3, t3 = await asyncio.to_thread(_fetch_text_curl_blocking, url)\n",
    "        return (s3 or 0), (c3 or \"\"), (t3 or \"\")\n",
    "    return (s or 0), (ctype or \"\"), (text or \"\")\n",
    "\n",
    "def _load_exact_sitemaps_for_site(conn: sqlite3.Connection, site: str) -> List[str]:\n",
    "    base = site_base(site)\n",
    "    rows = conn.execute(f\"\"\"\n",
    "        SELECT site, COALESCE(product_sitemaps_json,'')\n",
    "        FROM {POP_TABLE}\n",
    "        WHERE status=1\n",
    "    \"\"\").fetchall()\n",
    "    out: List[str] = []\n",
    "    for row_site, pjson in rows:\n",
    "        if not row_site or not pjson:\n",
    "            continue\n",
    "        if not same_site(canonicalize_url(row_site), base):\n",
    "            continue\n",
    "        urls: List[str] = []\n",
    "        try:\n",
    "            data = json.loads(pjson)\n",
    "            if isinstance(data, list):\n",
    "                urls = [canonical_keep_query(u) for u in data if isinstance(u, str) and u.strip()]\n",
    "            elif isinstance(data, dict):\n",
    "                for key in (\"sitemaps\",\"product_sitemaps\",\"urls\",\"products\"):\n",
    "                    if key in data and isinstance(data[key], list):\n",
    "                        urls += [canonical_keep_query(u) for u in data[key] if isinstance(u, str) and u.strip()]\n",
    "        except Exception:\n",
    "            urls = []\n",
    "        out.extend(urls)\n",
    "    # de-dupe and keep only same-site\n",
    "    out = [u for u in dict.fromkeys(out) if same_site(u, base)]\n",
    "    return out\n",
    "\n",
    "async def _harvest_products_from_sitemap(site: str, sm_url: str, throttler: _ProbeThrottle) -> List[str]:\n",
    "    status, ctype, txt = await _fetch_text_probe(sm_url, throttler)\n",
    "    if status in FORBIDDEN_STATUSES:\n",
    "        log(f\"[fallback] {site} [{status}] forbidden → {sm_url}\")\n",
    "        return []\n",
    "    if status == 0 and not txt:\n",
    "        log(f\"[fallback] {site} [0] network error → {sm_url}\")\n",
    "        return []\n",
    "    if not xmlish(txt, ctype or \"\"):\n",
    "        preview = (txt or \"\")[:120].replace(\"\\n\",\" \")\n",
    "        log(f\"[fallback] {site} [{status}] not-xml/empty → {sm_url} body='{preview}'\")\n",
    "        return []\n",
    "\n",
    "    tag, locs = parse_xml(txt)\n",
    "    locs = [canonical_keep_query(u) for u in (locs or []) if u]\n",
    "    products: List[str] = []\n",
    "\n",
    "    if tag == \"sitemapindex\":\n",
    "        children = [u for u in locs if same_site(u, site)]\n",
    "        log(f\"[fallback] {site} index children={len(children)} ← {sm_url}\")\n",
    "        for child in children:\n",
    "            s2, ct2, t2 = await _fetch_text_probe(child, throttler)\n",
    "            if s2 in FORBIDDEN_STATUSES or not xmlish(t2, ct2 or \"\"):\n",
    "                continue\n",
    "            ttag, urls2 = parse_xml(t2)\n",
    "            if ttag == \"urlset\":\n",
    "                urls2 = [canonical_keep_query(u) for u in urls2 if u and same_site(u, site)]\n",
    "                chosen = [u for u in urls2 if looks_like_product_page_url(u)] or urls2\n",
    "                products.extend(chosen)\n",
    "        return list(dict.fromkeys(products))\n",
    "\n",
    "    if tag == \"urlset\":\n",
    "        urls = [u for u in locs if same_site(u, site)]\n",
    "        chosen = [u for u in urls if looks_like_product_page_url(u)] or urls\n",
    "        log(f\"[fallback] {site} urlset +{len(chosen)} ← {sm_url}\")\n",
    "        return list(dict.fromkeys(chosen))\n",
    "\n",
    "    log(f\"[fallback] {site} unknown tag={tag} ← {sm_url}\")\n",
    "    return []\n",
    "\n",
    "# ==============================\n",
    "# Fallback discovery (patched with slow-session timings)\n",
    "\n",
    "# ==============================\n",
    "# ==============================\n",
    "# Site-level fallback using PROBE logic and exact DB sitemaps\n",
    "# ==============================\n",
    "async def fallback_discovery_for_site(conn: sqlite3.Connection, site: str,\n",
    "                                      session: aiohttp.ClientSession,   # unused but kept to match signature\n",
    "                                      throttler: DomainThrottler,       # unused but kept to match signature\n",
    "                                      stats: Dict[str,int],\n",
    "                                      added_by_site: Dict[str,int]) -> int:\n",
    "    # 1) Load EXACT sitemaps saved for this site (no guessing)\n",
    "    sitemaps = _load_exact_sitemaps_for_site(conn, site)\n",
    "    if not sitemaps:\n",
    "        log(f\"[fallback] {site} → no exact sitemaps in DB; skipping\")\n",
    "        return 0\n",
    "\n",
    "    log(f\"[fallback] {site} → probing {len(sitemaps)} DB sitemap(s) with httpx→aiohttp→curl\")\n",
    "\n",
    "    gained = 0\n",
    "    per_host = _ProbeThrottle(gap=A5_PER_HOST_MIN_S)\n",
    "\n",
    "    # 2) Sequential per-site (polite), but we still keep overall agent concurrency elsewhere\n",
    "    for sm in sitemaps:\n",
    "        products = await _harvest_products_from_sitemap(site, sm, per_host)\n",
    "        if not products:\n",
    "            mark_processed(conn, site, sm, 0, 200, \"fallback-probe:0\")\n",
    "            continue\n",
    "        # 3) Upsert products (source=that sitemap) and mark processed\n",
    "        added_here = 0\n",
    "        for pu in products:\n",
    "            if same_site(pu, site):\n",
    "                upsert_product_url(conn, site, pu, source=sm, is_required=1)\n",
    "                gained += 1\n",
    "                added_here += 1\n",
    "        mark_processed(conn, site, sm, 1, 200, f\"fallback-probe:+{added_here}\")\n",
    "        stats[\"processed\"] += 1\n",
    "        stats[\"added\"] += added_here\n",
    "        added_by_site[site] += added_here\n",
    "\n",
    "    return gained\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Load candidates ONLY from product_sitemaps_json\n",
    "# ==============================\n",
    "def load_candidates(conn: sqlite3.Connection):\n",
    "    \"\"\"\n",
    "    Returns two lists:\n",
    "      direct_products:  List[(site, url)]             where sitemap_kind='product'\n",
    "      sitemap_sources:  List[(site, sitemap_url)]     other kinds (treat as product sitemaps)\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT site,\n",
    "               LOWER(COALESCE(sitemap_kind,'')) AS kind,\n",
    "               COALESCE(product_sitemaps_json,'') AS pjson,\n",
    "               COALESCE(website_for,'') AS website_for\n",
    "        FROM {POP_TABLE}\n",
    "        WHERE status = 1\n",
    "    \"\"\"\n",
    "    rows = conn.execute(sql).fetchall()\n",
    "\n",
    "    direct_products: List[Tuple[str, str]] = []\n",
    "    sitemap_sources: List[Tuple[str, str]] = []\n",
    "\n",
    "    for site, kind, pjson, website_for in rows:\n",
    "        if not site:\n",
    "            continue\n",
    "        if SKIP_WEBSITE_FOR and website_for and SKIP_WEBSITE_FOR in website_for.lower():\n",
    "            continue\n",
    "        s_norm = canonicalize_url(site.rstrip(\"/\"))\n",
    "        if ONLY_SITES and s_norm.lower() not in ONLY_SITES:\n",
    "            continue\n",
    "        if s_norm.lower() in SKIP_SITES:\n",
    "            continue\n",
    "\n",
    "        urls: List[str] = []\n",
    "        if pjson:\n",
    "            try:\n",
    "                data = json.loads(pjson)\n",
    "                if isinstance(data, list):\n",
    "                    urls = [canonicalize_url(u) for u in data if isinstance(u, str) and u.strip()]\n",
    "                elif isinstance(data, dict):\n",
    "                    for key in (\"sitemaps\",\"products\",\"product_sitemaps\",\"urls\"):\n",
    "                        if key in data and isinstance(data[key], list):\n",
    "                            urls.extend([canonicalize_url(u) for u in data[key] if isinstance(u, str) and u.strip()])\n",
    "            except Exception:\n",
    "                urls = []\n",
    "\n",
    "        if not urls:\n",
    "            continue\n",
    "\n",
    "        if (kind or \"\") == \"product\":\n",
    "            for u in urls:\n",
    "                cu = canonicalize_url(u)\n",
    "                if cu:\n",
    "                    direct_products.append((s_norm, cu))\n",
    "        else:\n",
    "            for u in urls:\n",
    "                cu = canonicalize_url(u)\n",
    "                if cu:\n",
    "                    sitemap_sources.append((s_norm, cu))\n",
    "\n",
    "    # de-dupe\n",
    "    direct_products = list(dict.fromkeys(direct_products))\n",
    "    sitemap_sources = list(dict.fromkeys(sitemap_sources))\n",
    "    return direct_products, sitemap_sources\n",
    "\n",
    "def choose_sitemaps_for_site(site: str, db_json: List[str], discovered: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Always prefer DB JSON product_sitemaps if present.\n",
    "    Otherwise fallback to discovered sitemaps.\n",
    "    \"\"\"\n",
    "    if db_json:\n",
    "        return db_json\n",
    "    return discovered\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Export (pretty JSON)\n",
    "# ==============================\n",
    "def export_snapshot():\n",
    "    ensure_dirs()\n",
    "    conn = connect_db()\n",
    "\n",
    "    # latest meta per site\n",
    "    rows = conn.execute(f\"\"\"\n",
    "        SELECT site, COALESCE(platform,''), COALESCE(website_for,'')\n",
    "        FROM {POP_TABLE}\n",
    "        WHERE status=1\n",
    "        ORDER BY COALESCE(updated_at, discovered_at, '') DESC\n",
    "    \"\"\").fetchall()\n",
    "    meta: Dict[str, Dict[str, str]] = {}\n",
    "    for site, platform, website_for in rows:\n",
    "        if site and site not in meta:\n",
    "            if SKIP_WEBSITE_FOR and website_for and SKIP_WEBSITE_FOR in website_for.lower():\n",
    "                continue\n",
    "            meta[site] = {\"platform\": platform or \"\", \"website_for\": website_for or \"\"}\n",
    "\n",
    "    rows = conn.execute(f\"\"\"\n",
    "        SELECT site,\n",
    "               CASE WHEN source IS NULL OR source=='' THEN site ELSE source END AS norm_source,\n",
    "               is_required,\n",
    "               url\n",
    "        FROM {PRODUCT_URLS_TABLE}\n",
    "        ORDER BY site, norm_source, url\n",
    "    \"\"\").fetchall()\n",
    "\n",
    "    grouped: Dict[str, Dict[str, Dict[str, Any]]] = defaultdict(lambda: defaultdict(lambda: {\n",
    "        \"is_required\": 1, \"products\": []\n",
    "    }))\n",
    "    for site, source, is_required, url in rows:\n",
    "        if site not in meta:\n",
    "            continue\n",
    "        grouped[site][source][\"is_required\"] = int(is_required or 1)\n",
    "        if url:\n",
    "            grouped[site][source][\"products\"].append(url)\n",
    "\n",
    "    # map kind for sources from POP (best effort)\n",
    "    kind_map: Dict[Tuple[str, str], str] = {}\n",
    "    rows = conn.execute(f\"\"\"\n",
    "        SELECT site, COALESCE(product_sitemaps_json,''), LOWER(COALESCE(sitemap_kind,'')) AS kind\n",
    "        FROM {POP_TABLE}\n",
    "        WHERE status=1\n",
    "    \"\"\").fetchall()\n",
    "    for site, pjson, kind in rows:\n",
    "        if not pjson:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(pjson)\n",
    "            urls = data if isinstance(data, list) else []\n",
    "            for u in urls:\n",
    "                if isinstance(u, str) and u.strip():\n",
    "                    kind_map[(site.rstrip(\"/\"), canonicalize_url(u.strip()))] = (kind or \"\").lower()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    out_sites = []\n",
    "    for site in sorted(grouped.keys()):\n",
    "        sitemaps_list = []\n",
    "        for source, bundle in grouped[site].items():\n",
    "            k = kind_map.get((site.rstrip(\"/\"), canonicalize_url(source)), \"\")\n",
    "            sitemaps_list.append({\n",
    "                \"sitemap_url\": source,\n",
    "                \"kind\": k or (\"product\" if canonicalize_url(source) == canonicalize_url(site) else \"\"),\n",
    "                \"is_required\": bundle[\"is_required\"],\n",
    "                \"product_count\": len(bundle[\"products\"]),\n",
    "                \"products\": bundle[\"products\"],\n",
    "            })\n",
    "        out_sites.append({\n",
    "            \"site\": site,\n",
    "            \"platform\": meta.get(site, {}).get(\"platform\", \"\"),\n",
    "            \"website_for\": meta.get(site, {}).get(\"website_for\", \"\"),\n",
    "            \"sitemaps\": sitemaps_list\n",
    "        })\n",
    "\n",
    "    payload = {\n",
    "        \"generated_at_utc\": datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"total_sites\": len(out_sites),\n",
    "        \"sites\": out_sites\n",
    "    }\n",
    "    with open(SNAPSHOT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "    log(f\"Wrote snapshot → {SNAPSHOT_FILE}\")\n",
    "\n",
    "# ==============================\n",
    "# Main\n",
    "# ==============================\n",
    "async def run_agent5():\n",
    "    ensure_dirs()\n",
    "    conn = connect_db()\n",
    "    ensure_product_urls_table(conn)\n",
    "    ensure_processed_table(conn)\n",
    "\n",
    "    # Load candidates only from product_sitemaps_json\n",
    "    direct_products, sitemap_sources = load_candidates(conn)\n",
    "\n",
    "    # Startup summary\n",
    "    per_site_direct = Counter([s for s, _ in direct_products])\n",
    "    per_site_maps   = Counter([s for s, _ in sitemap_sources])\n",
    "\n",
    "    log(\"=== Fresh start ===\")\n",
    "    log(f\"DB: {DB_PATH}\")\n",
    "    log(f\"Sites with direct products (kind='product'): {len(per_site_direct)} (items: {len(direct_products)})\")\n",
    "    log(f\"Sites with sitemaps from JSON:            {len(per_site_maps)} (items: {len(sitemap_sources)})\")\n",
    "    if ONLY_SITES: log(f\"ONLY_SITES={sorted(list(ONLY_SITES))}\")\n",
    "    if SKIP_SITES: log(f\"SKIP_SITES={sorted(list(SKIP_SITES))}\")\n",
    "\n",
    "    timeout   = aiohttp.ClientTimeout(total=None, connect=A5_CONNECT_TIMEOUT_S, sock_read=A5_READ_TIMEOUT_S)\n",
    "    throttler = DomainThrottler(A5_PER_HOST_MIN_S)\n",
    "    sem       = asyncio.Semaphore(A5_CONCURRENCY)\n",
    "\n",
    "    # progress counters\n",
    "    stats = {\n",
    "        \"processed\": 0,\n",
    "        \"added\": 0,\n",
    "        \"skipped\": 0\n",
    "    }\n",
    "    added_by_site: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "    async def heartbeat():\n",
    "        while True:\n",
    "            await asyncio.sleep(HEARTBEAT_SEC)\n",
    "            log(f\"Heartbeat: processed={stats['processed']} added={stats['added']} skipped={stats['skipped']}\")\n",
    "\n",
    "    async def process_direct(site: str, product_url: str):\n",
    "        # treat as trusted product (source=site)\n",
    "        upsert_product_url(conn, site, product_url, source=site, is_required=1)\n",
    "        mark_processed(conn, site, product_url, 1, 200, \"direct-from-json\")\n",
    "        stats[\"processed\"] += 1\n",
    "        stats[\"added\"] += 1\n",
    "        added_by_site[site] += 1\n",
    "        log(f\"[direct] {site} → +1\")\n",
    "\n",
    "    async def process_map(site: str, sitemap_url: str, session: aiohttp.ClientSession):\n",
    "        await process_sitemap(conn, site, sitemap_url, session, throttler, stats, added_by_site)\n",
    "\n",
    "    # Phase 1: push all direct-products first (fast visible output)\n",
    "    if direct_products:\n",
    "        log(\"Phase 1: writing direct products...\")\n",
    "        for site, url in direct_products:\n",
    "            await process_direct(site, url)\n",
    "\n",
    "    # Phase 2: crawl sitemaps (concurrent)\n",
    "    log(\"Phase 2: crawling product sitemaps...\")\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        async def _job(s, u):\n",
    "            async with sem:\n",
    "                await process_map(s, u, session)\n",
    "\n",
    "        # Heartbeat task\n",
    "        hb = asyncio.create_task(heartbeat())\n",
    "\n",
    "        chosen_sitemaps = []\n",
    "        for s, u in sitemap_sources:\n",
    "            chosen = choose_sitemaps_for_site(s, [u], [])  # DB JSON always wins\n",
    "            for cu in chosen:\n",
    "                chosen_sitemaps.append((s, cu))\n",
    "\n",
    "        tasks = [asyncio.create_task(_job(s, u))\n",
    "                for (s, u) in chosen_sitemaps]\n",
    "\n",
    "        if tasks:\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "        hb.cancel()\n",
    "        try:\n",
    "            await hb\n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "\n",
    "                # --------- Site-level fallback (always if needed, respectful) ---------\n",
    "        all_sites = sorted(set([s for s,_ in direct_products] + [s for s,_ in sitemap_sources]))\n",
    "        low_sites = [s for s in all_sites if count_products_for_site(conn, s) < MIN_GOOD]\n",
    "\n",
    "        if low_sites:\n",
    "            log(f\"[fallback-phase] sites_needing_help={len(low_sites)}\")\n",
    "            for s in low_sites:\n",
    "                gained = await fallback_discovery_for_site(conn, s, session, throttler, stats, added_by_site)\n",
    "                total = count_products_for_site(conn, s)\n",
    "                log(f\"[fallback-summary] {s} gained={gained} total_now={total}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Per-site summary\n",
    "    for site, n in sorted(added_by_site.items(), key=lambda kv: kv[1], reverse=True):\n",
    "        log(f\"Summary: {site} → +{n} products\")\n",
    "\n",
    "    export_snapshot()\n",
    "\n",
    "# ==============================\n",
    "# Runner\n",
    "# ==============================\n",
    "def _run_main(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        if loop.is_running():\n",
    "            return asyncio.create_task(coro)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    return asyncio.run(coro)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = _run_main(run_agent5())\n",
    "    if t is not None:\n",
    "        try:\n",
    "            await t\n",
    "        except NameError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b663a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A6 18:53:37] Selection saved → D:\\museai\\data\\a6\\selection\\run_20250916T185337Z_selection.json\n",
      "[A6 18:53:37] Chosen 15 site(s) to crawl (completed skipped, filled to TOP_N if possible):\n",
      "[A6 18:53:37]   #28 www.jaypore.com  products=24757  manual=False\n",
      "[A6 18:53:37]   #29 www.libas.in  products=17634  manual=False\n",
      "[A6 18:53:37]   #30 www.berrylush.com  products=15716  manual=False\n",
      "[A6 18:53:37]   #31 wforwoman.com  products=15074  manual=False\n",
      "[A6 18:53:37]   #32 www.koskii.com  products=9796  manual=False\n",
      "[A6 18:53:37]   #33 okhai.org  products=8928  manual=False\n",
      "[A6 18:53:37]   #34 www.superkicks.in  products=7301  manual=False\n",
      "[A6 18:53:37]   #35 www.kazo.com  products=6521  manual=False\n",
      "[A6 18:53:37]   #36 www.styched.in  products=6466  manual=False\n",
      "[A6 18:53:37]   #37 www.tjori.com  products=6106  manual=False\n",
      "[A6 18:53:37]   #38 suta.in  products=5071  manual=False\n",
      "[A6 18:53:37]   #39 torani.in  products=4949  manual=False\n",
      "[A6 18:53:37]   #40 www.campussutra.com  products=4333  manual=False\n",
      "[A6 18:53:37]   #41 sudathi.com  products=4326  manual=False\n",
      "[A6 18:53:37]   #42 www.jaipurkurti.com  products=3899  manual=False\n",
      "[A6 18:53:39] [www.jaypore.com] 1/24757 via=json-ld 200 → https://www.jaypore.com/p/-ceramic-wall-frames-39579699.html\n",
      "[A6 18:53:39] [www.jaypore.com] 2/24757 via=json-ld 200 → https://www.jaypore.com/p/-ceramic-wall-frames-39579700.html\n",
      "[A6 18:53:40] [www.jaypore.com] 3/24757 via=json-ld 200 → https://www.jaypore.com/p/-ceramic-wall-frames-39579701.html\n",
      "[A6 18:53:40] [www.jaypore.com] 4/24757 via=json-ld 200 → https://www.jaypore.com/p/-cotton-wall-frames-39576854.html\n",
      "[A6 18:53:40] [www.jaypore.com] 5/24757 via=json-ld 200 → https://www.jaypore.com/p/-cotton-wall-frames-39576867.html\n",
      "[A6 18:53:40] [www.jaypore.com] 6/24757 via=json-ld 200 → https://www.jaypore.com/p/-hand-crafted-rectangle-tables-39600678.html\n",
      "[A6 18:53:41] [www.jaypore.com] 7/24757 via=json-ld 200 → https://www.jaypore.com/p/-hand-crafted-square-tables-39600708.html\n",
      "[A6 18:53:41] [www.jaypore.com] 8/24757 via=json-ld 200 → https://www.jaypore.com/p/-hand-crafted-tables-39600677.html\n",
      "[A6 18:53:42] [www.jaypore.com] 9/24757 via=json-ld 200 → https://www.jaypore.com/p/-rectangle-tables-39600702.html\n",
      "[A6 18:53:42] [www.jaypore.com] 10/24757 via=json-ld 200 → https://www.jaypore.com/p/-round-tables-39600683.html\n",
      "[A6 18:53:42] [www.jaypore.com] 11/24757 via=json-ld 200 → https://www.jaypore.com/p/-round-tables-39600684.html\n",
      "[A6 18:53:42] [www.jaypore.com] 12/24757 via=json-ld 200 → https://www.jaypore.com/p/-round-tables-39600698.html\n",
      "[A6 18:53:43] [www.jaypore.com] 14/24757 via=json-ld 200 → https://www.jaypore.com/p/-square-tables-39600704.html\n",
      "[A6 18:53:43] [www.jaypore.com] 13/24757 via=json-ld 200 → https://www.jaypore.com/p/-round-tables-39600706.html\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1066\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m t\n\u001b[32m   1067\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1028\u001b[39m, in \u001b[36mrun_agent6\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1025\u001b[39m     log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[site] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m marked as DONE (manual). done=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(state[\u001b[33m'\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pending=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(state[\u001b[33m'\u001b[39m\u001b[33mpending\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1026\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m crawl_site(site, urls, site_meta.get(site, {}))\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# Mark completed if nothing pending\u001b[39;00m\n\u001b[32m   1031\u001b[39m state = read_json(os.path.join(site_dir(host), \u001b[33m\"\u001b[39m\u001b[33mstate.json\u001b[39m\u001b[33m\"\u001b[39m), default={})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 854\u001b[39m, in \u001b[36mcrawl_site\u001b[39m\u001b[34m(site, urls, meta)\u001b[39m\n\u001b[32m    852\u001b[39m workers = [asyncio.create_task(worker(i+\u001b[32m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(PER_HOST_WORKERS)]\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*workers)\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    856\u001b[39m     \u001b[38;5;66;03m# Ensure last persisted state even on early stop\u001b[39;00m\n\u001b[32m    857\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m persist_state_and_manifest()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 835\u001b[39m, in \u001b[36mcrawl_site.<locals>.worker\u001b[39m\u001b[34m(worker_id)\u001b[39m\n\u001b[32m    833\u001b[39m idx = total_target - q.qsize()  \u001b[38;5;66;03m# progress-ish\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m handle_one(u, idx)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    837\u001b[39m     q.task_done()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 763\u001b[39m, in \u001b[36mcrawl_site.<locals>.handle_one\u001b[39m\u001b[34m(u, idx)\u001b[39m\n\u001b[32m    760\u001b[39m     lm = manifest.get(pkey, {}).get(\u001b[33m\"\u001b[39m\u001b[33mlast_modified\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m A6_USE_CONDITIONAL_GET \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    762\u001b[39m     \u001b[38;5;66;03m# If you add a WP fallback later, call fetch_product_any here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     via, status, product, http_meta = \u001b[38;5;28;01mawait\u001b[39;00m fetch_shopify_product(session, u, etag=et, last_mod=lm)\n\u001b[32m    765\u001b[39m \u001b[38;5;66;03m# 403 handling & cap\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status == \u001b[32m403\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 657\u001b[39m, in \u001b[36mfetch_shopify_product\u001b[39m\u001b[34m(session, product_url, etag, last_mod)\u001b[39m\n\u001b[32m    654\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mshopify.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m200\u001b[39m, product, {\u001b[33m\"\u001b[39m\u001b[33metag\u001b[39m\u001b[33m\"\u001b[39m: hdrs.get(\u001b[33m\"\u001b[39m\u001b[33mETag\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mlast_modified\u001b[39m\u001b[33m\"\u001b[39m: hdrs.get(\u001b[33m\"\u001b[39m\u001b[33mLast-Modified\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# 2) HTML → JSON-LD → OG\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m status, hdrs, html_text = \u001b[38;5;28;01mawait\u001b[39;00m fetch_text(session, product_url, HEADERS_HTML,\n\u001b[32m    658\u001b[39m                                            cond_etag=etag, cond_lastmod=last_mod)\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status == \u001b[32m304\u001b[39m:\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnot-modified\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m304\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, {\u001b[33m\"\u001b[39m\u001b[33metag\u001b[39m\u001b[33m\"\u001b[39m: hdrs.get(\u001b[33m\"\u001b[39m\u001b[33mETag\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mlast_modified\u001b[39m\u001b[33m\"\u001b[39m: hdrs.get(\u001b[33m\"\u001b[39m\u001b[33mLast-Modified\u001b[39m\u001b[33m\"\u001b[39m)}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 391\u001b[39m, in \u001b[36mfetch_text\u001b[39m\u001b[34m(session, url, headers, timeout, max_retries, cond_etag, cond_lastmod)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m limiter.acquire(host)\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session.get(\n\u001b[32m    392\u001b[39m         url,\n\u001b[32m    393\u001b[39m         headers=use_headers,\n\u001b[32m    394\u001b[39m         timeout=aiohttp.ClientTimeout(total=timeout),\n\u001b[32m    395\u001b[39m         proxy=PROXY_URL,\n\u001b[32m    396\u001b[39m         allow_redirects=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    397\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m    398\u001b[39m         last_status = r.status\n\u001b[32m    399\u001b[39m         last_headers = \u001b[38;5;28mdict\u001b[39m(r.headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\museai\\data\\myenv\\Lib\\site-packages\\aiohttp\\client.py:1488\u001b[39m, in \u001b[36m_BaseRequestContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> _RetType:\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m     \u001b[38;5;28mself\u001b[39m._resp: _RetType = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._coro\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._resp.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\museai\\data\\myenv\\Lib\\site-packages\\aiohttp\\client.py:770\u001b[39m, in \u001b[36mClientSession._request\u001b[39m\u001b[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size, middlewares)\u001b[39m\n\u001b[32m    767\u001b[39m     handler = _connect_and_send_request\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m handler(req)\n\u001b[32m    771\u001b[39m \u001b[38;5;66;03m# Client connector errors should not be retried\u001b[39;00m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    773\u001b[39m     ConnectionTimeoutError,\n\u001b[32m    774\u001b[39m     ClientConnectorError,\n\u001b[32m    775\u001b[39m     ClientConnectorCertificateError,\n\u001b[32m    776\u001b[39m     ClientConnectorSSLError,\n\u001b[32m    777\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\museai\\data\\myenv\\Lib\\site-packages\\aiohttp\\client.py:748\u001b[39m, in \u001b[36mClientSession._request.<locals>._connect_and_send_request\u001b[39m\u001b[34m(req)\u001b[39m\n\u001b[32m    746\u001b[39m resp = \u001b[38;5;28;01mawait\u001b[39;00m req.send(conn)\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m resp.start(conn)\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    750\u001b[39m     resp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\museai\\data\\myenv\\Lib\\site-packages\\aiohttp\\client_reqrep.py:532\u001b[39m, in \u001b[36mClientResponse.start\u001b[39m\u001b[34m(self, connection)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    531\u001b[39m     protocol = \u001b[38;5;28mself\u001b[39m._protocol\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     message, payload = \u001b[38;5;28;01mawait\u001b[39;00m protocol.read()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m http.HttpProcessingError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[32m    535\u001b[39m         \u001b[38;5;28mself\u001b[39m.request_info,\n\u001b[32m    536\u001b[39m         \u001b[38;5;28mself\u001b[39m.history,\n\u001b[32m   (...)\u001b[39m\u001b[32m    539\u001b[39m         headers=exc.headers,\n\u001b[32m    540\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\museai\\data\\myenv\\Lib\\site-packages\\aiohttp\\streams.py:672\u001b[39m, in \u001b[36mDataQueue.read\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    670\u001b[39m \u001b[38;5;28mself\u001b[39m._waiter = \u001b[38;5;28mself\u001b[39m._loop.create_future()\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._waiter\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (asyncio.CancelledError, asyncio.TimeoutError):\n\u001b[32m    674\u001b[39m     \u001b[38;5;28mself\u001b[39m._waiter = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# Agent 6 — Polite product crawler (JSON-first, isolated, resumable)\n",
    "# Reads:   A5 DB (READONLY): populated_product_sitemaps + product_urls\n",
    "# Writes:  D:\\museai\\data\\a6\\...\n",
    "# Selection: deterministic rank (Unisex > Men > Women > Accessories > Footwear > Jewellery)\n",
    "# Crawl:   Shopify .js → .json → HTML JSON-LD → OG (strict Color/Size, variants/media full)\n",
    "# Resumes: manifest.json (etag, last_modified, content_hash) + state.json\n",
    "# Tracks:  registry/sites_status.json → completed sites auto-skipped + fill up to TOP_N\n",
    "# Merges:  merged/structured_latest.json (site → products), cleaned of U+2028/U+2029\n",
    "# ==============================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import asyncio\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "# ------------------------------\n",
    "# Config (env)\n",
    "# ------------------------------\n",
    "DB_PATH                = os.getenv(\"CRAWLER_DB\", os.getenv(\"DB_PATH\", r\"D:\\museai\\data\\db\\crawler_meta.db\"))\n",
    "POP_TABLE              = os.getenv(\"A5_POP_TABLE\", \"populated_product_sitemaps\")\n",
    "PRODUCT_URLS_TABLE     = os.getenv(\"A5_PRODUCT_URLS_TABLE\", \"product_urls\")\n",
    "\n",
    "A6_OUTPUT_ROOT         = os.getenv(\"A6_OUTPUT_DIR\", r\"D:\\museai\\data\\a6\")\n",
    "A6_TOP_N               = int(os.getenv(\"A6_TOP_N\", \"15\"))\n",
    "A6_LIMIT_PER_SITE      = int(os.getenv(\"A6_LIMIT_PER_SITE\", \"0\")) or None\n",
    "A6_DRY_RUN             = os.getenv(\"A6_DRY_RUN\", \"false\").strip().lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "A6_BATCH_SIZE          = int(os.getenv(\"A6_BATCH_SIZE\", \"200\"))\n",
    "A6_ERROR_BUDGET        = float(os.getenv(\"A6_ERROR_BUDGET\", \"0.25\"))\n",
    "A6_USE_CONDITIONAL_GET = os.getenv(\"A6_USE_CONDITIONAL_GET\", \"true\").strip().lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "\n",
    "GLOBAL_CONCURRENCY     = int(os.getenv(\"A6_GLOBAL_CONCURRENCY\", \"40\"))\n",
    "PER_HOST_WORKERS       = int(os.getenv(\"A6_PER_HOST_WORKERS\", \"2\"))\n",
    "REQUEST_TIMEOUT        = int(os.getenv(\"A6_REQUEST_TIMEOUT\", \"18\"))\n",
    "RETRIES                = int(os.getenv(\"A6_RETRIES\", \"3\"))\n",
    "DEFAULT_RPS            = float(os.getenv(\"A6_DEFAULT_RPS\", \"1.0\"))\n",
    "MIN_RPS                = float(os.getenv(\"A6_MIN_RPS\", \"0.08\"))\n",
    "MAX_RPS                = float(os.getenv(\"A6_MAX_RPS\", \"2.0\"))\n",
    "BASE_JITTER            = float(os.getenv(\"A6_BASE_JITTER\", \"0.30\"))\n",
    "PROXY_URL              = os.environ.get(\"A6_PROXY_URL\", \"\").strip() or None\n",
    "\n",
    "# Selection filter\n",
    "A6_REQUIRE_FLAG_ONLY   = os.getenv(\"A6_REQUIRE_FLAG_ONLY\", \"true\").strip().lower() in (\"1\",\"true\",\"yes\",\"y\")\n",
    "\n",
    "# NEW: cap the number of 403s per site before skipping the rest\n",
    "A6_FORBID_LIMIT        = int(os.getenv(\"A6_FORBID_LIMIT\", \"2\"))  # stop a site after this many 403s\n",
    "\n",
    "# Manual-done sites list\n",
    "A6_MANUALLY_DONE_SITES = {\n",
    "    re.sub(r\"^\\s*https?://\", \"\", s.strip().lower())\n",
    "    for s in os.getenv(\n",
    "        \"A6_MANUALLY_DONE_SITES\",\n",
    "        \"www.sassafras.in, bonkerscorner.com, vastrado.com, trapin.co, offduty.in\"\n",
    "    ).split(\",\")\n",
    "    if s.strip()\n",
    "}\n",
    "\n",
    "FAST_UA = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/127.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "HEADERS_HTML = {\n",
    "    \"User-Agent\": FAST_UA,\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "}\n",
    "HEADERS_JSON = {\n",
    "    \"User-Agent\": FAST_UA,\n",
    "    \"Accept\": \"application/json,text/html;q=0.8,*/*;q=0.5\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "}\n",
    "\n",
    "# ------------------------------\n",
    "# Paths & small utils\n",
    "# ------------------------------\n",
    "A6_REGISTRY_DIR  = os.path.join(A6_OUTPUT_ROOT, \"registry\")\n",
    "A6_SITES_STATUS  = os.path.join(A6_REGISTRY_DIR, \"sites_status.json\")\n",
    "\n",
    "def now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "def ts_for_filename() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[A6 {datetime.now(timezone.utc).strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "def ensure_dirs():\n",
    "    os.makedirs(A6_OUTPUT_ROOT, exist_ok=True)\n",
    "    os.makedirs(os.path.join(A6_OUTPUT_ROOT, \"selection\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(A6_OUTPUT_ROOT, \"sites\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(A6_OUTPUT_ROOT, \"snapshots\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(A6_OUTPUT_ROOT, \"merged\"), exist_ok=True)\n",
    "    os.makedirs(A6_REGISTRY_DIR, exist_ok=True)\n",
    "\n",
    "def site_dir(host: str) -> str:\n",
    "    safe = re.sub(r\"[^A-Za-z0-9._-]\", \"_\", host)\n",
    "    p = os.path.join(A6_OUTPUT_ROOT, \"sites\", safe)\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def host_only(u: str) -> str:\n",
    "    try:\n",
    "        return urlsplit(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return (u or \"\").lower()\n",
    "\n",
    "def host_key(url: str) -> str:\n",
    "    p = urlsplit(url)\n",
    "    return f\"{p.scheme}://{p.netloc}\"\n",
    "\n",
    "def sha1_of(obj: Any) -> str:\n",
    "    try:\n",
    "        raw = json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\"), sort_keys=True).encode(\"utf-8\")\n",
    "    except Exception:\n",
    "        raw = str(obj).encode(\"utf-8\", errors=\"ignore\")\n",
    "    return hashlib.sha1(raw).hexdigest()\n",
    "\n",
    "def product_key_from_url(url: str) -> str:\n",
    "    try:\n",
    "        path = urlsplit(url).path.strip(\"/\")\n",
    "        parts = [p for p in path.split(\"/\") if p]\n",
    "        for i, p in enumerate(parts):\n",
    "            if p.lower() == \"products\" and i + 1 < len(parts):\n",
    "                handle = parts[i + 1].split(\"?\")[0].strip().replace(\".html\", \"\")\n",
    "                if handle:\n",
    "                    return f\"shopify:{handle}\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return f\"url:{url}\"\n",
    "\n",
    "def canonical_product_key(raw_product: dict, url: str) -> str:\n",
    "    # Prefer Shopify handle; else fall back to URL-based key\n",
    "    handle = (raw_product or {}).get(\"handle\")\n",
    "    if isinstance(handle, str) and handle.strip():\n",
    "        return f\"shopify:{handle.strip()}\"\n",
    "    return product_key_from_url(url)\n",
    "\n",
    "# ------------------------------\n",
    "# Registry helpers (completed sites)\n",
    "# ------------------------------\n",
    "def read_json(path: str, default):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def write_json(path: str, obj: Any):\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def append_jsonl(path: str, obj: Any):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_sites_status() -> dict:\n",
    "    return read_json(A6_SITES_STATUS, {})\n",
    "\n",
    "def save_sites_status(d: dict):\n",
    "    write_json(A6_SITES_STATUS, d)\n",
    "\n",
    "def mark_site_completed(host: str, total: int, done: int, failed: int):\n",
    "    st = load_sites_status()\n",
    "    st[host] = {\n",
    "        \"completed\": True,\n",
    "        \"last_completed_at\": now_iso(),\n",
    "        \"total_urls\": total,\n",
    "        \"done\": done,\n",
    "        \"failed\": failed,\n",
    "    }\n",
    "    save_sites_status(st)\n",
    "\n",
    "def is_site_completed(host: str) -> bool:\n",
    "    st = load_sites_status()\n",
    "    return bool(st.get(host, {}).get(\"completed\"))\n",
    "\n",
    "# ------------------------------\n",
    "# DB (read-only)\n",
    "# ------------------------------\n",
    "def connect_db_ro() -> sqlite3.Connection:\n",
    "    uri = f\"file:{DB_PATH}?mode=ro\"\n",
    "    return sqlite3.connect(uri, uri=True)\n",
    "\n",
    "def load_sites_and_urls() -> Tuple[Dict[str, Dict[str, str]], Dict[str, List[str]]]:\n",
    "    conn = connect_db_ro()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    site_meta: Dict[str, Dict[str, str]] = {}\n",
    "    for site, platform, website_for in cur.execute(\n",
    "        f\"SELECT site, COALESCE(platform,''), COALESCE(website_for,'') FROM {POP_TABLE} WHERE status=1;\"\n",
    "    ):\n",
    "        s = (site or \"\").rstrip(\"/\")\n",
    "        if not s:\n",
    "            continue\n",
    "        site_meta[s] = {\"platform\": platform or \"\", \"website_for\": website_for or \"\"}\n",
    "\n",
    "    site_urls: Dict[str, List[str]] = defaultdict(list)\n",
    "    if A6_REQUIRE_FLAG_ONLY:\n",
    "        q = f\"SELECT site, url FROM {PRODUCT_URLS_TABLE} WHERE is_required=1 ORDER BY site, url;\"\n",
    "    else:\n",
    "        q = f\"SELECT site, url FROM {PRODUCT_URLS_TABLE} ORDER BY site, url;\"\n",
    "\n",
    "    for site, url in cur.execute(q):\n",
    "        if site and url:\n",
    "            site_urls[site.rstrip(\"/\")].append(url)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # de-dupe\n",
    "    for s in list(site_urls.keys()):\n",
    "        site_urls[s] = list(dict.fromkeys(site_urls[s]))\n",
    "\n",
    "    if A6_REQUIRE_FLAG_ONLY:\n",
    "        for s in list(site_urls.keys()):\n",
    "            if not site_urls[s]:\n",
    "                site_urls.pop(s, None)\n",
    "\n",
    "    return site_meta, site_urls\n",
    "\n",
    "# ------------------------------\n",
    "# Ranking\n",
    "# ------------------------------\n",
    "CATEGORY_ORDER = [\"unisex\", \"men\", \"women\", \"accessories\", \"footwear\", \"jewellery\"]\n",
    "\n",
    "def category_rank(website_for: str) -> int:\n",
    "    w = (website_for or \"\").strip().lower()\n",
    "    for i, cat in enumerate(CATEGORY_ORDER):\n",
    "        if cat in w:\n",
    "            return i\n",
    "    if \"apparel\" in w or \"clothing\" in w:\n",
    "        return 0\n",
    "    return len(CATEGORY_ORDER)\n",
    "\n",
    "def score_site(site: str, meta: Dict[str, str], urls: List[str]) -> Tuple:\n",
    "    wfor = meta.get(\"website_for\", \"\")\n",
    "    plat = (meta.get(\"platform\", \"\") or \"\").lower()\n",
    "    cat = category_rank(wfor)\n",
    "    n = len(urls)\n",
    "    shopify_bonus = 1 if \"shopify\" in plat else 0\n",
    "    host = urlsplit(site).netloc.lower()\n",
    "\n",
    "    manual = 1 if (host in A6_MANUALLY_DONE_SITES or any(host == d or host.endswith(\".\" + d) for d in A6_MANUALLY_DONE_SITES)) else 0\n",
    "    return (manual, cat, -n, -shopify_bonus, host)\n",
    "\n",
    "def build_selection(site_meta, site_urls, top_n: int) -> Dict[str, Any]:\n",
    "    scored = []\n",
    "    for s, urls in site_urls.items():\n",
    "        if not urls:\n",
    "            continue\n",
    "        sc = score_site(s, site_meta.get(s, {}), urls)\n",
    "        scored.append((sc, s, site_meta.get(s, {}), len(urls)))\n",
    "    scored.sort(key=lambda x: x[0])\n",
    "\n",
    "    ranked = []\n",
    "    for idx, (sc, s, meta, count) in enumerate(scored, 1):\n",
    "        ranked.append({\n",
    "            \"rank\": idx,\n",
    "            \"site\": s,\n",
    "            \"host\": urlsplit(s).netloc,\n",
    "            \"platform\": meta.get(\"platform\", \"\"),\n",
    "            \"website_for\": meta.get(\"website_for\", \"\"),\n",
    "            \"product_count_in_db\": count,\n",
    "            \"score_tuple\": list(sc),\n",
    "            \"manual_done\": bool(sc[0]),\n",
    "        })\n",
    "\n",
    "    # Choose up to top_n, **skipping completed**, keep pulling further down\n",
    "    chosen_effective = []\n",
    "    for row in ranked:\n",
    "        host = row[\"host\"].lower()\n",
    "        if is_site_completed(host):\n",
    "            continue\n",
    "        chosen_effective.append(row)\n",
    "        if len(chosen_effective) >= top_n:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"generated_at_utc\": now_iso(),\n",
    "        \"db_path\": DB_PATH,\n",
    "        \"requested_top_n\": top_n,\n",
    "        \"total_sites_considered\": len(ranked),\n",
    "        \"ranking_rule\": {\n",
    "            \"order\": CATEGORY_ORDER,\n",
    "            \"tie_breakers\": [\"manual_done(last)\", \"category\", \"product_count(desc)\", \"platform(shopify bonus)\", \"host(A–Z)\"],\n",
    "        },\n",
    "        \"ranked_sites\": ranked,\n",
    "        \"chosen\": chosen_effective,\n",
    "    }\n",
    "\n",
    "# ------------------------------\n",
    "# Polite limiter\n",
    "# ------------------------------\n",
    "class AdaptiveHostLimiter:\n",
    "    def __init__(self, default_rps=DEFAULT_RPS, min_rps=MIN_RPS, max_rps=MAX_RPS, base_jitter=BASE_JITTER):\n",
    "        self.default_rps = float(default_rps)\n",
    "        self.min_rps = float(min_rps)\n",
    "        self.max_rps = float(max_rps)\n",
    "        self.base_jitter = float(base_jitter)\n",
    "        self._state: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def _ensure(self, host: str):\n",
    "        if host not in self._state:\n",
    "            self._state[host] = {\"rps\": self.default_rps, \"next_at\": 0.0, \"cooldown_until\": 0.0, \"consec_429\": 0, \"last_status\": None}\n",
    "        return self._state[host]\n",
    "\n",
    "    async def acquire(self, host: str):\n",
    "        st = self._ensure(host)\n",
    "        now = time.monotonic()\n",
    "        if st[\"cooldown_until\"] > now:\n",
    "            await asyncio.sleep(st[\"cooldown_until\"] - now)\n",
    "            now = time.monotonic()\n",
    "        min_delay = 1.0 / max(self.min_rps, st[\"rps\"])\n",
    "        jitter = random.uniform(0.0, self.base_jitter * min_delay)\n",
    "        wait_until = max(st[\"next_at\"], now)\n",
    "        sleep_for = wait_until - now\n",
    "        if sleep_for > 0:\n",
    "            await asyncio.sleep(sleep_for)\n",
    "            now = time.monotonic()\n",
    "        st[\"next_at\"] = now + min_delay + jitter\n",
    "\n",
    "    def feedback(self, host: str, status: int, headers: Optional[dict]):\n",
    "        st = self._ensure(host)\n",
    "        st[\"last_status\"] = status\n",
    "        if 200 <= status < 300:\n",
    "            st[\"consec_429\"] = 0\n",
    "            st[\"rps\"] = min(self.max_rps, st[\"rps\"] * 1.10)\n",
    "            return\n",
    "        if status in (0, 404):\n",
    "            st[\"rps\"] = max(self.min_rps, st[\"rps\"] * 0.90)\n",
    "            return\n",
    "        if status in (429, 403, 503, 520, 522):\n",
    "            cooldown = 0.0\n",
    "            if headers:\n",
    "                ra = headers.get(\"Retry-After\") or headers.get(\"retry-after\")\n",
    "                if ra:\n",
    "                    try:\n",
    "                        cooldown = float(ra)\n",
    "                    except:\n",
    "                        pass\n",
    "            if status == 429:\n",
    "                st[\"consec_429\"] += 1\n",
    "                st[\"rps\"] = max(self.min_rps, st[\"rps\"] * 0.5)\n",
    "                extra = 10.0 * st[\"consec_429\"]\n",
    "                cooldown = max(cooldown, min(90.0, 12.0 + extra))\n",
    "            else:\n",
    "                st[\"rps\"] = max(self.min_rps, st[\"rps\"] * 0.5)\n",
    "                cooldown = max(cooldown, 30.0)\n",
    "            st[\"cooldown_until\"] = time.monotonic() + cooldown\n",
    "            return\n",
    "        st[\"rps\"] = max(self.min_rps, st[\"rps\"] * 0.95)\n",
    "\n",
    "limiter = AdaptiveHostLimiter()\n",
    "\n",
    "# ------------------------------\n",
    "# HTTP + Shopify helpers\n",
    "# ------------------------------\n",
    "async def fetch_text(session, url: str, headers: dict, timeout=REQUEST_TIMEOUT, max_retries=RETRIES,\n",
    "                     cond_etag: Optional[str]=None, cond_lastmod: Optional[str]=None) -> Tuple[int, dict, str]:\n",
    "    host = host_key(url)\n",
    "    last_status, last_headers, last_text = 0, {}, \"\"\n",
    "    use_headers = dict(headers)\n",
    "    if A6_USE_CONDITIONAL_GET:\n",
    "        if cond_etag:\n",
    "            use_headers[\"If-None-Match\"] = cond_etag\n",
    "        if cond_lastmod:\n",
    "            use_headers[\"If-Modified-Since\"] = cond_lastmod\n",
    "\n",
    "    for _ in range(max_retries + 1):\n",
    "        await limiter.acquire(host)\n",
    "        try:\n",
    "            async with session.get(\n",
    "                url,\n",
    "                headers=use_headers,\n",
    "                timeout=aiohttp.ClientTimeout(total=timeout),\n",
    "                proxy=PROXY_URL,\n",
    "                allow_redirects=True,\n",
    "            ) as r:\n",
    "                last_status = r.status\n",
    "                last_headers = dict(r.headers)\n",
    "                txt = await r.text(errors=\"ignore\")\n",
    "                last_text = txt\n",
    "                limiter.feedback(host, r.status, last_headers)\n",
    "                if r.status in (200, 304):\n",
    "                    return r.status, last_headers, txt\n",
    "                if r.status in (429, 403, 503, 520, 522):\n",
    "                    continue\n",
    "                return r.status, last_headers, txt\n",
    "        except Exception:\n",
    "            limiter.feedback(host, 0, None)\n",
    "            continue\n",
    "    return last_status, last_headers, last_text\n",
    "\n",
    "async def fetch_json(session, url: str, referer: Optional[str]=None,\n",
    "                     cond_etag: Optional[str]=None, cond_lastmod: Optional[str]=None) -> Tuple[int, dict, Any]:\n",
    "    headers = dict(HEADERS_JSON)\n",
    "    if referer:\n",
    "        headers[\"Referer\"] = referer\n",
    "    status, hdrs, txt = await fetch_text(session, url, headers, cond_etag=cond_etag, cond_lastmod=cond_lastmod)\n",
    "    data = None\n",
    "    if status == 200 and txt:\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except Exception:\n",
    "            try:\n",
    "                data = json.loads(txt.strip())\n",
    "            except Exception:\n",
    "                data = None\n",
    "    return status, hdrs, data\n",
    "\n",
    "def text_only(s: Optional[str]) -> Optional[str]:\n",
    "    if s is None:\n",
    "        return None\n",
    "    try:\n",
    "        return re.sub(r\"<[^>]+>\", \"\", s).strip()\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def safe_json_loads(s: str):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            fixed = s.replace(\"\\n\", \" \").strip()\n",
    "            if fixed.startswith(\"{\") and fixed.endswith(\"}\"):\n",
    "                fixed2 = fixed.replace(\"}{\", \"}|{\")\n",
    "                parts = fixed2.split(\"|\")\n",
    "                return [json.loads(p) for p in parts]\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def find_product_node(json_obj):\n",
    "    def is_product_type(t):\n",
    "        if isinstance(t, list): return any(str(x).lower().endswith(\"product\") for x in t)\n",
    "        if isinstance(t, str):  return str(t).lower().endswith(\"product\")\n",
    "        return False\n",
    "    def walk(node):\n",
    "        if isinstance(node, dict):\n",
    "            if \"@type\" in node and is_product_type(node[\"@type\"]):\n",
    "                return node\n",
    "            if \"@graph\" in node and isinstance(node[\"@graph\"], list):\n",
    "                for sub in node[\"@graph\"]:\n",
    "                    r = walk(sub)\n",
    "                    if r: return r\n",
    "            for v in node.values():\n",
    "                r = walk(v)\n",
    "                if r: return r\n",
    "        elif isinstance(node, list):\n",
    "            for it in node:\n",
    "                r = walk(it)\n",
    "                if r: return r\n",
    "        return None\n",
    "    return walk(json_obj)\n",
    "\n",
    "def flatten_jsonld_product(node: dict) -> dict:\n",
    "    out = {}\n",
    "    try:\n",
    "        def pick_first(x):\n",
    "            return (x[0] if isinstance(x, list) and x else x)\n",
    "        out[\"title\"] = pick_first(node.get(\"name\"))\n",
    "        out[\"body_html\"] = pick_first(node.get(\"description\"))\n",
    "        out[\"vendor\"] = None\n",
    "        out[\"product_type\"] = pick_first(node.get(\"category\"))\n",
    "        img = node.get(\"image\")\n",
    "        if isinstance(img, list): out[\"image\"] = {\"src\": pick_first(img)}\n",
    "        elif isinstance(img, dict): out[\"image\"] = {\"src\": pick_first(img.get(\"url\") or img.get(\"contentUrl\"))}\n",
    "        else: out[\"image\"] = {\"src\": pick_first(img)} if img else None\n",
    "        offers = node.get(\"offers\")\n",
    "        if isinstance(offers, list): offers = offers[0]\n",
    "        variants = []\n",
    "        if isinstance(offers, dict):\n",
    "            price = offers.get(\"price\")\n",
    "            if price is not None:\n",
    "                variants = [{\"price\": price}]\n",
    "        out[\"variants\"] = variants\n",
    "        out[\"options\"] = []\n",
    "        out[\"media\"] = []\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "def strict_colors_from_options(product: dict) -> List[str]:\n",
    "    colors = []\n",
    "    for opt in (product.get(\"options\") or []):\n",
    "        name = (opt.get(\"name\") or \"\").strip().lower()\n",
    "        if name in (\"color\", \"colour\") or \"color\" in name or \"colour\" in name:\n",
    "            for v in (opt.get(\"values\") or []):\n",
    "                s = str(v).strip()\n",
    "                if s: colors.append(s)\n",
    "    seen=set(); out=[]\n",
    "    for c in colors:\n",
    "        k=c.lower()\n",
    "        if k not in seen:\n",
    "            out.append(c); seen.add(k)\n",
    "    return out\n",
    "\n",
    "def strict_sizes_from_options(product: dict) -> List[str]:\n",
    "    sizes = []\n",
    "    for opt in (product.get(\"options\") or []):\n",
    "        name = (opt.get(\"name\") or \"\").strip().lower()\n",
    "        if \"size\" in name:\n",
    "            for v in (opt.get(\"values\") or []):\n",
    "                s = str(v).strip()\n",
    "                if s: sizes.append(s)\n",
    "    seen=set(); out=[]\n",
    "    for s in sizes:\n",
    "        k=s.lower()\n",
    "        if k not in seen:\n",
    "            out.append(s); seen.add(k)\n",
    "    return out\n",
    "\n",
    "def image_urls(product: dict) -> List[str]:\n",
    "    out=[]\n",
    "    for it in (product.get(\"images\") or []):\n",
    "        if isinstance(it, str):\n",
    "            u = it.strip()\n",
    "            if u: out.append(u)\n",
    "        elif isinstance(it, dict):\n",
    "            u = (it.get(\"src\") or it.get(\"url\") or \"\").strip()\n",
    "            if u: out.append(u)\n",
    "    for m in (product.get(\"media\") or []):\n",
    "        if isinstance(m, dict):\n",
    "            if m.get(\"media_type\") == \"image\" and m.get(\"src\"):\n",
    "                out.append(m[\"src\"])\n",
    "            elif isinstance(m.get(\"preview_image\"), dict) and m[\"preview_image\"].get(\"src\"):\n",
    "                out.append(m[\"preview_image\"][\"src\"])\n",
    "    seen=set(); ded=[]\n",
    "    for u in out:\n",
    "        if u and u not in seen:\n",
    "            ded.append(u); seen.add(u)\n",
    "    return ded\n",
    "\n",
    "def variant_summary(variants: List[dict]) -> dict:\n",
    "    if not isinstance(variants, list): variants=[]\n",
    "    prices, cap_prices, skus, barcodes = [], [], [], []\n",
    "    any_available = False\n",
    "    for v in variants:\n",
    "        try: prices.append(float(str(v.get(\"price\",\"\")).replace(\",\",\"\")))\n",
    "        except: pass\n",
    "        try: cap_prices.append(float(str(v.get(\"compare_at_price\",\"\")).replace(\",\",\"\")))\n",
    "        except: pass\n",
    "        if v.get(\"available\") is True: any_available = True\n",
    "        if v.get(\"sku\"): skus.append(str(v.get(\"sku\")))\n",
    "        if v.get(\"barcode\"): barcodes.append(str(v.get(\"barcode\")))\n",
    "    def _min_nonempty(a):\n",
    "        a2=[x for x in a if x or x==0]\n",
    "        return (min(a2) if a2 else None)\n",
    "    def _max_nonempty(a):\n",
    "        a2=[x for x in a if x or x==0]\n",
    "        return (max(a2) if a2 else None)\n",
    "    skus = list(dict.fromkeys(skus))\n",
    "    barcodes = list(dict.fromkeys(barcodes))\n",
    "    return {\n",
    "        \"variants_count\": len(variants),\n",
    "        \"any_variant_available\": any_available,\n",
    "        \"price_min\": _min_nonempty(prices),\n",
    "        \"price_max\": _max_nonempty(prices),\n",
    "        \"compare_at_price_min\": _min_nonempty(cap_prices),\n",
    "        \"compare_at_price_max\": _max_nonempty(cap_prices),\n",
    "        \"skus\": skus or None,\n",
    "        \"barcodes\": barcodes or None,\n",
    "    }\n",
    "\n",
    "def flatten_product_record(product: dict, url: str, site_name: str, via: str, http_meta: dict, lastmod_hint=None) -> dict:\n",
    "    rec = {\n",
    "        \"site_name\": site_name,\n",
    "        \"url\": url,\n",
    "        \"fetched_at\": now_iso(),\n",
    "        \"source\": via,\n",
    "        \"status\": 200,\n",
    "        \"lastmod\": lastmod_hint,\n",
    "        \"source_sitemap\": None,\n",
    "    }\n",
    "    for k in [\"id\",\"title\",\"handle\",\"body_html\",\"vendor\",\"product_type\",\"tags\",\n",
    "              \"template_suffix\",\"published_scope\",\"created_at\",\"updated_at\",\"published_at\"]:\n",
    "        rec[k] = product.get(k)\n",
    "    rec[\"body_text\"] = text_only(product.get(\"body_html\") or \"\") if product.get(\"body_html\") else None\n",
    "    rec[\"option_names\"] = [opt.get(\"name\") for opt in (product.get(\"options\") or [])]\n",
    "    rec[\"options_json\"] = json.dumps(product.get(\"options\") or [], ensure_ascii=False)\n",
    "    rec[\"colors\"] = strict_colors_from_options(product) or None\n",
    "    rec[\"sizes\"] = strict_sizes_from_options(product) or None\n",
    "    imgs = image_urls(product)\n",
    "    rec[\"images\"] = imgs or None\n",
    "    rec[\"images_count\"] = len(imgs)\n",
    "    feat = product.get(\"image\") or {}\n",
    "    if isinstance(feat, dict):\n",
    "        rec[\"featured_image\"] = feat.get(\"src\") or feat.get(\"url\")\n",
    "        rec[\"featured_alt\"] = feat.get(\"alt\")\n",
    "    else:\n",
    "        rec[\"featured_image\"] = None\n",
    "        rec[\"featured_alt\"] = None\n",
    "    rec[\"media_json\"] = json.dumps(product.get(\"media\") or [], ensure_ascii=False)\n",
    "    rec[\"media_count\"] = len(product.get(\"media\") or [])\n",
    "    variants = product.get(\"variants\") or []\n",
    "    rec[\"variants_json\"] = json.dumps(variants, ensure_ascii=False)\n",
    "    rec.update(variant_summary(variants))\n",
    "    rec[\"http_etag\"] = http_meta.get(\"etag\")\n",
    "    rec[\"http_last_modified\"] = http_meta.get(\"last_modified\")\n",
    "    return rec\n",
    "\n",
    "# ------------------------------\n",
    "# Fetch product (Shopify-first)\n",
    "# ------------------------------\n",
    "async def fetch_shopify_product(session, product_url: str,\n",
    "                                etag: Optional[str]=None, last_mod: Optional[str]=None) -> Tuple[str, int, dict | None, dict]:\n",
    "    \"\"\"Return (via, status, product_dict_or_none, http_meta). via in {'shopify.js','shopify.json','json-ld','opengraph','-','not-modified'}\"\"\"\n",
    "    handle = None\n",
    "    try:\n",
    "        path = urlsplit(product_url).path.strip(\"/\")\n",
    "        parts = [p for p in path.split(\"/\") if p]\n",
    "        for i, p in enumerate(parts):\n",
    "            if p.lower() == \"products\" and i + 1 < len(parts):\n",
    "                handle = parts[i+1].split(\"?\")[0].strip(\"/\").replace(\".html\", \"\")\n",
    "    except Exception:\n",
    "        handle = None\n",
    "\n",
    "    # 1) /products/{handle}.js/.json\n",
    "    if handle:\n",
    "        base = host_key(product_url)\n",
    "        for ext in (\".js\", \".json\"):\n",
    "            api = f\"{base}/products/{handle}{ext}\"\n",
    "            status, hdrs, data = await fetch_json(session, api, referer=product_url,\n",
    "                                                  cond_etag=etag, cond_lastmod=last_mod)\n",
    "            if status == 304:\n",
    "                return \"not-modified\", 304, None, {\"etag\": hdrs.get(\"ETag\"), \"last_modified\": hdrs.get(\"Last-Modified\")}\n",
    "            if status == 200 and isinstance(data, (dict, list)):\n",
    "                if ext == \".js\":\n",
    "                    product = data if (isinstance(data, dict) and \"title\" in data and \"variants\" in data) else (data.get(\"product\") if isinstance(data, dict) else None)\n",
    "                    if isinstance(product, dict) and product.get(\"title\"):\n",
    "                        return \"shopify.js\", 200, product, {\"etag\": hdrs.get(\"ETag\"), \"last_modified\": hdrs.get(\"Last-Modified\")}\n",
    "                else:\n",
    "                    product = data.get(\"product\") if isinstance(data, dict) else None\n",
    "                    if isinstance(product, dict) and product.get(\"title\"):\n",
    "                        return \"shopify.json\", 200, product, {\"etag\": hdrs.get(\"ETag\"), \"last_modified\": hdrs.get(\"Last-Modified\")}\n",
    "\n",
    "    # 2) HTML → JSON-LD → OG\n",
    "    status, hdrs, html_text = await fetch_text(session, product_url, HEADERS_HTML,\n",
    "                                               cond_etag=etag, cond_lastmod=last_mod)\n",
    "    if status == 304:\n",
    "        return \"not-modified\", 304, None, {\"etag\": hdrs.get(\"ETag\"), \"last_modified\": hdrs.get(\"Last-Modified\")}\n",
    "    if status == 200 and html_text:\n",
    "        try:\n",
    "            product_node = None\n",
    "            for blk in re.findall(r'<script[^>]+type=[\"\\']application/ld\\+json[\"\\'][^>]*>(.*?)</script>',\n",
    "                                  html_text, flags=re.I|re.S):\n",
    "                parsed_any = safe_json_loads(blk.strip())\n",
    "                if parsed_any is None:\n",
    "                    continue\n",
    "                cand = find_product_node(parsed_any)\n",
    "                if cand is not None:\n",
    "                    product_node = cand\n",
    "                    break\n",
    "            if product_node is not None:\n",
    "                mapped = flatten_jsonld_product(product_node)\n",
    "                return \"json-ld\", 200, mapped, {\"etag\": hdrs.get(\"ETag\"), \"last_modified\": hdrs.get(\"Last-Modified\")}\n",
    "            # OG fallback\n",
    "            og = {}\n",
    "            for m in re.findall(r'<meta\\s+(?:name|property)=[\"\\'](og:[^\"\\']+)[\"\\']\\s+content=[\"\\']([^\"\\']+)[\"\\']', html_text, flags=re.I):\n",
    "                og[m[0].lower()] = m[1]\n",
    "            mapped = {\n",
    "                \"title\": og.get(\"og:title\"),\n",
    "                \"body_html\": og.get(\"og:description\"),\n",
    "                \"image\": {\"src\": og.get(\"og:image\")} if og.get(\"og:image\") else None,\n",
    "                \"variants\": [],\n",
    "                \"options\": [],\n",
    "                \"media\": [],\n",
    "            }\n",
    "            return \"opengraph\", 200, mapped, {\"etag\": hdrs.get(\"ETag\"), \"last_modified\": hdrs.get(\"Last-Modified\")}\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"-\", status, None, {\"etag\": hdrs.get(\"ETag\") if isinstance(hdrs, dict) else None,\n",
    "                               \"last_modified\": hdrs.get(\"Last-Modified\") if isinstance(hdrs, dict) else None}\n",
    "\n",
    "# ------------------------------\n",
    "# Crawl one site\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "# Crawl one site  (worker-queue; hard-stop after A6_FORBID_LIMIT×403)\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "# Crawl one site  (worker-queue; hard-stop after A6_FORBID_LIMIT×403; serialized writes)\n",
    "# ------------------------------\n",
    "async def crawl_site(site: str, urls: List[str], meta: Dict[str,str]):\n",
    "    host = urlsplit(site).netloc\n",
    "    sdir = site_dir(host)\n",
    "    jsonl_path = os.path.join(sdir, f\"products-{datetime.now(timezone.utc).strftime('%Y%m%d')}.jsonl\")\n",
    "    manifest_path = os.path.join(sdir, \"manifest.json\")\n",
    "    state_path = os.path.join(sdir, \"state.json\")\n",
    "    grouped_path = os.path.join(sdir, \"grouped-latest.json\")\n",
    "\n",
    "    manifest = read_json(manifest_path, default={})\n",
    "    state = read_json(state_path, default={\"pending\": [], \"done\": [], \"failed\": [], \"last_run\": None})\n",
    "    state[\"last_run\"] = now_iso()\n",
    "\n",
    "    already = set(state.get(\"pending\", [])) | set(state.get(\"failed\", [])) | set(state.get(\"done\", []))\n",
    "    fresh = [u for u in urls if u not in already]\n",
    "    state[\"pending\"].extend(fresh)\n",
    "\n",
    "    if A6_LIMIT_PER_SITE:\n",
    "        state[\"pending\"] = state[\"pending\"][:A6_LIMIT_PER_SITE]\n",
    "\n",
    "    write_json(state_path, state)\n",
    "\n",
    "    total_target = len(state[\"pending\"])\n",
    "    if total_target == 0:\n",
    "        log(f\"[site] {host} nothing to do.\")\n",
    "        return\n",
    "\n",
    "    global_sem = asyncio.Semaphore(GLOBAL_CONCURRENCY)\n",
    "    per_host_sem = asyncio.Semaphore(PER_HOST_WORKERS)\n",
    "\n",
    "    # NEW: per-site hard-stop and counters\n",
    "    stop_event = asyncio.Event()\n",
    "    forbidden_count = 0\n",
    "    error_count = 0\n",
    "    processed = 0\n",
    "\n",
    "    # NEW: serialize writes to JSON files to avoid Windows PermissionError\n",
    "    write_lock = asyncio.Lock()\n",
    "\n",
    "    # Work queue\n",
    "    q: asyncio.Queue[str] = asyncio.Queue()\n",
    "    for u in state[\"pending\"]:\n",
    "        q.put_nowait(u)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "\n",
    "        async def persist_state_and_manifest():\n",
    "            # single place that writes both files under the lock\n",
    "            async with write_lock:\n",
    "                write_json(manifest_path, manifest)\n",
    "                write_json(state_path, state)\n",
    "\n",
    "        async def handle_one(u: str, idx: int):\n",
    "            nonlocal error_count, processed, forbidden_count\n",
    "\n",
    "            async with global_sem, per_host_sem:\n",
    "                pkey = product_key_from_url(u)\n",
    "                et = manifest.get(pkey, {}).get(\"etag\") if A6_USE_CONDITIONAL_GET else None\n",
    "                lm = manifest.get(pkey, {}).get(\"last_modified\") if A6_USE_CONDITIONAL_GET else None\n",
    "\n",
    "                # If you add a WP fallback later, call fetch_product_any here\n",
    "                via, status, product, http_meta = await fetch_shopify_product(session, u, etag=et, last_mod=lm)\n",
    "\n",
    "            # 403 handling & cap\n",
    "            if status == 403:\n",
    "                forbidden_count += 1\n",
    "                state[\"failed\"].append(u)\n",
    "                error_count += 1\n",
    "                log(f\"[{host}] {idx}/{total_target} 403 (#{forbidden_count}) → {u}\")\n",
    "                if forbidden_count >= A6_FORBID_LIMIT and not stop_event.is_set():\n",
    "                    log(f\"[{host}] hit {A6_FORBID_LIMIT}×403 → skipping remainder of site\")\n",
    "                    stop_event.set()\n",
    "                # persist quickly so state shows the failure\n",
    "                await persist_state_and_manifest()\n",
    "                return\n",
    "\n",
    "            if status == 304:\n",
    "                state[\"done\"].append(u)\n",
    "                processed += 1\n",
    "                if idx % 25 == 1:\n",
    "                    log(f\"[{host}] {idx}/{total_target} 304 not modified → {u}\")\n",
    "                # light persistence every so often\n",
    "                if processed % A6_BATCH_SIZE == 0:\n",
    "                    await persist_state_and_manifest()\n",
    "                return\n",
    "\n",
    "            if status != 200 or product is None:\n",
    "                error_count += 1\n",
    "                state[\"failed\"].append(u)\n",
    "                if idx % 10 == 1 or status in (429,403,503):\n",
    "                    log(f\"[{host}] {idx}/{total_target} ERR status={status} via={via} → {u}\")\n",
    "                # persist error so we don’t re-try on crash\n",
    "                await persist_state_and_manifest()\n",
    "                return\n",
    "\n",
    "            # Success path\n",
    "            flat = flatten_product_record(product, u, site, via, http_meta, lastmod_hint=None)\n",
    "            raw_obj = {\"site_name\": site, \"url\": u, \"fetched_at\": flat.get(\"fetched_at\"), \"raw_product\": product}\n",
    "\n",
    "            content_hash = sha1_of(product)\n",
    "            pkey = product_key_from_url(u)\n",
    "            prior_hash = manifest.get(pkey, {}).get(\"content_hash\")\n",
    "            if prior_hash != content_hash:\n",
    "                # append_jsonl is atomic-ish; no lock needed here\n",
    "                append_jsonl(jsonl_path, raw_obj)\n",
    "\n",
    "            manifest[pkey] = {\n",
    "                \"url\": u,\n",
    "                \"etag\": http_meta.get(\"etag\"),\n",
    "                \"last_modified\": http_meta.get(\"last_modified\"),\n",
    "                \"content_hash\": content_hash,\n",
    "                \"last_seen\": flat[\"fetched_at\"],\n",
    "                \"status\": 200,\n",
    "            }\n",
    "\n",
    "            state[\"done\"].append(u)\n",
    "            processed += 1\n",
    "            if (idx % 25 == 1) or (via not in (\"shopify.js\",\"shopify.json\")):\n",
    "                log(f\"[{host}] {idx}/{total_target} via={via} 200 → {u}\")\n",
    "\n",
    "            # Periodic persistence (serialized)\n",
    "            if processed % A6_BATCH_SIZE == 0:\n",
    "                await persist_state_and_manifest()\n",
    "\n",
    "        async def worker(worker_id: int):\n",
    "            # Pull next item while stop not set\n",
    "            while not stop_event.is_set():\n",
    "                try:\n",
    "                    u = q.get_nowait()\n",
    "                except asyncio.QueueEmpty:\n",
    "                    return\n",
    "                idx = total_target - q.qsize()  # progress-ish\n",
    "                try:\n",
    "                    await handle_one(u, idx)\n",
    "                finally:\n",
    "                    q.task_done()\n",
    "\n",
    "                # Error budget guard:\n",
    "                # - Ignore while we’re tripping the 403 cap (it has its own stop rule)\n",
    "                # - Only apply once we have a minimum sample size to avoid 1/1 = 100%\n",
    "                min_samples = max(20, PER_HOST_WORKERS * 4)\n",
    "                total_seen = processed + error_count\n",
    "                if (forbidden_count == 0) and (total_seen >= min_samples):\n",
    "                    frac = (error_count) / max(1, total_seen)\n",
    "                    if frac > A6_ERROR_BUDGET and not stop_event.is_set():\n",
    "                        log(f\"[{host}] error budget exceeded ({error_count}/{total_seen}); pausing\")\n",
    "                        stop_event.set()\n",
    "                        return\n",
    "\n",
    "        # Launch limited workers\n",
    "        workers = [asyncio.create_task(worker(i+1)) for i in range(PER_HOST_WORKERS)]\n",
    "        try:\n",
    "            await asyncio.gather(*workers)\n",
    "        finally:\n",
    "            # Ensure last persisted state even on early stop\n",
    "            await persist_state_and_manifest()\n",
    "\n",
    "    # Recompute remaining\n",
    "    remaining = [u for u in state[\"pending\"] if u not in state[\"done\"] and u not in state[\"failed\"]]\n",
    "    state[\"pending\"] = remaining\n",
    "    # Final write under lock (single-threaded here, but keep consistent)\n",
    "    async def _final_persist():\n",
    "        async with write_lock:\n",
    "            write_json(manifest_path, manifest)\n",
    "            write_json(state_path, state)\n",
    "    # run the small coroutine to persist\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        if loop.is_running():\n",
    "            await _final_persist()\n",
    "    except RuntimeError:\n",
    "        # fallback if not in a loop; unlikely here\n",
    "        pass\n",
    "\n",
    "    # Optional: mark completed if 403 cap tripped\n",
    "    if stop_event.is_set() and forbidden_count >= A6_FORBID_LIMIT:\n",
    "        mark_site_completed(host, total=len(urls), done=len(state.get(\"done\", [])), failed=len(state.get(\"failed\", [])))\n",
    "        log(f\"[site] {host} marked completed due to repeated 403s (done={len(state['done'])}, failed={len(state['failed'])})\")\n",
    "\n",
    "    # Grouped output\n",
    "    if not A6_DRY_RUN:\n",
    "        grouped = []\n",
    "        try:\n",
    "            with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        grouped.append({\"url\": obj.get(\"url\"), \"product\": obj.get(\"raw_product\")})\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            write_json(grouped_path, grouped)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    log(f\"[site] {host} done={len(state['done'])} failed={len(state['failed'])} remaining={len(state['pending'])}\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Structured merge (site → products JSON) with cleaning & upsert\n",
    "# ------------------------------\n",
    "LS = \"\\u2028\"\n",
    "PS = \"\\u2029\"\n",
    "\n",
    "def _clean_text(x):\n",
    "    if isinstance(x, str):\n",
    "        if LS in x or PS in x:\n",
    "            x = x.replace(LS, \" \").replace(PS, \" \")\n",
    "    return x\n",
    "\n",
    "def _clean_obj(o):\n",
    "    if isinstance(o, dict):\n",
    "        return {k: _clean_obj(_clean_text(v)) for k, v in o.items()}\n",
    "    if isinstance(o, list):\n",
    "        return [_clean_obj(_clean_text(v)) for v in o]\n",
    "    return _clean_text(o)\n",
    "\n",
    "def run_structured_merge(a6_root=A6_OUTPUT_ROOT):\n",
    "    merged_dir = os.path.join(a6_root, \"merged\")\n",
    "    sites_dir  = os.path.join(a6_root, \"sites\")\n",
    "    out_path   = os.path.join(merged_dir, \"structured_latest.json\")\n",
    "    os.makedirs(merged_dir, exist_ok=True)\n",
    "\n",
    "    # Read ALL historical per-site jsonl files\n",
    "    all_rows = []\n",
    "    for sdir in sorted(glob.glob(os.path.join(sites_dir, \"*\"))):\n",
    "        for fp in sorted(glob.glob(os.path.join(sdir, \"products-*.jsonl\"))):\n",
    "            try:\n",
    "                with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            all_rows.append(json.loads(line))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "    # Upsert by canonical product key (prefer handle, else URL); keep newest fetched_at\n",
    "    latest_by_key: Dict[str, dict] = {}\n",
    "    for row in all_rows:\n",
    "        url = row.get(\"url\") or \"\"\n",
    "        key = canonical_product_key(row.get(\"raw_product\") or {}, url)\n",
    "        ts  = row.get(\"fetched_at\") or \"\"\n",
    "        prev = latest_by_key.get(key)\n",
    "        if (not prev) or (ts > prev.get(\"fetched_at\", \"\")):\n",
    "            latest_by_key[key] = row\n",
    "\n",
    "    # Group by site\n",
    "    per_site: Dict[str, dict] = {}\n",
    "    for row in latest_by_key.values():\n",
    "        site = row.get(\"site_name\") or \"\"\n",
    "        host = host_only(site or row.get(\"url\") or \"\")\n",
    "        bucket = per_site.setdefault(site, {\"site\": site, \"host\": host, \"products\": []})\n",
    "        bucket[\"products\"].append({\"url\": row.get(\"url\"), \"product\": row.get(\"raw_product\")})\n",
    "\n",
    "    # Sort products within each site for stable output\n",
    "    for b in per_site.values():\n",
    "        b[\"products\"].sort(key=lambda x: (x.get(\"url\") or \"\"))\n",
    "\n",
    "    out = {\n",
    "        \"generated_at_utc\": now_iso(),\n",
    "        \"total_sites\": len(per_site),\n",
    "        \"total_products\": sum(len(b[\"products\"]) for b in per_site.values()),\n",
    "        \"sites\": list(per_site.values()),\n",
    "    }\n",
    "    out = _clean_obj(out)\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "    log(f\"Structured merge → {out_path}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Top-level Agent 6\n",
    "# ------------------------------\n",
    "async def run_agent6():\n",
    "    ensure_dirs()\n",
    "    site_meta, site_urls = load_sites_and_urls()\n",
    "\n",
    "    selection = build_selection(site_meta, site_urls, top_n=A6_TOP_N)\n",
    "    sel_ts = ts_for_filename()\n",
    "    selection_path = os.path.join(A6_OUTPUT_ROOT, \"selection\", f\"run_{sel_ts}_selection.json\")\n",
    "    write_json(selection_path, selection)\n",
    "    log(f\"Selection saved → {selection_path}\")\n",
    "\n",
    "    chosen_hosts = [e[\"site\"] for e in selection[\"chosen\"]]\n",
    "    if not chosen_hosts:\n",
    "        log(\"No sites chosen (all completed or none available).\")\n",
    "        run_structured_merge(A6_OUTPUT_ROOT)\n",
    "        return\n",
    "\n",
    "    log(f\"Chosen {len(chosen_hosts)} site(s) to crawl (completed skipped, filled to TOP_N if possible):\")\n",
    "    for e in selection[\"chosen\"]:\n",
    "        log(f\"  #{e['rank']:>2} {e['host']}  products={e['product_count_in_db']}  manual={e['manual_done']}\")\n",
    "\n",
    "    if A6_DRY_RUN:\n",
    "        log(\"DRY RUN enabled — not fetching or writing site products.\")\n",
    "        run_structured_merge(A6_OUTPUT_ROOT)\n",
    "        return\n",
    "\n",
    "    # Crawl each selected site (skip manual-done but mark completed)\n",
    "    for site in chosen_hosts:\n",
    "        urls = site_urls.get(site, [])\n",
    "        host = host_only(site)\n",
    "        if not urls:\n",
    "            log(f\"[site] {site} has 0 URLs; skipping\")\n",
    "            continue\n",
    "\n",
    "        if host in A6_MANUALLY_DONE_SITES:\n",
    "            sdir = site_dir(host)\n",
    "            state_path = os.path.join(sdir, \"state.json\")\n",
    "            state = read_json(state_path, default={\"pending\": [], \"done\": [], \"failed\": [], \"last_run\": None})\n",
    "            already_done = set(state.get(\"done\", []))\n",
    "            new_done = [u for u in urls if u not in already_done]\n",
    "            state[\"done\"] = list(already_done.union(new_done))\n",
    "            state[\"pending\"] = [u for u in state.get(\"pending\", []) if u not in state[\"done\"]]\n",
    "            state[\"last_run\"] = now_iso()\n",
    "            write_json(state_path, state)\n",
    "\n",
    "            manifest_path = os.path.join(sdir, \"manifest.json\")\n",
    "            manifest = read_json(manifest_path, default={})\n",
    "            write_json(manifest_path, manifest)\n",
    "\n",
    "            mark_site_completed(host, total=len(urls), done=len(state[\"done\"]), failed=len(state[\"failed\"]))\n",
    "            log(f\"[site] {host} marked as DONE (manual). done={len(state['done'])} pending={len(state['pending'])}\")\n",
    "            continue\n",
    "\n",
    "        await crawl_site(site, urls, site_meta.get(site, {}))\n",
    "\n",
    "        # Mark completed if nothing pending\n",
    "        state = read_json(os.path.join(site_dir(host), \"state.json\"), default={})\n",
    "        if not state.get(\"pending\"):\n",
    "            mark_site_completed(host, total=len(urls), done=len(state.get(\"done\", [])), failed=len(state.get(\"failed\", [])))\n",
    "\n",
    "    # Snapshot\n",
    "    snap = {\n",
    "        \"generated_at_utc\": now_iso(),\n",
    "        \"db_path\": DB_PATH,\n",
    "        \"requested_top_n\": A6_TOP_N,\n",
    "        \"dry_run\": A6_DRY_RUN,\n",
    "        \"sites_processed\": chosen_hosts,\n",
    "    }\n",
    "    snap_path = os.path.join(A6_OUTPUT_ROOT, \"snapshots\", f\"A6_run_{sel_ts}_summary.json\")\n",
    "    write_json(snap_path, snap)\n",
    "    log(f\"Wrote A6 snapshot → {snap_path}\")\n",
    "\n",
    "    # Final structured merge (site → products)\n",
    "    run_structured_merge(A6_OUTPUT_ROOT)\n",
    "\n",
    "# ------------------------------\n",
    "# Runner shim\n",
    "# ------------------------------\n",
    "def _run_main(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        if loop.is_running():\n",
    "            return asyncio.create_task(coro)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    return asyncio.run(coro)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = _run_main(run_agent6())\n",
    "    if t is not None:\n",
    "        try:\n",
    "            await t\n",
    "        except NameError:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0483474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065263d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebdfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409d498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb717f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35992107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111dac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
